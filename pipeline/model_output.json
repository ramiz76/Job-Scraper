[
    {
        "Data Analyst / Data Engineer (must be strong technically in SQL & python and Azure etc) required by a leading Milton Keynes based software house. Working hybrid (2 days in Central Milton Keynes) and closely with Marketing, this role sits within the data analytics and engineering team. We are looking for various engineers to join the team with roles ranging from \u00a370-120k plus package.Strong in SQL & Python and experience of working with marketing analytics tools is key as is someone who loves problem solving and has a vision / desire to help create a best-in-class analytics team.\u00a0Day to day will include -** deliver compelling insights to influence decision and roadmaps** Analytically tackle problems and deliver robust modelling** Visual storytelling** shape sophisticated datasets** apply creative thinking and mentor others.": [
            [
                "SQL",
                "HARD"
            ],
            [
                "python",
                "HARD"
            ],
            [
                "Azure",
                "HARD"
            ],
            [
                "Central Milton Keynes)",
                "CERT"
            ],
            [
                "SQL",
                "HARD"
            ],
            [
                "Python",
                "HARD"
            ],
            [
                "analytics",
                "SOFT"
            ],
            [
                "problem solving",
                "SOFT"
            ],
            [
                "modelling",
                "SOFT"
            ]
        ],
        "Flexible hybrid working, pension, bonus and much more on offer.": [
            [
                "hybrid working",
                "PERK"
            ]
        ]
    },
    {
        "To deliver site-based maintenance at the advertised location, and to participate in a call out rota. Full UK driving license required.": [],
        "Ensure that routine PPM (to SFG 20) is carried out to all mechanical and electrical plant in accordance with Site task schedules and asset lists.": [],
        "Ensure that reactive tasks throughout the building are completed and be proactive in highlighting areas where improvements can be made. These reactive calls to also include concessionary stores and fabric related items.": [],
        "Ensure that plant faults and defects are swiftly remedied to maintain plant in serviceable order at all times.": [],
        "Ensure that suitable spares are available to carry out both maintenance and reactive works. To provide parts lists and estimated timescales to carry out remedial works.": [],
        "Ensure that all relevant paperwork is completed regarding PPM, reactive and breakdown works and that it is passed without delay to the contract administrator.": [],
        "Ensure that Method Statements and Risk Assessments are prepared and used for all tasks undertaken to always ensure safe working practices.": [],
        "To develop a good working relationship with all members of CBRE staff, subcontractors, and the client representatives.": [],
        "Ensure the provision of a Safe and Healthy working environment. To always include the wearing of uniform and PPE.": [],
        "Ensure the professional image of CBRE Managed Services is always presented.": [],
        "Ensure up to date shift logs are always kept.": [],
        "Ensure that all plant rooms under the responsibility of CBRE Managed services are always locked and are kept in a clean and tidy condition.": [],
        "Supervising and monitoring of sub-contractors works whilst they are on site undertaking maintenance.": [],
        "To arrange and provide holiday and sickness cover at short notice and be flexible in their working patterns.": [],
        "To undertake lone working when and where required, subject to passing satisfactory training.": [],
        "Ensure assigned tasks are actioned and completed as appropriate.": [],
        "To ensure tasks as directed are completed with the appropriate H&S awareness/implementation.": [],
        "Ensure an awareness and compliance to the contractual KPI's/SLA's.": [],
        "Ensure Computer Based Maintenance system, SI LOCAL, is kept up to date and that PPM and reactive tickets are closed out with accurate data captured.": [],
        "To operate a Permit to Work System in accordance with CBRE Quality, Health & Safety procedures and client requirements.": [],
        "To provide holiday and emergency cover as required, at sites not normally covered.": [],
        "Any other task as directed by the account management team.": [],
        "Committed to the delivery of excellent customer service.": [
            [
                "customer service",
                "SOFT"
            ]
        ],
        "Calm manner, able to work under pressure.": [],
        "Able to make sound decisions when needed.": [],
        "Physically fit and able to carry tools and components by hand up to 20kg.": [
            [
                "20kg",
                "PERK"
            ]
        ],
        "Able to ascend and descend vertical access equipment. Able to work at heights.": [],
        "A team player, able to work with CBRE Managed Services and Client representatives at all levels.": [],
        "Good PC skills": [
            [
                "PC",
                "SOFT"
            ]
        ],
        "Good all round knowledge of general building works, diagnostic and repair procedures, working knowledge of building services systems and maintenance schedules.": [],
        "Formally trained in electrical/mechanical Maintenance.": [],
        "BMS Knowledge.": [
            [
                "BMS",
                "HARD"
            ]
        ],
        "Mechanical and commissioning experience.": [],
        "Recognised Health and Safety Qualification, such as IOSH.": [
            [
                "Recognised Health",
                "PERK"
            ],
            [
                "IOSH",
                "CERT"
            ]
        ],
        "Engineers must be able to demonstrate relevant knowledge of UPS, Generators, HV, PDU's, Crac units or equivalent, which will include the ability to fault find using the production of graphical information and the utilisation of current alarm data.": [
            [
                "alarm data",
                "SOFT"
            ]
        ],
        "Engineers must also understand the demarcation between maintainable assets and BPL maintained assets within laboratory and production environments.": [
            [
                "production",
                "SOFT"
            ]
        ],
        "Experience of maintaining all relevant mechanical and electrical equipment to clearly defined criteria.": [],
        "Good all round knowledge of general building works, diagnostic and repair procedures.": [],
        "Working knowledge of building services systems and maintenance schedules.": [],
        "Good verbal communication skills.": [
            [
                "verbal communication",
                "SOFT"
            ]
        ],
        "Good written English skills.": [
            [
                "written",
                "SOFT"
            ]
        ],
        "Must be willing to provide holiday and sickness cover at short notice and be flexible in their working patterns.": [],
        "Must have full UK driving license.": []
    },
    {
        "We are currently looking for an Azure Data Engineer who is seeking an exciting opportunity in a prominent company that is operating at a high level in the Real Estate industry. This company are a leader in their field and upon joining the company you will play a pivotal role advancing the data platform to new heights.": [
            [
                "Azure Data Engineer",
                "HARD"
            ]
        ],
        "As a Data Engineer, you will be hands on with the data everyday and responsible for building and implementing data pipelines, analytics, reporting, and alerting systems that enable our clients to harness the insights generated. This is a fantastic time to be part of the team, as you could be instrumental in contributing to their growth and paving the way for ground-breaking advancements in the industry.": [
            [
                "data pipelines",
                "SOFT"
            ],
            [
                "analytics",
                "SOFT"
            ]
        ],
        "IMPORTANT TO NOTE:": [],
        "This company are not able to offer sponsorship ": [],
        "This role is FULLY onsite in London for the firts 3 MONTHS. ": [],
        "Benefits to you:": [],
        "If you're a Data Engineer with strong SQL coding skills, and a background in Azure who is passionate about driving innovation and want to be part of a dynamic team pushing the boundaries of technology, we would love to hear from you.": [
            [
                "SQL",
                "HARD"
            ],
            [
                "Azure",
                "HARD"
            ]
        ],
        "Competitive salary \ud83d\udcb0": [],
        "Team colloboration and socials ": [
            [
                "colloboration",
                "SOFT"
            ]
        ],
        "Remote working options \ud83c\udfe0 (after 3 months)": [],
        "Growth and development opportunities \ud83c\udf31": []
    },
    {
        "Out client, an Aerospace and Defence contractor is looking for a Cable Design Engineer to join them on a contract basis at their site in Stevenage.": [],
        "Responsibilities:": [],
        "Skillset/experience required:": [],
        "Due to the nature of the role, applicants must be a sole British national and either hold SC Clearance or be eligible to obtain this.": [],
        "6-month initial contract, potential to extend.": [
            [
                "6-month initial contract",
                "PERK"
            ]
        ],
        "Hybrid working, 1-2 days a week onsite in Stevenage.": [
            [
                "Hybrid working",
                "PERK"
            ]
        ],
        "\u00a338.95 p/h Umbrella, inside IR35": [],
        "Creating bespoke cable designs for power, signal & data transmission.": [],
        "Interpreting Sub-System requirements to generate electrical schematics and cable design definitions on CAD.": [
            [
                "CAD",
                "SOFT"
            ]
        ],
        "Carrying out investigations into a full range of cable problems, issues or developments and developing and preparing solutions, individually or as a member of a project team.": [],
        "Liaison with manufacturing to resolve manufacturing problems and defects, plus support to functional and environmental trials.": [],
        "Cable/Harness design": [],
        "Experience on HarnWare, E3 or similar CAD package advantageous.": [
            [
                "HarnWare",
                "HARD"
            ]
        ],
        "Product life cycle awareness": [],
        "An understanding of Military Defence Standards and cable harness Manufacture (IPC-WHMA-A-620C). D38999 connector understanding.": [
            [
                "Defence Standards and cable harness Manufacture (IPC-WHMA-A-620C)",
                "PERK"
            ]
        ],
        "Understanding / experience of flexible circuits": [],
        "Ability to define cable construction based upon electrical constraints (Current, screening, signal types, volt drop, EMC and environmental requirements)": [],
        "Work effectively in collaboration with mechanical, electronic designers and project managers": [],
        "An appreciation of Interconnect production/process engineering": []
    },
    {
        "About\u00a0Autoglass\u00ae\u00a0and\u00a0Laddaw\u00ae": [],
        "We\u2019re a\u00a0recognised\u00a0super brand\u00a0and we know where we\u2019re going.\u00a0We\u2019re a business with direction and purpose and regardless of\u00a0your role\u00a0here, it\u2019s about us all making a difference with real care.\u00a0We never stand still. We\u2019re relentless,\u00a0innovative\u00a0and ambitious, always looking to go further and improve, which creates a world where we need genuine and driven people to help us get there.": [
            [
                "real care",
                "SOFT"
            ]
        ],
        "Making a Difference Together": [],
        "As a truly people focused\u00a0business, we believe in promoting opportunities for all, welcoming people who share our passion for enabling exceptional customer experience.\u00a0We\u2019ll support you with fantastic tools\u00a0and training in an atmosphere that encourages idea contribution and collaboration working with warm,\u00a0friendly\u00a0and real people.\u00a0If this sounds like somewhere you\u2019ll thrive,\u00a0keep reading.": [],
        "Role: Data Engineer": [],
        "Compensation: Competitive Salary, generous bonus scheme, up to 10% employer contributed workplace pension plus much more...": [
            [
                "10% employer contributed workplace pension plus much more...",
                "PERK"
            ]
        ],
        "Location: Commutable distance to our Bedford offices as this is a hybrid role.": [],
        "We are transforming our business through improved processes, flexible ways of working and a greater use of technology to make a difference to our customers and our people. We are developing super-talented people in our business to be the best; we want to create a great place to work and grow a business we are committed to and feel proud of. We are committed to doing the right things and doing them right; we care about our impact on the environment and are driven on giving back to society. We are determined to keep our business performance health to enable prosperity for all of us.": [],
        "This role and the role holder are integral to our success.": [],
        "The part you\u2019ll play in achieving our transformation will be:": [],
        "Your key responsibilities will be to:": [],
        "Skills, Experience and Qualifications": [],
        "The ideal candidate will have relevant experience in ingesting, transforming and supporting data processes within Snowflakes ecosystem.": [
            [
                "Snowflakes",
                "HARD"
            ]
        ],
        "Essential\u00a0Skills and Experience:": [],
        "Desirable": [],
        "Scale of the role": [],
        "As part of Belron UK and a newly created department, our ambition is to develop a data enabled business for the future. The role holder will be responsible for establishing best practice with the scale and maturity of the organisation and developing a culture of continuous improvement to transform the business production, use and management of Data & BI to enable business growth.": [],
        "Key stakeholders": [],
        "Our Customers, Our People, Society and Financial (fit for growth) are our key stakeholders as a business. The specific key stakeholders for this role are:": [],
        "If you think you fit the bill, then please apply now! We look forward to hearing form you!": [],
        "Accessibility:We make every effort to make our web presenceaccessible to all. Upon request and consistent with applicable laws, we\u2019ll provide reasonable accommodations to individuals who need assistance in the application/hiring process.": [],
        "Responsibility for developing accurate, efficient data transformations which meet customer needs to agreed timescales": [
            [
                "data transformations",
                "SOFT"
            ]
        ],
        "Ensuring the stability, robustness, and resilience of the projects you design and build, enjoying substantial autonomy while working within agreed standards": [],
        "Align activities to the Group wide data transformation programme both current future orientated": [],
        "Design and build of reliable, robust, and accurate data pipelines based on agreed best practices": [
            [
                "data pipelines",
                "SOFT"
            ]
        ],
        "Transformation of source data to meet business requirements": [],
        "Support the team in consolidating manual processes into a managed data environment": [
            [
                "data",
                "SOFT"
            ]
        ],
        "Work with key stakeholders and other data consumers to gather requirements": [],
        "Ad-hoc support on database administration tasks as needed": [
            [
                "database",
                "SOFT"
            ]
        ],
        "Understanding of Snowflake architecture, data modelling and administration": [
            [
                "Snowflake",
                "HARD"
            ],
            [
                "data modelling",
                "SOFT"
            ]
        ],
        "Experience in designing and implementing efficient ETL/ELT pipelines": [
            [
                "ETL",
                "SOFT"
            ],
            [
                "ELT pipelines",
                "SOFT"
            ]
        ],
        "Experience with AWS Data Services - AWS S3": [
            [
                "AWS Data Services",
                "HARD"
            ],
            [
                "AWS S3",
                "HARD"
            ]
        ],
        "Comfortable working with a range of data sources and formats e.g. JSON, XML, Flat files, API Integration": [
            [
                "JSON",
                "HARD"
            ],
            [
                "XML",
                "SOFT"
            ],
            [
                "Flat files",
                "SOFT"
            ],
            [
                "API Integration",
                "SOFT"
            ]
        ],
        "Understanding of dimensional modelling for Data Warehousing (Kimball)": [
            [
                "dimensional modelling",
                "SOFT"
            ],
            [
                "Data Warehousing",
                "SOFT"
            ]
        ],
        "Proficient in writing SQL, Stored procedures and views. Creating and optimising complex queries, analysing query performance, use of partitioning and clustering.": [
            [
                "SQL",
                "HARD"
            ],
            [
                "analysing query",
                "SOFT"
            ]
        ],
        "Experience in effectively coaching novice developers": [],
        "Good problem solving and data analysis skills": [
            [
                "problem solving",
                "SOFT"
            ],
            [
                "data analysis",
                "SOFT"
            ]
        ],
        "Excellent written and oral communication skills, ability to communicate complex concepts": [
            [
                "written",
                "SOFT"
            ],
            [
                "oral communication skills",
                "SOFT"
            ]
        ],
        "Ability to translate business requirements into technical solutions": [],
        "Experience with other databases MS SQL Server, Oracle": [
            [
                "databases MS",
                "SOFT"
            ],
            [
                "SQL Server",
                "HARD"
            ],
            [
                "Oracle",
                "HARD"
            ]
        ],
        "Experience using Informatica": [],
        "Python": [
            [
                "Python",
                "HARD"
            ]
        ],
        "Understanding of Data Lakes": [
            [
                "Data Lakes",
                "SOFT"
            ]
        ],
        "Familiar with Jira, Confluence": [
            [
                "Jira",
                "HARD"
            ]
        ],
        "Data Engineering Manager": [],
        "Insight & Analytics": [],
        "Data Led Decisions Programme": []
    },
    {
        "Data Engineer, Workforce Intelligence PXT-Talent": [
            [
                "PXT-Talent",
                "SOFT"
            ]
        ],
        "Amazon\u2019s mission is to be the most customer centric company in the world. The Workforce Staffing organization is on the front line of that mission by hiring the hourly fulfilment associates who make that mission a reality. To drive the necessary growth and continued scale of Amazon\u2019s associate needs within a constrained employment environment, Amazon is creating a Workforce Staffing Analytics program.": [],
        "This program re-invents how Amazon attracts, communicates with, and ultimately hires its hourly associates. This team own multi-layered research and program implementations to drive deep learnings, process improvements, and strategic recommendations to global leadership. Are you passionate about data? Do you enjoy questioning the status quo? Do complex and difficult challenges excite you? If yes, this may be the team for you.": [],
        "As a Data Engineer, you should be an expert in the architecture of data solutions for the Enterprise using multiple platforms. You should excel in the design, creation, management, and business use of extremely large data sets. You should have excellent business and communication skills to be able to work with business analysts and engineers to determine how best to design the data warehouse for reporting and analytics. You will be responsible for designing and implementing scalable ETL processes in the data platform to support the rapidly growing and dynamic business demand for data, and use it to deliver the data as service which will have an immediate influence on day-to-day decision making. You should have the ability to develop and tune SQL to provide optimized solutions to the business.": [
            [
                "data solutions",
                "SOFT"
            ],
            [
                "data warehouse",
                "SOFT"
            ],
            [
                "analytics",
                "SOFT"
            ],
            [
                "implementing",
                "SOFT"
            ],
            [
                "ETL",
                "SOFT"
            ],
            [
                "SQL",
                "HARD"
            ]
        ],
        "Basic qualifications": [],
        "Preferred qualifications": [],
        "Amazon is an equal opportunities employer. We believe passionately that employing a diverse workforce is central to our success. We make recruiting decisions based on your experience and skills. We value your passion to discover, invent, simplify and build. Protecting your privacy and the security of your data is a longstanding top priority for Amazon.": [],
        "3+ years of data engineering experience": [],
        "Experience with big data technologies such as: Hadoop, Hive, Spark, EMR": [
            [
                "big data",
                "SOFT"
            ],
            [
                "Hadoop",
                "HARD"
            ],
            [
                "Hive",
                "HARD"
            ],
            [
                "Spark",
                "HARD"
            ],
            [
                "EMR",
                "HARD"
            ]
        ],
        "Experience with SQL": [
            [
                "SQL",
                "HARD"
            ]
        ],
        "Experience in at least one modern scripting or programming language, such as Python, Java, Scala, or NodeJS": [
            [
                "Python",
                "HARD"
            ],
            [
                "Java",
                "HARD"
            ],
            [
                "Scala",
                "HARD"
            ],
            [
                "NodeJS",
                "HARD"
            ]
        ],
        "Experience with data modeling, warehousing and building ETL pipelines": [
            [
                "data modeling",
                "SOFT"
            ],
            [
                "ETL",
                "SOFT"
            ]
        ],
        "Experience in tuning performance in big data / distributed systems": [
            [
                "big data",
                "SOFT"
            ],
            [
                "distributed",
                "SOFT"
            ]
        ],
        "Bachelor's Degree in Computer Science, Engineering, Mathematics, or a related field": [
            [
                "Computer Science",
                "CERT"
            ],
            [
                "Engineering",
                "CERT"
            ],
            [
                "Mathematics",
                "CERT"
            ]
        ],
        "Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions": [
            [
                "AWS",
                "HARD"
            ],
            [
                "Redshift",
                "HARD"
            ],
            [
                "S3",
                "HARD"
            ],
            [
                "Glue",
                "HARD"
            ],
            [
                "Kinesis",
                "HARD"
            ],
            [
                "FireHose",
                "HARD"
            ],
            [
                "Lambda",
                "HARD"
            ],
            [
                "IAM",
                "HARD"
            ]
        ],
        "Experience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases)": [
            [
                "non-relational databases",
                "SOFT"
            ],
            [
                "graph databases",
                "SOFT"
            ],
            [
                "column-family databases",
                "SOFT"
            ]
        ],
        "Knowledge of professional software engineering & best practices for full software development life cycle, including coding standards, software architectures, code reviews, source control management, continuous deployments, testing, and operational excellence": [
            [
                "software development",
                "SOFT"
            ],
            [
                "software architectures",
                "SOFT"
            ]
        ],
        "Experience with Lakehouse formats, such as Apache Iceberg, Delta lake, Apache Hudi.": [
            [
                "Lakehouse",
                "SOFT"
            ],
            [
                "Apache Iceberg",
                "HARD"
            ],
            [
                "Delta lake",
                "HARD"
            ],
            [
                "Apache Hudi",
                "HARD"
            ]
        ]
    },
    {
        "Ecommerce / E-Commerce Data Analyst / Data Engineer / SQL / Python / MI / Business Intelligence": [
            [
                "SQL",
                "HARD"
            ],
            [
                "Python",
                "HARD"
            ]
        ],
        "This Role": [],
        "This is an excellent opportunity to join us on our fast-growth journey! We're looking for a superb Data candidate to join our team in this newly created role to super-charge our growth by helping the team make reliable data driven decisions. You must have experience working within Ecommerce.": [],
        "You'll help implement split tests and analyse the data to determine what works and what doesn't, what helps and what hinders conversions. You'll be given a high level of autonomy and your work will have a real impact on the performance of the company.": [
            [
                "analyse",
                "SOFT"
            ]
        ],
        "Your responsibilities will include:": [],
        "You'll excel in this role if you have the following skills & experience:": [],
        "Benefits:": [],
        "Work to uncover trends in the data, building models to predict sales.": [],
        "Create dashboards and visual reports for reporting to the team and management.": [
            [
                "visual reports",
                "SOFT"
            ]
        ],
        "Work with our developers to devise algorithms for testing and production.": [
            [
                "testing",
                "SOFT"
            ]
        ],
        "Developing and maintaining marketing data infrastructure": [
            [
                "data",
                "SOFT"
            ]
        ],
        "Collaborating with our design and development team, together with other stakeholders, to help drive the user experience on our website and soon, mobile app.": [],
        "Leading weekly conversion optimisation strategy meetings with management": [],
        "Formulating ideas and hypothesis to keep our testing roadmap full.": [],
        "Interpreting raw data test results, turn them into insights and presenting those finding to senior management.": [
            [
                "Interpreting",
                "SOFT"
            ]
        ],
        "Experience within a data driven, analytics environment.": [
            [
                "analytics",
                "SOFT"
            ]
        ],
        "Comfortable working with large data sets.": [],
        "Ecommerce sector experience required": [],
        "demonstrable ability to interpret complex data easily, summarising it in easy-to-understand learnings for your colleagues and management": [],
        "SQL, Python experience would be necessary.": [
            [
                "SQL",
                "HARD"
            ],
            [
                "Python",
                "HARD"
            ]
        ],
        "Free breakfast every morning": [
            [
                "Free breakfast every morning",
                "PERK"
            ]
        ],
        "Minimum of 25 days holiday per year as standard (plus more the longer you stay)": [
            [
                "25 days holiday per year",
                "PERK"
            ]
        ],
        "Flexible hours": [
            [
                "Flexible hours",
                "PERK"
            ]
        ],
        "Your birthday off work": [
            [
                "birthday off work",
                "PERK"
            ]
        ],
        "Free EV charging": [
            [
                "Free EV charging",
                "PERK"
            ]
        ],
        "Free car washing": [
            [
                "Free car washing",
                "PERK"
            ]
        ],
        "Regular team and charity fundraising events": [],
        "Generous staff discount": [],
        "Company pension": [
            [
                "Company pension",
                "PERK"
            ]
        ]
    },
    {
        "Data Engineer (Synapse, ETL, Azure, SQL, CI/CD, Spark) - \u00a350k - \u00a360k - Hybrid  Data Engineer (Synapse, ETL, Azure, SQL, CI/CD, Spark) - \u00a350k - \u00a360k - Hybrid   I am partnered with one of the UK's leading real estate organisations and due to a lot of recent success in the market, they are looking to expand across their engineering function and are looking to connect with Data Engineers (ETL, Azure, SQL, CI/CD, Spark) to help move their legacy databases to the cloud.  As a business they are offering flexible hybrid working and ask that the engineering teams are in the office twice a week.  As they expand, the Data Engineer (ETL, Azure, SQL, CI/CD, Spark) will be responsible for building scalable pipelines for now and the future to help cleanse data from several sources. The Data Engineers (ETL, Azure, SQL, CI/CD, Spark) will work hand in hand with the software engineering team and architects to ensure that data delivery is clean and consistent through the transformation.   Skill set required": [
            [
                "Synapse",
                "HARD"
            ],
            [
                "ETL",
                "SOFT"
            ],
            [
                "Azure",
                "HARD"
            ],
            [
                "SQL",
                "HARD"
            ],
            [
                "CI/CD",
                "SOFT"
            ],
            [
                "Spark",
                "HARD"
            ],
            [
                "Synapse",
                "HARD"
            ],
            [
                "ETL",
                "SOFT"
            ],
            [
                "Azure",
                "HARD"
            ],
            [
                "SQL",
                "HARD"
            ],
            [
                "CI/CD",
                "SOFT"
            ],
            [
                "Spark",
                "HARD"
            ],
            [
                "ETL",
                "SOFT"
            ],
            [
                "Azure",
                "HARD"
            ],
            [
                "SQL",
                "HARD"
            ],
            [
                "CI/CD",
                "SOFT"
            ],
            [
                "Spark",
                "HARD"
            ],
            [
                "hybrid working",
                "PERK"
            ],
            [
                "ETL",
                "SOFT"
            ],
            [
                "Azure",
                "HARD"
            ],
            [
                "SQL",
                "HARD"
            ],
            [
                "CI/CD",
                "SOFT"
            ],
            [
                "Spark",
                "HARD"
            ],
            [
                "scalable pipelines",
                "SOFT"
            ],
            [
                "cleanse data from",
                "SOFT"
            ],
            [
                "ETL",
                "SOFT"
            ],
            [
                "Azure",
                "HARD"
            ],
            [
                "SQL",
                "HARD"
            ],
            [
                "CI/CD",
                "SOFT"
            ],
            [
                "Spark",
                "HARD"
            ]
        ],
        " If you are a Data Engineer (ETL, Azure, SQL, CI/CD, Spark) and the above sounds like you, then please click on the apply button below.  Data Engineer (Synapse, ETL, Azure, SQL, CI/CD, Spark) - \u00a350k - \u00a360k - Hybrid  Data Engineer (Synapse, ETL, Azure, SQL, CI/CD, Spark) - \u00a350k - \u00a360k - Hybrid": [
            [
                "ETL",
                "SOFT"
            ],
            [
                "Azure",
                "HARD"
            ],
            [
                "SQL",
                "HARD"
            ],
            [
                "CI/CD",
                "SOFT"
            ],
            [
                "Spark",
                "HARD"
            ],
            [
                "Synapse",
                "HARD"
            ],
            [
                "ETL",
                "SOFT"
            ],
            [
                "Azure",
                "HARD"
            ],
            [
                "SQL",
                "HARD"
            ],
            [
                "CI/CD",
                "SOFT"
            ],
            [
                "Spark",
                "HARD"
            ],
            [
                "Synapse",
                "HARD"
            ],
            [
                "ETL",
                "SOFT"
            ],
            [
                "Azure",
                "HARD"
            ],
            [
                "SQL",
                "HARD"
            ],
            [
                "CI/CD",
                "SOFT"
            ],
            [
                "Spark",
                "HARD"
            ],
            [
                "Hybrid",
                "PERK"
            ]
        ],
        "Azure": [
            [
                "Azure",
                "HARD"
            ]
        ],
        "ETL": [
            [
                "ETL",
                "SOFT"
            ]
        ],
        "Data Factory": [
            [
                "Data Factory",
                "SOFT"
            ]
        ],
        "Synapse": [
            [
                "Synapse",
                "HARD"
            ]
        ],
        "Data Lakes": [
            [
                "Data Lakes",
                "SOFT"
            ]
        ],
        "CI/CD - Azure DevOps": [
            [
                "CI/CD",
                "SOFT"
            ],
            [
                "Azure DevOps",
                "HARD"
            ]
        ],
        "Spark / PySpark": [
            [
                "Spark",
                "HARD"
            ],
            [
                "PySpark",
                "HARD"
            ]
        ]
    },
    {},
    {
        "Cyber Security Engineer | Data Security Engineer Fully Remote Permanent \u00a350,000 - \u00a360,000 ": [],
        "A Cyber / Data Security Engineer is required to join a highly successful, up and coming technology consulting services business to take responsibility for designing, implementing, and maintaining robust data security measures to protect sensitive information for end clients. You will be working closely with cross-functional teams to identify vulnerabilities, develop security solutions, and ensure the integrity, confidentiality, and availability of data. Expertise required in M365, Azure, Microsoft Purview and Sharepoint from a Security perspective. This is a client facing role so strong communication skills are extremely important.": [
            [
                "M365",
                "PERK"
            ],
            [
                "Azure",
                "HARD"
            ],
            [
                "Microsoft",
                "HARD"
            ],
            [
                "Sharepoint from",
                "PERK"
            ]
        ],
        "What the role entails:": [],
        "* Data Security Infrastructure: Design, implement, and maintain data security infrastructure, including firewalls, intrusion detection systems, encryption mechanisms, access controls, and authentication systems. Ensure the infrastructure aligns with industry best practices and regulatory requirements. * Vulnerability Assessment and Penetration Testing: Conduct regular vulnerability assessments and penetration testing to identify potential security weaknesses in data systems, networks, and applications. Collaborate with stakeholders to address identified vulnerabilities and implement necessary patches and updates. * Data Encryption and Protection: Develop and implement data encryption mechanisms to safeguard sensitive data at rest and in transit. Apply appropriate encryption algorithms and key management practices to protect data integrity and confidentiality. * Security Incident Response: Collaborate with the incident response team to investigate security incidents, breaches, and data leaks. Develop incident response plans and procedures to mitigate risks, minimize impact, and ensure timely resolution. * Security Monitoring and Threat Intelligence: Implement and manage security monitoring tools and systems to detect and respond to potential threats. Stay updated with the latest security threats and trends by monitoring threat intelligence sources and applying necessary preventive measures.": [
            [
                "data security",
                "SOFT"
            ],
            [
                "detection systems",
                "SOFT"
            ],
            [
                "authentication systems",
                "SOFT"
            ],
            [
                "Penetration Testing",
                "SOFT"
            ],
            [
                "penetration testing",
                "SOFT"
            ],
            [
                "data systems",
                "SOFT"
            ],
            [
                "Data Encryption",
                "SOFT"
            ],
            [
                "key management",
                "SOFT"
            ],
            [
                "data leaks",
                "SOFT"
            ],
            [
                "detect",
                "SOFT"
            ]
        ],
        "What we need from you:": [],
        "* Bachelor's degree in computer science, Information Security, or a related field. * Proven experience (5+ years) in data security engineering or a related role, with a focus on implementing and maintaining data security measures. * Strong understanding of data security principles, protocols, and best practices. * Proficiency in security technologies, such as firewalls, intrusion detection systems, encryption algorithms, authentication mechanisms, and vulnerability assessment tools. * Experience with security incident response, including investigation, containment, and remediation. * Familiarity with relevant data protection regulations and industry standards. * Knowledge of network protocols, architecture, and common application security vulnerabilities. * Strong problem-solving and analytical skills, with the ability to assess and address complex security issues. * Excellent communication and collaboration skills to work effectively with technical and non-technical stakeholders. * Ability to adapt to changing security threats and technologies and continuously update knowledge and skills.": [
            [
                "*",
                "CERT"
            ],
            [
                "computer science",
                "CERT"
            ],
            [
                "data security engineering",
                "SOFT"
            ],
            [
                "detection systems",
                "SOFT"
            ],
            [
                "encryption algorithms",
                "SOFT"
            ],
            [
                "vulnerability assessment",
                "SOFT"
            ],
            [
                "*",
                "SOFT"
            ],
            [
                "relevant data protection",
                "SOFT"
            ],
            [
                "network protocols",
                "SOFT"
            ],
            [
                "problem-solving",
                "SOFT"
            ],
            [
                "analytical",
                "SOFT"
            ],
            [
                "communication",
                "SOFT"
            ],
            [
                "collaboration",
                "SOFT"
            ]
        ],
        "If you are a seasoned Data Security Engineer that would like to join a technology centric company embarking on a new and exciting journey working with household names, then please hit apply now!": []
    },
    {
        "An exciting and challenging opportunity has arisen for a Data Centre Engineer to support one of our data centres based in Southall, Middlesex.": [],
        "The successful applicant should be technically competent, have excellent written and verbal communication skills and be familiar with the data centre environment.\u00a0": [
            [
                "written",
                "SOFT"
            ],
            [
                "verbal communication",
                "SOFT"
            ]
        ],
        "This is a permanent full-time position working Monday - Friday, 08:00 - 16:00 and also working as part of the shared on call rota to provide the required 24/7 on-site support.": [],
        "The successful candidate must either already hold or be able to obtain SC security clearance. For this reason, we are only able to progress with applications from British nationals.": [],
        "As a Data Centre Engineer you will:": [],
        "What we're looking for:": [],
        "Desirable skills:\u00a0": [],
        "This is an excellent opportunity to join us and as an employee you will gain access to a large library of training courses and accreditations to help further your skills and development.": [
            [
                "development",
                "SOFT"
            ]
        ],
        "As an FDS Employee you will benefit from:": [],
        "Apply now for consideration!": [],
        "By completing the application process you agree that ES Field Delivery UK LTD (FDS) may contact you in line with General Data Protection Regulation (GDPR) in connection with your application via the contact details you provided in relation to the vacancy you have applied for. Our Privacy Notice can be viewed via our website.": [
            [
                "Data Protection",
                "SOFT"
            ]
        ],
        "Plan and implement where new cabinets and free-standing equipment should be installed\u00a0": [],
        "Design and manage all structured cabling infrastructure requirements": [],
        "Advise requirements for additional network or storage capability and liaise with the associated support teams": [],
        "Install and configure hardware in the Data Hall": [],
        "Manage the asset register for all equipment installed in the Data Hall": [],
        "Troubleshoot hardware issues reported by Support/Platform Teams": [],
        "Provide local Smart Hands assistance to Support/Platform Teams": [],
        "Escort and supervise maintenance services provided by third party vendors": [],
        "Schedule 3rd party break-fix activities": [],
        "Strong communication skills both verbal and written": [
            [
                "communication skills",
                "SOFT"
            ],
            [
                "verbal",
                "SOFT"
            ],
            [
                "written",
                "SOFT"
            ]
        ],
        "Ability to work precisely and quality oriented": [
            [
                "quality",
                "SOFT"
            ]
        ],
        "Flexibility and willingness to cope with unexpected problems and situations": [],
        "Capacity to manage customer expectations and interactions effectively": [],
        "Basic understanding of server HW (console, devices\u2026)": [
            [
                "server HW (console",
                "SOFT"
            ]
        ],
        "Generic media management knowledge": [],
        "Proficiency in Microsoft Windows, Microsoft Office environment and general PC hardware": [
            [
                "Microsoft Windows",
                "CERT"
            ],
            [
                "Microsoft Office",
                "PERK"
            ]
        ],
        "Basic understanding of Networking": [
            [
                "Networking",
                "SOFT"
            ]
        ],
        "A solid understanding of data centre environments and SOP\u2019s\u00a0": [],
        "Professional certification in a relative field": [],
        "Proven work history in an IT environment\u00a0": [],
        "Experience of IT operations in a data centre or similar environment": [],
        "Flexible benefits including, medical insurance and eyecare vouchers": [
            [
                "Flexible benefits including",
                "PERK"
            ],
            [
                "eyecare vouchers",
                "PERK"
            ]
        ],
        "Company pension scheme": [
            [
                "Company pension scheme",
                "PERK"
            ]
        ],
        "Income protection after 6 month\u2019s service": [
            [
                "Income protection",
                "PERK"
            ]
        ],
        "23 days holiday raising to max 25": [
            [
                "23 days holiday raising to max 25",
                "PERK"
            ]
        ],
        "Option to purchase / sell additional holiday": [],
        "Life insurance": [
            [
                "Life insurance",
                "PERK"
            ]
        ],
        "Employee Assistance Programme": []
    },
    {
        "Urban Empire Recruitment has partnered with a start-up company that is a disrupter in the insurance industry. In the same way that fintech companies have disrupted the traditional banking sector, reimagining financial platforms for a new generation.": [
            [
                "generation",
                "SOFT"
            ]
        ],
        "My client has developed a flexible insurance that is a mobile-first, frictionless platform which lets people interact with their insurance provider with the same ease, speed and transparency they\u2019re already used to having with providers in other sectors. Customers can pay monthly, instantly make changes to their cover, and bring all their disclosure, payment and claims information together in a single place.": [],
        "This is a fantastic opportunity for you to build something special with a close knit team, we offer lots of progression in a fast paced position where two days are never the same!": [],
        "Responsibilities:": [],
        "Qualifications:": [],
        "You\u00a0must\u00a0also have experience using the following software/tools:": [],
        "When you apply for this role a consultant from Urban Empire Recruitment will give you a call if you\u2019re successful.": [],
        "Please note due to the large volume of applications we receive for these roles, if we have not contacted you within 7 days then unfortunately your application hasn't been successful. however, we may contact you regarding other roles. We're sorry we can't contact you directly, but we wish you all the best in your job search": [],
        "Job Title:\u00a0Data Engineer": [],
        "Salary:\u00a375,000 - \u00a385,000 D.O.E": [],
        "Location:\u00a0London (Hybrid 3days in the office)": [
            [
                "Hybrid 3days",
                "PERK"
            ]
        ],
        "Job Type:\u00a0Full Time, Permanent": [],
        "Develop and sustain an efficient data pipeline architecture that can adapt to the needs of the organisation.": [
            [
                "data pipeline",
                "SOFT"
            ]
        ],
        "Compile extensive, intricate datasets that align with both functional and non-functional business objectives.": [
            [
                "non-functional business",
                "SOFT"
            ]
        ],
        "Devise and execute enhancements to internal workflows by automating manual tasks, refining data distribution methods, and redesigning infrastructure to improve scalability.": [
            [
                "data distribution",
                "SOFT"
            ]
        ],
        "Construct the necessary framework for effective data extraction, transformation, and loading (ETL), leveraging SQL and AWS-based 'big data' technologies.": [
            [
                "data extraction",
                "SOFT"
            ],
            [
                "loading",
                "SOFT"
            ],
            [
                "ETL",
                "SOFT"
            ],
            [
                "SQL",
                "HARD"
            ],
            [
                "AWS",
                "HARD"
            ],
            [
                "big data",
                "SOFT"
            ]
        ],
        "Develop analytics solutions that draw from the data pipeline, offering actionable insights into areas like customer acquisition, operational efficacy, and other vital business performance indicators.": [
            [
                "data pipeline",
                "SOFT"
            ],
            [
                "customer acquisition",
                "SOFT"
            ]
        ],
        "Collaborate with software engineers, database architects, data analysts, and data scientists to drive data-centric projects and ensure a streamlined data delivery architecture.": [
            [
                "drive data",
                "SOFT"
            ],
            [
                "data delivery",
                "SOFT"
            ]
        ],
        "Innovate and automate data and reporting infrastructures for more effective data management.": [
            [
                "automate data",
                "SOFT"
            ],
            [
                "reporting infrastructures",
                "SOFT"
            ],
            [
                "data management",
                "SOFT"
            ]
        ],
        "Assist our data scientists in transitioning machine learning and data science algorithms from the experimental stage to scalable production deployments.": [
            [
                "machine learning",
                "SOFT"
            ],
            [
                "data science",
                "SOFT"
            ],
            [
                "scalable production",
                "SOFT"
            ]
        ],
        "Implement data governance best practices, ensuring data security, quality, and compliance with relevant regulations.": [
            [
                "data security",
                "SOFT"
            ]
        ],
        "Constantly monitor the data pipelines and architecture, making improvements to enhance performance and reduce costs where possible.": [
            [
                "data pipelines",
                "SOFT"
            ],
            [
                "architecture",
                "SOFT"
            ]
        ],
        "Stay abreast of new technologies and tools in the data engineering space to maintain the relevancy and efficiency of our data systems.": [
            [
                "data systems",
                "SOFT"
            ]
        ],
        "4+ years of experience in a Data Engineer role.": [],
        "Must have lots of AWS experience\u00a0(Essential)": [
            [
                "AWS",
                "HARD"
            ]
        ],
        "Experience building and optimising \u2018big data\u2019 data pipelines, architectures and data sets.": [
            [
                "big data",
                "SOFT"
            ],
            [
                "data pipelines",
                "SOFT"
            ],
            [
                "architectures",
                "SOFT"
            ]
        ],
        "Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.": [
            [
                "data",
                "SOFT"
            ]
        ],
        "Strong analytic skills related to working with unstructured datasets and ability to build processes supporting data transformation, data structures, metadata, dependency and workload management.": [
            [
                "analytic",
                "SOFT"
            ],
            [
                "data transformation",
                "SOFT"
            ],
            [
                "data structures",
                "SOFT"
            ],
            [
                "workload management",
                "SOFT"
            ]
        ],
        "A successful history of manipulating, processing and extracting value from large disconnected datasets.": [],
        "Working knowledge of message queuing, stream processing, and highly scalable \u2018big data\u2019 data stores.": [
            [
                "message queuing",
                "SOFT"
            ],
            [
                "stream processing",
                "SOFT"
            ],
            [
                "big data",
                "SOFT"
            ]
        ],
        "AWS Services: Deep understanding of AWS services like S3, Redshift, EMR, Kinesis, and Glue, used for data storage and processing.": [
            [
                "AWS Services: Deep",
                "CERT"
            ],
            [
                "AWS",
                "HARD"
            ],
            [
                "S3",
                "HARD"
            ],
            [
                "Redshift",
                "HARD"
            ],
            [
                "Kinesis",
                "HARD"
            ],
            [
                "Glue",
                "HARD"
            ],
            [
                "data storage",
                "SOFT"
            ]
        ],
        "ETL Processes: Experience with Extract, Transform, and Load (ETL) processes and tools, particularly in an AWS environment using services like AWS Glue or Data Pipeline.": [
            [
                "ETL Processes",
                "SOFT"
            ],
            [
                "Extract",
                "SOFT"
            ],
            [
                "Transform",
                "SOFT"
            ],
            [
                "Load (ETL)",
                "SOFT"
            ],
            [
                "AWS",
                "HARD"
            ],
            [
                "AWS Glue",
                "HARD"
            ],
            [
                "Data Pipeline",
                "SOFT"
            ]
        ],
        "Big Data Technologies: Proficiency in big data technologies like Hadoop and Spark, and how they integrate with AWS services like EMR.": [
            [
                "Big Data Technologies",
                "SOFT"
            ],
            [
                "big data",
                "SOFT"
            ],
            [
                "Hadoop",
                "HARD"
            ],
            [
                "Spark",
                "HARD"
            ],
            [
                "AWS",
                "HARD"
            ]
        ],
        "Programming Languages: Strong programming skills in languages commonly used in data engineering such as Python, Java, or Scala.": [
            [
                "data engineering",
                "CERT"
            ],
            [
                "Python",
                "HARD"
            ],
            [
                "Java",
                "HARD"
            ],
            [
                "Scala",
                "HARD"
            ]
        ],
        "SQL and NoSQL Databases: String proficiency in SQL for querying relational databases like Amazon RDS, and experience with NoSQL databases like DynamoDB.": [
            [
                "SQL",
                "HARD"
            ],
            [
                "NoSQL",
                "HARD"
            ],
            [
                "SQL",
                "HARD"
            ],
            [
                "querying relational",
                "SOFT"
            ],
            [
                "Amazon RDS",
                "HARD"
            ],
            [
                "NoSQL",
                "HARD"
            ],
            [
                "DynamoDB",
                "HARD"
            ]
        ],
        "Data Modeling: Ability to design and implement data models in data warehousing solutions like Amazon Redshift.": [
            [
                "Data Modeling",
                "SOFT"
            ],
            [
                "data warehousing",
                "SOFT"
            ],
            [
                "Amazon Redshift",
                "HARD"
            ]
        ],
        "Streaming Data: Experience with real-time data processing using AWS services like Kinesis or managed Kafka.": [
            [
                "real-time data",
                "SOFT"
            ],
            [
                "AWS",
                "HARD"
            ],
            [
                "Kinesis",
                "HARD"
            ],
            [
                "Kafka",
                "HARD"
            ]
        ],
        "Data Security: Understanding of data encryption, IAM roles, and other security best practices within AWS.": [
            [
                "data encryption",
                "SOFT"
            ],
            [
                "IAM",
                "HARD"
            ],
            [
                "AWS",
                "HARD"
            ]
        ],
        "DevOps: Familiarity with DevOps practices including CI/CD, and tools like Terraform, Cludformation, AWS CodeBuild, CodeDeploy, and CodePipeline.": [
            [
                "DevOps",
                "SOFT"
            ],
            [
                "DevOps",
                "SOFT"
            ],
            [
                "CI/CD",
                "SOFT"
            ],
            [
                "Terraform",
                "HARD"
            ],
            [
                "Cludformation",
                "HARD"
            ],
            [
                "AWS",
                "HARD"
            ],
            [
                "CodePipeline",
                "HARD"
            ]
        ],
        "APIs: Knowledge of RESTful API interactions, especially within the AWS ecosystem.": [
            [
                "RESTful",
                "SOFT"
            ],
            [
                "AWS",
                "HARD"
            ]
        ],
        "Experience supporting and working with cross-functional teams in a dynamic environment.": [],
        "Strong problem-solving skills, with the ability to troubleshoot and address technical challenges.": [
            [
                "problem-solving",
                "SOFT"
            ]
        ],
        "Excellent communication skills, with the ability to work in a team and convey technical concepts clearly.": [
            [
                "communication skills",
                "SOFT"
            ]
        ]
    },
    {
        "Lead Data Engineer": [],
        "Do you have an interest in Data & Analytics and a desire to expand your skillset and further your career? Are you keen to join a lively team, using your investigative skills to achieve the best outcome for our clients? Are you looking for an opportunity to grow and develop in a hybrid role with the flexibility to work virtually and from an Aon office? If so, get ready to embark on a career at Aon!": [],
        "As a Lead Data Engineer within the UK Data & Analytics team, you will join a growing team which plays a pivotal role within Aon Business Services; innovating, developing, and delivering market leading analytics to our stakeholders. Our standard of excellence accelerates growth and empowers the business to bring the best of Aon to our clients!": [],
        "Aon is in the business of better decisions": [],
        "At Aon, we shape decisions for the better to protect and enrich the lives of people around the world.": [],
        "As an organisation, we are united through trust as one inclusive, diverse team, and we are passionate about helping our colleagues and clients succeed.": [],
        "What the day will look like": [],
        "You will be responsible for designing, building and maintaining scalable and efficient data pipelines to collect data from various sources. You will define the development, test and release standards across the team and work closely with the Data Architect to ensure all solutions conform to team standards around quality, security, data privacy and performance optimisation.": [
            [
                "scalable",
                "SOFT"
            ],
            [
                "data pipelines",
                "SOFT"
            ],
            [
                "collect data from",
                "SOFT"
            ]
        ],
        "You will manage a team of data engineers, providing guidance, technical expertise and fostering a culture of continuous learning. You will be responsible for developing a strategic roadmap for Data Engineering capability in conjunction with the Data & Analytics Leader.": [],
        "How this opportunity is different": [],
        "This is an exceptional opportunity to build and influence the data & analytics capability across Aon UK. Our Data & Analytics team empowers the business to make better decisions, and with our data expertise, we deliver advanced analytics to fuel growth, meet client needs and boost innovation across the business to bring the future of data & analytics to our clients today.": [],
        "Skills and experience that will lead to success": [],
        "How we support our colleagues": [],
        "In addition to our comprehensive benefits package, we encourage a diverse workforce. Plus, our agile, inclusive environment allows you to manage your wellbeing and work/life balance, ensuring you can be your best self at Aon. Furthermore, all colleagues enjoy two \u201cGlobal Wellbeing Days\u201d each year, encouraging you to take time to focus on yourself. We offer a variety of working style solutions, but we also recognise that flexibility goes beyond just the place of work... and we are all for it. We call this Smart Working!": [],
        "Our continuous learning culture inspires and equips you to learn, share and grow, helping you achieve your fullest potential. As a result, at Aon, you are more connected, more relevant, and more valued.": [],
        "We provide individuals with disabilities reasonable accommodations to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment in accordance with applicable law. Please contact us to request an accommodation on ": [],
        "Aon values an innovative, diverse workplace where all colleagues feel empowered to be their authentic selves. Aon is proud to be an equal opportunity workplace.": [],
        "#LI-JK1": [],
        "#LI-HYBRID": [
            [
                "LI",
                "SOFT"
            ],
            [
                "HYBRID",
                "SOFT"
            ]
        ],
        "Proven experience in a Data & Analytics/Data Engineering role": [],
        "Good knowledge of SQL": [
            [
                "SQL",
                "HARD"
            ]
        ],
        "Agile Development Methodology": [
            [
                "Agile Development",
                "SOFT"
            ]
        ],
        "Experience in building data ingestion pipelines and/or modelling data set.": [
            [
                "data ingestion",
                "SOFT"
            ],
            [
                "modelling data",
                "SOFT"
            ]
        ],
        "Technical expertise in cloud-based data & analytics technologies, and DevOps tooling, including Continuous Improvement/Continuous Development": [
            [
                "cloud-based data",
                "SOFT"
            ],
            [
                "DevOps",
                "SOFT"
            ],
            [
                "Continuous Improvement/Continuous Development",
                "SOFT"
            ]
        ]
    },
    {
        "To deliver site-based maintenance at the advertised location, and to participate in a call out rota. Full UK driving license required.": [],
        "Ensure that routine PPM (to SFG 20) is carried out to all mechanical and electrical plant in accordance with Site task schedules and asset lists.": [],
        "Ensure that reactive tasks throughout the building are completed and be proactive in highlighting areas where improvements can be made. These reactive calls to also include concessionary stores and fabric related items.": [],
        "Ensure that plant faults and defects are swiftly remedied to maintain plant in serviceable order at all times.": [],
        "Ensure that suitable spares are available to carry out both maintenance and reactive works. To provide parts lists and estimated timescales to carry out remedial works.": [],
        "Ensure that all relevant paperwork is completed regarding PPM, reactive and breakdown works and that it is passed without delay to the contract administrator.": [],
        "Ensure that Method Statements and Risk Assessments are prepared and used for all tasks undertaken to always ensure safe working practices.": [],
        "To develop a good working relationship with all members of CBRE staff, subcontractors, and the client representatives.": [],
        "Ensure the provision of a Safe and Healthy working environment. To always include the wearing of uniform and PPE.": [],
        "Ensure the professional image of CBRE Managed Services is always presented.": [],
        "Ensure up to date shift logs are always kept.": [],
        "Ensure that all plant rooms under the responsibility of CBRE Managed services are always locked and are kept in a clean and tidy condition.": [],
        "Supervising and monitoring of sub-contractors works whilst they are on site undertaking maintenance.": [],
        "To arrange and provide holiday and sickness cover at short notice and be flexible in their working patterns.": [],
        "To undertake lone working when and where required, subject to passing satisfactory training.": [],
        "Ensure assigned tasks are actioned and completed as appropriate.": [],
        "To ensure tasks as directed are completed with the appropriate H&S awareness/implementation.": [],
        "Ensure an awareness and compliance to the contractual KPI's/SLA's.": [],
        "Ensure Computer Based Maintenance system, SI LOCAL, is kept up to date and that PPM and reactive tickets are closed out with accurate data captured.": [],
        "To operate a Permit to Work System in accordance with CBRE Quality, Health & Safety procedures and client requirements.": [],
        "To provide holiday and emergency cover as required, at sites not normally covered.": [],
        "Any other task as directed by the account management team.": [],
        "Committed to the delivery of excellent customer service.": [
            [
                "customer service",
                "SOFT"
            ]
        ],
        "Calm manner, able to work under pressure.": [],
        "Able to make sound decisions when needed.": [],
        "Physically fit and able to carry tools and components by hand up to 20kg.": [
            [
                "20kg",
                "PERK"
            ]
        ],
        "Able to ascend and descend vertical access equipment. Able to work at heights.": [],
        "A team player, able to work with CBRE Managed Services and Client representatives at all levels.": [],
        "Good PC skills": [
            [
                "PC",
                "SOFT"
            ]
        ],
        "Good all round knowledge of general building works, diagnostic and repair procedures, working knowledge of building services systems and maintenance schedules.": [],
        "Formally trained in electrical/mechanical Maintenance.": [],
        "BMS Knowledge.": [
            [
                "BMS",
                "HARD"
            ]
        ],
        "Mechanical and commissioning experience.": [],
        "Recognised Health and Safety Qualification, such as IOSH.": [
            [
                "Recognised Health",
                "PERK"
            ],
            [
                "IOSH",
                "CERT"
            ]
        ],
        "Engineers must be able to demonstrate relevant knowledge of UPS, Generators, HV, PDU's, Crac units or equivalent, which will include the ability to fault find using the production of graphical information and the utilisation of current alarm data.": [
            [
                "alarm data",
                "SOFT"
            ]
        ],
        "Engineers must also understand the demarcation between maintainable assets and BPL maintained assets within laboratory and production environments.": [
            [
                "production",
                "SOFT"
            ]
        ],
        "Experience of maintaining all relevant mechanical and electrical equipment to clearly defined criteria.": [],
        "Good all round knowledge of general building works, diagnostic and repair procedures.": [],
        "Working knowledge of building services systems and maintenance schedules.": [],
        "Good verbal communication skills.": [
            [
                "verbal communication",
                "SOFT"
            ]
        ],
        "Good written English skills.": [
            [
                "written",
                "SOFT"
            ]
        ],
        "Must be willing to provide holiday and sickness cover at short notice and be flexible in their working patterns.": [],
        "Must have full UK driving license.": []
    },
    {
        "Staff Engineer": [],
        "Who we are & what we do": [],
        "Founded in 2016, Chetwood is a new kind of bank. We create targeted products to make people better off.": [
            [
                "2016",
                "CERT"
            ]
        ],
        "Unlike traditional banks, we\u2019re not looking to build a customer base and then cross-sell to them. Instead, we focus on distinct customer segments that are underserved by the market, creating innovative products to meet their needs. We do this through different brands, not one, because what works for one customer isn\u2019t necessarily right for all customers. We also do this for other industries and companies too, offering them white-labelled products with no upfront investment.": [],
        "We come from a diverse range of backgrounds and experiences, and it\u2019s because of this that we\u2019re able to challenge every aspect of how financial services operate today and build unique and dynamic products for our customers.": [],
        "We\u2019re always on the lookout for exceptional talent to join the team. We\u2019re looking for people who share our belief in challenging conventions and making a positive difference for our customers.": [],
        "Role Purpose": [],
        "We\u2019re looking for some exceptional software engineers to join the team at Chetwood as Staff Software Engineers.\u00a0": [],
        "As part of the engineering leadership you will contribute to the technology vision for Chetwood and help to set the bar for all of the engineering work we do.": [],
        "You will provide technical leadership for the engineering teams helping them to understand & align their work with achieving the business goals and working with them to ensure we design and build robust, reliable, secure systems. You\u2019ll be the driving force to solve the hardest technology challenges, improve engineering practices and support other engineers through coaching and mentoring to help them develop. ": [],
        "Responsibilities/Accountabilities": [],
        "Skills and experience\u00a0": [],
        "Technologies we use:": [],
        "What we offer": [],
        "Chetwood Highlights": [],
        "Chetwood Financial Ltd does not accept speculative or unsolicited CVs from Recruitment Agencies.": [],
        "Any unsolicited CVs received will be treated as the property of Chetwood Financial and Terms & Conditions associated with the use of such CVs will be considered null and void.": [],
        "Design, and implement highly scalable, reliable, maintainable, and secure serverless backend applications.": [
            [
                "backend",
                "SOFT"
            ]
        ],
        "Mentor and lead other engineers to build and maintain scalable, performant, and reliable asynchronous and event-driven systems that are easily testable and maintainable.": [
            [
                "asynchronous",
                "SOFT"
            ]
        ],
        "Collaborate with and lead cross-functional teams to design and implement new features and functionality.": [],
        "Collaborate with front-end and mobile developers to design APIs and ensure seamless integration of services.": [
            [
                "front-end",
                "SOFT"
            ],
            [
                "APIs",
                "SOFT"
            ]
        ],
        "Participate in code and design reviews, and provide guidance to other team members.": [],
        "Use your expert knowledge and experience to lead architectural discussions with your teams, and contribute to the overall System Architecture.\u00a0": [],
        "Help your teams manage the trade-offs when shipping new products and features to market \u2013 balancing business priorities, polish, and customer needs": [],
        "Work closely with the product team to understand business requirements and translate them into engineering designs and solutions with the appropriate trade-offs.": [],
        "Ensure that solutions being built are observable, secure and operable for the team to be able to manage system health and availability": [],
        "Lead by example, raising the bar for software engineering, across all disciplines throughout the SDLC, ensuring that systems are delivered in line with our engineering principles and using recognised design patterns.": [],
        "Support teams in adopting and embedding good software engineering practices / standards and implementing metrics to monitor them.": [
            [
                "embedding",
                "SOFT"
            ],
            [
                "software engineering",
                "SOFT"
            ]
        ],
        "Promote agile ways of working and continuous improvements across teams improving overall performance": [],
        "Promote the benefits of good engineering practices with senior stakeholders across the organisation": [
            [
                "engineering",
                "CERT"
            ]
        ],
        "Invest in automation across all aspects of the work we do to ensure practices and processes are repeatable, controlled and reliable.\u00a0": [
            [
                "automation",
                "SOFT"
            ]
        ],
        "Recognise the importance of the data that systems generate and consume and strive to ensure it is well-described, accurate, complete, reliable, relevant and timely.": [],
        "Technology - you can talk for hours about serverless, Python, Containers, and AWS and want to be on the forefront of FinTech and Banking-as-a-Service revolution": [
            [
                "Technology",
                "SOFT"
            ],
            [
                "Python",
                "HARD"
            ],
            [
                "AWS",
                "HARD"
            ],
            [
                "FinTech",
                "SOFT"
            ]
        ],
        "Software engineering - proficient in Python (or JS / TypeScript, Java, Kotlin, etc.), have used different programming languages & technologies and are passionate about writing high quality code": [
            [
                "Python",
                "SOFT"
            ],
            [
                "JS",
                "HARD"
            ],
            [
                "TypeScript",
                "HARD"
            ],
            [
                "Java",
                "HARD"
            ],
            [
                "Kotlin",
                "HARD"
            ],
            [
                "quality",
                "SOFT"
            ]
        ],
        "Experience with building event-driven systems and experience with event-driven messaging patterns and techniques like Event Sourcing and CQRS": [],
        "Experience with serverless architecture, design patterns, and best practices.": [],
        "Software design - you have gained an understanding of different software architectures, software engineering principles and patterns (e.g. microservices, distributed systems) with non-functional and security considerations are not an afterthought.": [
            [
                "Software design",
                "SOFT"
            ],
            [
                "software architectures",
                "SOFT"
            ],
            [
                "distributed systems",
                "SOFT"
            ],
            [
                "non-functional",
                "SOFT"
            ]
        ],
        "Experience and strong understanding of AWS services such as DynamoDB, Step Functions, API Gateway, Kinesis, Lambda, AWS SAM and CodePipeline / Deploy.": [
            [
                "AWS",
                "HARD"
            ],
            [
                "DynamoDB",
                "HARD"
            ],
            [
                "API Gateway",
                "SOFT"
            ],
            [
                "Kinesis",
                "HARD"
            ],
            [
                "Lambda",
                "HARD"
            ],
            [
                "AWS SAM",
                "HARD"
            ],
            [
                "CodePipeline",
                "HARD"
            ]
        ],
        "Familiarity with React and React Native ecosystem, though front-end development is not core to the role.": [
            [
                "React",
                "HARD"
            ],
            [
                "React",
                "HARD"
            ],
            [
                "front-end development",
                "SOFT"
            ]
        ],
        "Good communication skills - you are able to communicate with engineering teams and other stakeholders so that they understand and buy in to your proposals": [
            [
                "communication skills",
                "SOFT"
            ]
        ],
        "Problem solving - you enjoy the challenge of solving complex engineering problems and designing & implementing large scale systems (and the challenges this can entail)": [
            [
                "Problem solving",
                "SOFT"
            ],
            [
                "scale systems",
                "SOFT"
            ]
        ],
        "Automation & efficiency - you look for ways to increase productivity and flow by removing manual work from both engineering teams and the solutions that the teams build.": [
            [
                "Automation",
                "SOFT"
            ],
            [
                "flow by",
                "SOFT"
            ]
        ],
        "Quality mindset - you care about building high quality systems and have experience of building a quality culture in teams\u00a0": [
            [
                "quality",
                "SOFT"
            ]
        ],
        "Care - you understand that people are the heart of our engineering team and enjoy coaching / mentoring others to help them grow and develop their skills and progress their careers": [
            [
                "Care",
                "SOFT"
            ]
        ],
        "Cloud - we use AWS with a preference for native and serverless services (i.e. API Gateway, AWS Lambda, AWS Step Functions, Kinesis and DynamoDB)\u00a0": [
            [
                "Cloud",
                "SOFT"
            ],
            [
                "AWS",
                "HARD"
            ],
            [
                "API Gateway",
                "SOFT"
            ],
            [
                "AWS Lambda",
                "HARD"
            ],
            [
                "AWS Step",
                "HARD"
            ],
            [
                "Kinesis",
                "HARD"
            ],
            [
                "DynamoDB",
                "HARD"
            ]
        ],
        "Programming languages - mainly Python for software engineering and Terraform, AWS SAM and Cloud Formation for IaC": [
            [
                "Python",
                "HARD"
            ],
            [
                "Terraform",
                "HARD"
            ],
            [
                "AWS SAM",
                "HARD"
            ],
            [
                "Cloud",
                "SOFT"
            ],
            [
                "IaC",
                "SOFT"
            ]
        ],
        "Database - a mixture of SQL (Postgres) and NoSQL (DynamoDB)": [
            [
                "Database",
                "SOFT"
            ],
            [
                "SQL",
                "HARD"
            ],
            [
                "Postgres",
                "HARD"
            ],
            [
                "NoSQL",
                "HARD"
            ],
            [
                "DynamoDB",
                "HARD"
            ]
        ],
        "APIs - mainly RESTful APIs with AWS API Gateway": [
            [
                "APIs",
                "SOFT"
            ],
            [
                "RESTful",
                "SOFT"
            ],
            [
                "AWS API Gateway",
                "HARD"
            ]
        ],
        "CICD - mixture of GitHub Actions, AWS CodePipeline, Jenkins and tools including Trivy, Bandit, Sonarqube, Dependabot among others": [
            [
                "GitHub",
                "HARD"
            ],
            [
                "AWS CodePipeline",
                "HARD"
            ]
        ],
        "Big data - using native AWS services like S3, Redshift, Glue, Kinesis": [
            [
                "Big data",
                "SOFT"
            ],
            [
                "AWS",
                "HARD"
            ],
            [
                "S3",
                "HARD"
            ],
            [
                "Redshift",
                "HARD"
            ],
            [
                "Glue",
                "HARD"
            ],
            [
                "Kinesis",
                "HARD"
            ]
        ],
        "A relaxed, sociable and flexible working environment, included hybrid working options.\u00a0": [
            [
                "hybrid working",
                "PERK"
            ]
        ],
        "Great benefits including: life insurance, pension, private medical insurance (including dental and optical), free breakfast, snacks and drinks, monthly social events, cycle to work scheme, birthday day off, up to 30 days of holidays (depending on years of service), training and conferences arrangements": [
            [
                "pension",
                "PERK"
            ],
            [
                "30 days of holidays (depending on years of service)",
                "PERK"
            ]
        ],
        "Competitive salary and annual bonus scheme\u00a0": [
            [
                "annual bonus scheme",
                "PERK"
            ]
        ],
        "The opportunity to work for a progressive and exciting company.": [],
        "Here are just a few examples of what we\u2019ve achieved so far and what\u2019s coming soon.\u00a0": [],
        "Secured strategic investment from Elliott Advisors of \u00a3150mil of capital, underpinning the planned growth of the business over the next few years.\u00a0": [],
        "Secured a full banking licence from the PRA in 2018 - the only retail bank to do so that year.": [],
        "Launched the LiveLend Reward Loan; the world\u2019s first dynamic loan that responds to improvements in customers\u2019 credit score.\u00a0": [],
        "Secured several distribution partnerships with our lending product and have already seen 400%+ growth in new business since 2019.\u00a0": [],
        "Launched SmartSave; providing customers with a simple, online savings account at a great rate.\u00a0": [
            [
                "Launched SmartSave; providing customers with a simple",
                "PERK"
            ]
        ],
        "Achieved and maintained an \u2018Excellent\u2019 Trustpilot rating from both lending and savings customers.": []
    },
    {
        "Your new role A large government organisation is looking to recruit a Senior Data Engineer to design, implement, test and operate their data products, pipelines and services. You will work in a cross-functional, agile, team to integrate them into wider systems and business processes.  While you will have opportunities to contribute to the wider solution to satisfy their user needs, the team will rely on your ability to lead development of data products, to optimise the code, and help with database management. You will ensure internal and industry standards are followed in delivery of reusable solutions. You will help more junior team members to gain your level of expertise, and ensure the teams are working to the standards set by the lead data engineer. You will be confident in using languages like PostgreSQL, Python, Django, Airflow and have a deeper understanding of how they work.What you'll need to succeed ": [
            [
                "pipelines",
                "SOFT"
            ],
            [
                "database management",
                "SOFT"
            ],
            [
                "PostgreSQL",
                "HARD"
            ],
            [
                "Python",
                "HARD"
            ],
            [
                "Django",
                "HARD"
            ],
            [
                "Airflow",
                "SOFT"
            ]
        ],
        "What you need to do nowIf you're interested in this role, click 'apply now' to forward an up-to-date copy of your CV, or call us now.If this job isn't quite right for you but you are looking for a new position, please contact us for a confidential discussion on your career.": [],
        "Hays Specialist Recruitment Limited acts as an employment agency for permanent recruitment and employment business for the supply of temporary workers. By applying for this job you accept the T&C's, Privacy Policy and Disclaimers which can be found at hays.co.uk": [],
        "Agile methodologies. You can lead work following agile methodologies on data projects.": [
            [
                "Agile",
                "SOFT"
            ]
        ],
        "Data development process. You can lead design, build and test of data products based on feeds from multiple systems using a range of different storage technologies and/or access methods. You create repeatable, reusable products.": [
            [
                "Data development",
                "SOFT"
            ]
        ],
        "Data integration design. You know how to select and implement the appropriate technologies to deliver resilient, scalable and future-proofed data solutions.": [
            [
                "Data integration",
                "SOFT"
            ],
            [
                "data solutions",
                "SOFT"
            ]
        ],
        "Data modelling. You understand the concepts and principles of data modelling and can produce relevant data models across multiple subject areas. You know how to reverse-engineer data models from a live system. You understand industry-recognised data modelling patterns and standards and when to apply them. You can compare and align different data models.": [
            [
                "Data modelling",
                "SOFT"
            ],
            [
                "data modelling",
                "SOFT"
            ]
        ],
        "Problem resolution (data). You know how to respond to problems in databases, data processes, data products and services as they occur. You can initiate actions, monitor services and identify trends to resolve problems. You can determine the appropriate remedy and assist with implementation of it as well as preventative measures.": [
            [
                "Problem resolution (data)",
                "SOFT"
            ],
            [
                "databases",
                "SOFT"
            ],
            [
                "data processes",
                "SOFT"
            ]
        ],
        "Programming and build (data engineering). You know how to use agreed standards and tools to design, code, test, correct and document moderate-to-complex programs and scripts from agreed specifications and subsequent iterations. You can collaborate with others to review specifications where appropriate.": [
            [
                "scripts from",
                "SOFT"
            ]
        ],
        "Technical understanding (data engineering). You understand core technical concepts like data normalisation, modelling, performance of data intensive solutions. You understand basic software development concepts.": [
            [
                "data normalisation",
                "SOFT"
            ],
            [
                "modelling",
                "SOFT"
            ],
            [
                "data intensive",
                "SOFT"
            ],
            [
                "software development",
                "SOFT"
            ]
        ]
    },
    {
        "Role OVO-View": [],
        "Location: Hub based! Bristol, London, Glasgow or Remote (You have the flexibility to work wherever suits you best)": [],
        "Team: Information Security ": [],
        "Salary banding: \u00a331K-\u00a369K": [
            [
                "Salary banding: \u00a331K-\u00a369",
                "PERK"
            ]
        ],
        "Experience: Mid-level": [],
        "Working pattern: Full-Time": [],
        "Reporting to: Raymond Fenton": [],
        "Sponsorship: Unfortunately we are unable to offer sponsorship for this role.": [],
        "This role in 3 words: Build, Operate, Discover": [],
        "Top 3 qualities for this role: Stakeholder Engagement, Problem Solving, Data Driven": [
            [
                "Problem Solving",
                "SOFT"
            ]
        ],
        "In the words of the team, you should leave your current role for this one because\u2026.": [],
        "\u201cWe\u2019re building a leading edge data driven business intelligence function in Information Security and we want you to help define and build this future\u201d": [],
        "Everyone belongs at OVO": [],
        "At OVO, we are on a mission to solve one of humanity's biggest challenges, the climate crisis. And we know it takes all of us to change the world. That's why we need diverse people from all gender identities, ethnicities, ages, sexual orientations, life experiences and backgrounds to join us.": [],
        "Teamworking for the planet": [],
        "Everything we do here spins around Plan Zero. So, naturally, the team you\u2019ll be joining plays a gigantic role in making that happen. Here\u2019s how:": [],
        "Trust is key to Plan Zero success. We're looking for people who are are passionate about the role information security plays in building and safeguarding trust and about driving progress to net zero carbon living.": [],
        "This role in a nutshell:": [
            [
                "nutshell",
                "SOFT"
            ]
        ],
        "Your key outcomes will be:": [],
        "Within your first 6 months month you\u2019ll:": [],
        "Systems: Graph Databases, Cypher Query Language, Tableau, Excel/Google Sheets": [
            [
                "Graph Databases",
                "SOFT"
            ],
            [
                "Query",
                "HARD"
            ],
            [
                "Tableau",
                "HARD"
            ],
            [
                "Excel",
                "HARD"
            ],
            [
                "Google Sheets",
                "HARD"
            ]
        ],
        "You\u2019ll be a successful InfoSec Data Engineer at OVO if you\u2026": [],
        "You won\u2019t like this role if you\u2026": [],
        "Let\u2019s talk about what\u2019s in it for you": [],
        "We\u2019ll pay you between \u00a331,000 and \u00a349,000, depending on your specific skills and experience. If your expectations are a little different, have a chat with us!": [],
        "We keep our pay ranges broad on purpose to give us, and you, flexibility to match your experience to our zero carbon mission.": [],
        "You\u2019ll be eligible for an on-target bonus of 15%. We have one OVO bonus plan that focuses on the collective performance of our people to deliver our Plan Zero goal. ": [
            [
                "15%. We have one OVO bonus plan",
                "PERK"
            ]
        ],
        "We also offer plenty of green benefits and progressive policies to help you feel like you belong at OVO\u2026and there\u2019s flex pay. It\u2019s an extra 9% of your salary on top of your core pay to use as you like. You can take it as cash, add to your pension, or choose to spend it on a huge range of flex benefits. Here\u2019s a taster of what\u2019s on offer: ": [
            [
                "9% of your salary on top of your core pay",
                "PERK"
            ]
        ],
        "For starters, you\u2019ll get 34 days of holiday (including bank holidays). For your healthWith benefits like a healthcare cash plan or private medical insurance depending on your career level, critical illness cover, life assurance, health assessments, and moreFor your wellbeingWith gym membership, gadget, travel and cyber insurance, workplace ISA, will writing services, DNA testing, dental insurance, and more For your lifestyle With extra holiday buying, discount dining, culture cards, tech loans, and supporting your favourite charities with give-as-you-earn donationsFor your home Get up to \u00a3300 off any OVO Energy plan (when you pay by Direct Debit), plus personal carbon offsetting and great discounts on smart thermostats and EV chargersFor your commute Nab a great deal on ultra-low emission car leasing, plus our cycle to work scheme and public transport season ticket loans Want to hear about our full range of flexible benefits and progressive people policies? Our People Team can tell you everything you need to know.": [
            [
                "dental insurance",
                "PERK"
            ],
            [
                "holiday buying",
                "PERK"
            ],
            [
                "benefits and progressive people policies? Our People Team can tell you everything you need to know",
                "PERK"
            ]
        ],
        "For your Belonging": [],
        "To find better ways to support our people, we need to listen to each other\u2019s experiences and find ways to build a truly inclusive and diverse workplace. As part of this, we have 8 Belonging Networks at OVO. Led by our people, for our people - so when you join OVO, you can play a part - big or small - with any of the Networks. It's up to you.": [],
        "Oh, and one last thing...": [],
        "We\u2019d be thrilled if you tick off all our boxes yet we also believe it\u2019s just as important we tick off all of yours. And if you think you have most of what we\u2019re looking for but not every single thing, go ahead and hit apply. We\u2019d still love to hear from you!": [],
        "If you have any additional requirements, there\u2019s a space to let us know on the application form; we want to make the process as easy and comfortable for you as possible..": [],
        "Working closely with stakeholders to understand the various source systems including details of their interfaces, data models and capabilities.": [],
        "Defining and building effective monitoring for data platform health.": [],
        "Using Graph DB & Graph Query to build an end to end schematic to understand the systems, flow of data, controls and internal inter-dependencies capable of capturing a changing architecture to identify gaps.": [
            [
                "Using Graph DB",
                "SOFT"
            ],
            [
                "Graph Query",
                "HARD"
            ]
        ],
        "Working with security tools and cloud technologies, AWS, GCP": [
            [
                "cloud",
                "SOFT"
            ],
            [
                "AWS",
                "HARD"
            ],
            [
                "GCP",
                "HARD"
            ]
        ],
        "Getting the right data in the right place at the right time": [],
        "Keeping core data system running and stable": [
            [
                "core data",
                "SOFT"
            ]
        ],
        "Experimenting to find more efficient ways to manage and exploit our dataWithin your first 6 months month you\u2019ll:": [],
        "Understood the data models in play": [],
        "Defined and identified data gaps and have a plan to fill them": [],
        "Built new data connections": [
            [
                "data",
                "SOFT"
            ]
        ],
        "Communicated the road map to deliver future requirements": [],
        "You can consult with relevant stakeholders to define goals.": [],
        "You are able to present your findings with remediation options and data based recommendations.": [
            [
                "remediation",
                "SOFT"
            ]
        ],
        "You can collaborate with co-workers and management to implement improvements.": [],
        "You are comfortable working in an agile development environment that employs continuous integration and continuous delivery best practices, and have experience of Graph DB & Graph Query.": [
            [
                "agile development",
                "SOFT"
            ],
            [
                "integration",
                "SOFT"
            ],
            [
                "Graph Query",
                "HARD"
            ]
        ],
        "You love taking complex data sources and providing insight into their construction, interaction and performance levels.": [],
        "You have strong analytical and critical thinking skills.": [
            [
                "analytical",
                "SOFT"
            ],
            [
                "critical thinking",
                "SOFT"
            ]
        ],
        "You seek learning opportunities to deepen your expertise or broaden your knowledge.": [],
        "You are excited to advance OVO\u2019s plan zero initiative.": [],
        "You are keen to learn about or have knowledge of cyber security.": [],
        "Experience of implementing changes within a change programme environment, focussing on milestone deliveries.": [],
        "Experience designing and building large-scale solutions to simplify, bring understanding and change solutions in large companies.": [],
        "You struggle with ambiguity and non perfect data sets": [
            [
                "ambiguity",
                "SOFT"
            ]
        ],
        "You can\u2019t work in cross functional teams with multiple priorities": [],
        "You like each day to be the same": []
    },
    {
        "Data Engineer - Up to 55k (D.O.E)": [],
        "A financial services client of mine is actively looking for a Data Engineer, with big ambitions to create a positive impact in their company - it's an exciting time to join them!": [],
        "The ideal Data engineer will bring the ability to focus on getting the job done, will be a DWH expert and will be act as a key figure in developing this practice within the business.": [],
        "What you'll do:": [],
        "You will need to work across departments to understand what business leaders want to gain from their datasets. You will be expected to create and/or validate Pipelines to support service level artifact promotion such as Power BI Dashboards. You will aid with preparations prior to, and during, any Technical Design Authority approvals.": [
            [
                "validate Pipelines",
                "SOFT"
            ],
            [
                "Power BI",
                "HARD"
            ]
        ],
        "What you'll need:": [],
        "Commercial experience with SQL or Python Experience with DBT or similar technologies like Snowflake Demonstrable experience of data integration and pipeline-centric development (deployed in an Azured environment is highly favoured). Excellent communication skills - verbal and written.": [
            [
                "SQL",
                "HARD"
            ],
            [
                "Python",
                "HARD"
            ],
            [
                "Snowflake",
                "HARD"
            ],
            [
                "data integration",
                "SOFT"
            ],
            [
                "pipeline",
                "SOFT"
            ],
            [
                "communication skills",
                "SOFT"
            ],
            [
                "verbal",
                "SOFT"
            ],
            [
                "written",
                "SOFT"
            ]
        ],
        "Some of the benefits include:": [],
        "Discretionary Bonus Bupa Private medical insurance - Family cover Pension scheme scaling up to - 16% This is a full-time role that offers hybrid working arrangement - 2 days onsite in their Epsom office after the on boarding period.": [
            [
                "Discretionary Bonus Bupa Private medical insurance",
                "PERK"
            ],
            [
                "Pension scheme scaling up to - 16% This is a full-time role",
                "PERK"
            ],
            [
                "hybrid working",
                "PERK"
            ]
        ],
        "If this role sounds of interest, please send your CV ": []
    },
    {
        "Data Engineer": [],
        "Location:\u00a0Remote": [],
        "Salary:\u00a0\u00a360K to \u00a370K plus bonus plus benefits including training and development, pension, private medical": [
            [
                "plus bonus",
                "PERK"
            ],
            [
                "development",
                "SOFT"
            ],
            [
                "pension",
                "PERK"
            ]
        ],
        "Data Engineer with GCP, BigQuery, Python, SQL skills required to join exciting media business working on a new modern data stack.": [
            [
                "GCP",
                "HARD"
            ],
            [
                "BigQuery",
                "HARD"
            ],
            [
                "Python",
                "HARD"
            ],
            [
                "SQL",
                "HARD"
            ],
            [
                "data stack",
                "SOFT"
            ]
        ],
        "You will lead ETL solutions for the delivery of BI and data warehousing projects as part of the technology and digital team. They are a data focused organisation and are putting in new systems to enable them to improve services and continue growth.": [
            [
                "ETL",
                "SOFT"
            ],
            [
                "BI",
                "SOFT"
            ],
            [
                "data warehousing",
                "SOFT"
            ]
        ],
        "They require:": [],
        "Strong SQL, BigQuery & dbt knowledge gained in a production environment.": [
            [
                "SQL",
                "HARD"
            ],
            [
                "BigQuery",
                "HARD"
            ]
        ],
        "Experience of building scalable ETL / ELT pipelines": [
            [
                "ETL",
                "SOFT"
            ],
            [
                "ELT pipelines",
                "SOFT"
            ]
        ],
        "Knowledge and strong interest to work with modern cloud-based data architectures and platforms built on Google Cloud Platform - GCP, BigQuery & Kubernetes.": [
            [
                "cloud",
                "SOFT"
            ],
            [
                "Google Cloud",
                "HARD"
            ],
            [
                "GCP",
                "HARD"
            ],
            [
                "BigQuery",
                "HARD"
            ],
            [
                "Kubernetes",
                "HARD"
            ]
        ],
        "Python programming \u2013 able to perform complex transformations with commercial data.": [
            [
                "Python",
                "HARD"
            ]
        ],
        "Source Control, CI/CD and deployment through CI Pipelines (Azure DevOps)": [
            [
                "CI/CD",
                "SOFT"
            ],
            [
                "CI Pipelines",
                "SOFT"
            ],
            [
                "Azure DevOps",
                "HARD"
            ]
        ],
        "Familiar with modern data tools and technologies like: Airflow, Terraform, Docker, Azure DevOps, Cloud Dataflow, Fivetran.": [
            [
                "data tools",
                "SOFT"
            ],
            [
                "Airflow",
                "SOFT"
            ],
            [
                "Terraform",
                "HARD"
            ],
            [
                "Docker",
                "HARD"
            ],
            [
                "Azure DevOps",
                "HARD"
            ],
            [
                "Cloud Dataflow",
                "SOFT"
            ],
            [
                "Fivetran",
                "HARD"
            ]
        ],
        "Experience of Datawarehouse and BI modelling implementations like Kimball\u2019s modelling techniques for data marts.": [
            [
                "Datawarehouse",
                "HARD"
            ],
            [
                "BI modelling",
                "SOFT"
            ],
            [
                "modelling",
                "SOFT"
            ]
        ],
        "Experience data modelling for BI implementations needed for a variety of traditional and cloud based BI and reporting tools (Looker ideally)": [
            [
                "data modelling",
                "SOFT"
            ],
            [
                "BI",
                "SOFT"
            ],
            [
                "cloud based BI",
                "SOFT"
            ],
            [
                "reporting tools",
                "SOFT"
            ]
        ],
        "Excellent communication, analytical and problem-solving skills.": [
            [
                "communication",
                "SOFT"
            ],
            [
                "analytical",
                "SOFT"
            ],
            [
                "problem-solving",
                "SOFT"
            ]
        ]
    },
    {
        "Amazon Transportation Services seeks a Sr Manager, TPM that combines strategic vision with collaborative and analytical skills to help building the the EU Middle Mile Transportation data reporting and analytics infrastructure. In this role you will contribute, as part of the Data and Analytics (DnA) team to define the data strategy across all business lines, owning the portfolio of tech products and collaborating with business and tech stakeholders to define priorities and create effective roadmaps. You will work closely with a cross-disciplinary team of product and program managers, business intelligence engineers and business analysts, data engineers and process improvement specialists.": [
            [
                "Amazon Transportation",
                "SOFT"
            ],
            [
                "analytical",
                "SOFT"
            ],
            [
                "analytics",
                "SOFT"
            ]
        ],
        "We are looking for a Sr Manager who is highly analytical, resourceful, customer-focused, and team-oriented with excellent written and verbal communication skills to drive decisions at the senior executive level. A successful candidate should thrive in a highly collaborative, creative, and fast-paced environment with a proven track record in taking on end-to-end ownership and successfully delivering results. You would be a great fit for this role if you enjoy and excel at:": [
            [
                "analytical",
                "SOFT"
            ],
            [
                "customer-focused",
                "SOFT"
            ],
            [
                "written",
                "SOFT"
            ],
            [
                "verbal communication",
                "SOFT"
            ],
            [
                "fast-paced",
                "SOFT"
            ]
        ],
        "We are open to hiring candidates to work out of one of the following locations:": [],
        "London, UK": [],
        "Luxembourg, LU": [],
        "London, GBR": [],
        "BASIC QUALIFICATIONS": [
            [
                "BASIC",
                "HARD"
            ]
        ],
        "PREFERRED QUALIFICATIONS": [],
        "Amazon is an equal opportunities employer. We believe passionately that employing a diverse workforce is central to our success. We make recruiting decisions based on your experience and skills. We value your passion to discover, invent, simplify and build. Protecting your privacy and the security of your data is a longstanding top priority for Amazon. Please consult our Privacy Notice to know more about how we collect, use and transfer the personal data of our candidates.": [],
        "Our inclusive culture empowers Amazonians to deliver the best results for our customers. If you have a disability and need an adjustment during the application and hiring process, including support for the interview or onboarding process, please contact the Applicant-Candidate Accommodation Team (ACAT), Monday through Friday from 7:00 am GMT - 4:00 pm GMT.": [],
        "Thinking strategically and developing 3-year plans across different organizations;": [],
        "Developing and writing technical proposals and business plans, gaining approval from stakeholders (e.g., adjacent product managers, legal, PR) and leadership to move those plans forward;": [],
        "Collaborating with BI and product managers as well as operations teams, user experience teams, and central support functions, to create product vision and road map for reporting tools;": [
            [
                "BI",
                "SOFT"
            ]
        ],
        "Building strategies to improve data governance and discoverability.": [],
        "Master's Degree in STEM disciplines": [
            [
                "STEM",
                "CERT"
            ]
        ],
        "10+ years experience managing technical teams": [
            [
                "10+ years experience managing technical",
                "PERK"
            ]
        ],
        "Experience in product management, coding, data infrastructure and AWS platforms": [
            [
                "product management",
                "SOFT"
            ],
            [
                "data infrastructure",
                "SOFT"
            ],
            [
                "AWS",
                "HARD"
            ]
        ]
    },
    {
        "Data Engineer": [],
        "The Data Engineer will be part of a diverse and talented team, managing the lifecycle of Data Pipelines, which are vital to our UK Data Strategy. This involves creating scalable processes that meet business needs, support our company initiatives, and align with our goal of using data effectively.": [
            [
                "Data Pipelines",
                "SOFT"
            ]
        ],
        "Key tasks include:- ": [],
        "You should have:": [],
        "AbbVie is committed to operating with integrity, driving innovation, transforming lives, serving our community, and embracing diversity and inclusion. It is AbbVie\u2019s policy to employ qualified persons of the greatest ability without discrimination against any employee or applicant for employment because of race, color, religion, national origin, age, sex (including pregnancy), physical or mental disability, medical condition, genetic information, gender identity or expression, sexual orientation, marital status, status as a protected veteran, or any other legally protected group status.": [],
        "Travel: Yes, 5 % of the Time": [
            [
                "5 % of the Time",
                "PERK"
            ]
        ],
        "Job Type: Experienced": [],
        "Schedule: Full-time": [],
        "Working closely with Data Analysts and Global Data Product Managers to deliver data requirements.": [],
        "Leading projects that fit with our local and global Data Strategies.": [],
        "Building scalable ELT and Data Pipelines to meet business needs and our data strategy. This data will be used for machine learning, analytics, and business intelligence.": [
            [
                "ELT",
                "SOFT"
            ],
            [
                "Data Pipelines",
                "SOFT"
            ],
            [
                "machine learning",
                "SOFT"
            ],
            [
                "analytics",
                "SOFT"
            ]
        ],
        "Collaborating with engineers worldwide to ensure a collective approach to delivery.": [],
        "Gaining a comprehensive understanding of data lineages and data objects within the Palantir Foundry system. This will help improve our processes, data quality, and integrity.": [
            [
                "data quality",
                "SOFT"
            ]
        ],
        "Continually checking data quality and system observations.": [
            [
                "data quality",
                "SOFT"
            ]
        ],
        "Implementing Data Engineering standards and best practices.": [],
        "Making sure data is available through performance tuning and best practice implementation.": [],
        "Collaborating effectively with both technical and non-technical team members.": [],
        "Developing technical documentation.": [],
        "Bachelor\u2019s degree or equivalent technical experience, or a Master's degree plus 5+ years experience. However, if you have enough experience to demonstrate technical equivalency, this is also acceptable.": [],
        "Experience as a Business System Analyst.": [],
        "Experience with Systems Testing and IT Implementation.": [
            [
                "Systems Testing",
                "SOFT"
            ],
            [
                "IT Implementation",
                "SOFT"
            ]
        ],
        "Project Management experience.": [
            [
                "Project Management",
                "SOFT"
            ]
        ],
        "Engineering experience. ": [
            [
                "Engineering",
                "SOFT"
            ]
        ]
    },
    {
        "About the team": [],
        "Product Management at Deliveroo takes many forms, and involves many facets. We\u2019re a consumer-facing brand, with websites and mobile apps which help customers find great quality food which can be brought to them to satisfy their needs any time day or night. We\u2019re a logistics company, directing a fleet of drivers around major cities across the globe using a dedicated native app. We\u2019re a partner to restaurants, who see us as a way to unlock access to new customers and thus maximise the revenue of their existing business. And we\u2019re a home for numerous internal teams building internally facing tools for Marketing, Promotions, Care, finance, and others, to keep the operation running effectively.": [
            [
                "quality",
                "SOFT"
            ]
        ],
        "Life as a product manager can mean working in any or all of these areas, in order to deliver value to customers, drivers, and partners. Day to day, you\u2019ll tackle new, interesting business problems, and find innovative, creative ways to solve them. We\u2019re seeking brilliant and motivated Product Managers to help us keep moving things forward.": [],
        "We are looking for someone to lead the Market and User Insights squad within the Market Intelligence team. The Market Intelligence team\u2019s mission is to provide a range of business stakeholders, from GMs to account managers and other product teams, with market/competitor insights and data so that they can make better decisions to grow the business. The insights produced by the team are crucial to powering the work of many other teams (sourcing leads, setting pricing strategy, setting selection strategy etc.), and the team has high visibility with execs. This role sits within the Consumer Product Group. ": [],
        "What you'll be doing": [],
        "Requirements": [],
        "Why Deliveroo": [],
        "Our mission is to be the definitive food company. We are transforming the way the world eats by making food more convenient and accessible. We give people the opportunity to eat what they want, as they want it, when and where they want it.": [],
        "We are a technology-driven company at the forefront of the most rapidly expanding industry in the world. We are still a small team, making a very large impact, looking to answer some of the most interesting questions out there. We move fast, value autonomy and ownership, and we are always looking for new ideas.": [],
        "Workplace & Diversity": [],
        "At Deliveroo we know that people are the heart of the business and we prioritise their welfare. We offer a wide range of competitive benefits in areas including health, family, finance, community, convenience, growth, time away and relocation.": [],
        "We believe a great workplace is one that represents the world we live in and how beautifully diverse it can be. That means we have no judgement when it comes to any one of the things that make you who you are - your gender, race, sexuality, religion or a secret aversion to coriander. All you need is a passion for (most) food and a desire to be part of one of the fastest growing startups in an incredibly exciting space.": [],
        "You\u2019ll deeply understand our business needs around insights on competitor performance, demographic data and app engagement insights and develop a roadmap to solve for them": [
            [
                "data",
                "SOFT"
            ]
        ],
        "You\u2019ll partner with data scientists and data engineers to build insights/tools and data products to power better decision making by business stakeholders, other product teams and various systems": [],
        "You\u2019ll lead delivery of work end to end, from defining requirements to driving user adoption. This includes leading data procurement, ongoing improvements and data quality management.": [
            [
                "data quality",
                "SOFT"
            ]
        ],
        "You are a strong problem solver and can tackle ambiguous problems ": [
            [
                "problem solver",
                "SOFT"
            ]
        ],
        "You are highly detail-oriented and analytical, and love working with data": [
            [
                "analytical",
                "SOFT"
            ],
            [
                "data",
                "SOFT"
            ]
        ],
        "You have experience leading a technical team and are able to articulate the work succinctly to business stakeholders": [],
        "You have experience in product prioritisation, building a roadmap, and delivery of user-facing functionality": [
            [
                "user-facing",
                "SOFT"
            ]
        ],
        "You can manage a broad set of stakeholders with strong written and verbal communication skills.": [
            [
                "written",
                "SOFT"
            ],
            [
                "verbal communication",
                "SOFT"
            ]
        ],
        "You can juggle multiple projects and shifting priorities": [],
        "You thrive in an environment where impact and speed are paramount.": [],
        "You have experience working with data and insights vendors": []
    },
    {
        "Posted 1 days ago": [],
        "Apply online now": [],
        "At Centrica our purpose is to help our customers live simply, sustainably and affordably. Achieving this is made possible through our family of trusted brands and businesses.": [],
        "We are seeking an accomplished leader to head our Data Engineering department, playing a pivotal role in architecting and managing data infrastructure, ensuring the reliability, scalability, and efficiency of our data pipelines. The Group Head of Data Engineering will be instrumental in enabling our data science and analytics teams to harness the power of data for strategic decision-making.": [
            [
                "data infrastructure",
                "SOFT"
            ],
            [
                "data pipelines",
                "SOFT"
            ]
        ],
        "The role holder will be required to work out of our London and Windsor offices approximately 3 days per week.": [],
        "Key responsibilities for the role include:": [],
        "The Person:": [],
        "#morethanacareer": [],
        "At Centrica we embrace diversity and actively seek to attract individuals with unique backgrounds and perspectives. To build a more sustainable future, we need the best team - a team with a diverse mix of people and skills, where everyone feels welcome and able to succeed. We are dedicated in helping to close the diversity gap and would love to see more females, people of colour and LGBTQ+ employees, as well as those from a variety of cultures and ethnicity to veterans and the differently abled. Supporting diversity and inclusion is a big part of who we are, we are not looking for people to fit into our culture but to add to it!": [],
        "PLEASE APPLY ONLINE by hitting the ' Apply ' button.": [],
        "Applications will ONLY be accepted via the 'Apply' button.": [],
        "This role is being handled by the Centrica recruitment team and NO agency contact is required.": [],
        "Home": [],
        "Careers": [],
        "Careers Search": [],
        "Group Head of Data Engineering": [
            [
                "Group Head of Data Engineering",
                "PERK"
            ]
        ],
        "Location: Windsor": [],
        "Country: GB": [],
        "Organisation:": [],
        "Category:": [],
        "Lead, mentor, and develop a high-performing data engineering team": [],
        "Define and execute the data engineering strategy in alignment with organizational goals": [],
        "Collaborate closely with cross-functional teams and data stakeholders to understand data requirements and ensure data accessibility": [
            [
                "data",
                "SOFT"
            ]
        ],
        "Oversee the design, implementation, and maintenance of robust data pipelines, ETL processes, and data warehouses": [
            [
                "data pipelines",
                "SOFT"
            ],
            [
                "ETL processes",
                "SOFT"
            ],
            [
                "data warehouses",
                "SOFT"
            ]
        ],
        "Develop data governance and data management policies to ensure data quality, consistency, and security": [
            [
                "data management",
                "SOFT"
            ],
            [
                "data quality",
                "SOFT"
            ]
        ],
        "Evaluate and select appropriate data storage, processing, and integration technologies": [
            [
                "data storage",
                "SOFT"
            ],
            [
                "integration",
                "SOFT"
            ]
        ],
        "Establish best practices for data engineering, ensuring scalability, performance, and cost-effectiveness": [],
        "Partner with data science and analytics teams to provide them with clean, high-quality data for analysis and modeling": [
            [
                "-quality data",
                "SOFT"
            ],
            [
                "analysis",
                "SOFT"
            ],
            [
                "modeling",
                "SOFT"
            ]
        ],
        "Monitor data infrastructure performance, troubleshoot issues, and implement optimizations as needed": [],
        "Stay current with emerging data engineering technologies and trends, and assess their applicability to the organization": [],
        "Work in close collaboration with the Group Head of Data Science to ensure that data infrastructure and data science capabilities are aligned, enabling the organization to harness data-driven insights effectively.": [
            [
                "data science",
                "SOFT"
            ],
            [
                "harness data",
                "SOFT"
            ]
        ],
        "A Master's or PhD in Computer Science, Data Engineering, Data Science, or a related field": [
            [
                "Computer Science",
                "CERT"
            ]
        ],
        "Significant experience in data engineering, with experience of also having been in a senior leadership role": [
            [
                "data engineering",
                "CERT"
            ]
        ],
        "Proficiency in data engineering tools, platforms, and languages relevant to our technology stack (e.g., Hadoop, Spark, Kafka, AWS, Azure)": [
            [
                "Hadoop",
                "HARD"
            ],
            [
                "Spark",
                "HARD"
            ],
            [
                "Kafka",
                "HARD"
            ],
            [
                "AWS",
                "HARD"
            ],
            [
                "Azure",
                "HARD"
            ]
        ],
        "Knowledge of containerization and orchestration tools (e.g., Docker, Kubernetes)": [
            [
                "containerization",
                "SOFT"
            ],
            [
                "orchestration tools",
                "SOFT"
            ],
            [
                "Docker",
                "HARD"
            ],
            [
                "Kubernetes",
                "HARD"
            ]
        ],
        "Experience with real-time data processing and streaming technologies": [
            [
                "real-time data processing",
                "SOFT"
            ]
        ],
        "Familiarity with data governance frameworks and compliance requirements": [
            [
                "data governance",
                "SOFT"
            ]
        ],
        "Proven track record of designing and implementing scalable, high-performance data pipelines and architectures": [
            [
                "data pipelines",
                "SOFT"
            ],
            [
                "architectures",
                "SOFT"
            ]
        ],
        "Strong problem-solving skills and the ability to troubleshoot and optimize data processes": [
            [
                "problem-solving",
                "SOFT"
            ],
            [
                "optimize data",
                "SOFT"
            ]
        ],
        "Excellent communication and collaboration skills, with the ability to work effectively across teams and convey complex technical concepts to non-technical stakeholders.": [
            [
                "communication",
                "SOFT"
            ],
            [
                "collaboration",
                "SOFT"
            ],
            [
                "non-technical stakeholders",
                "SOFT"
            ]
        ]
    },
    {
        "Who are we?": [],
        "Thames Water is the UK\u2019s largest water and wastewater company. We make a daily difference to our 15 million customers by supplying 2.6 billion litres of water through 32,000 km of pipes, keeping taps flowing and toilets flushing.": [
            [
                "Thames Water",
                "PERK"
            ],
            [
                "billion litres",
                "SOFT"
            ],
            [
                "32,000",
                "SOFT"
            ]
        ],
        "At Thames Water, every one of our actions, big and small, matters every day. Water is essential to life, so our business is always open.": [],
        "In Team Digital, we\u2019re planning for a future where the technology solutions we co-create and design enable us to achieve our goal to protect our greatest natural resource and allow our customers, communities and the environment to thrive.": [],
        "What you\u2019ll be doing": [],
        "As a team, our vision is to create an everyday digital experience for the people we serve - our customers - by putting them at the heart of everything we do. As part of our award-winning team, you\u2019ll help the business become an intelligent, connected organization to deliver our digital transformation and turnaround.": [],
        "The successful Data Engineer will be joining our Data Engineering and Platform team; this team develops and operates a sector-leading, modern data platform. Your expertise would be enabling dev teams downstream to develop ground-breaking data science and data products. This team is also the centre of excellence for data engineering in the organisation and your passion will help us expand Thames Water's data engineering capability further.": [
            [
                "data products",
                "SOFT"
            ]
        ],
        "Our Data Engineers work alongside data scientists, software developers, delivery leads and other disciplines; to complete data acquisition requests and evolve our leading-edge platforms and development environments. You would be working on stimulating technical challenges to support our continued transformation towards a data-driven business. The right candidate will be joining a close-knit and skilled team where you can learn from, bounce ideas off and support your colleagues.": [],
        "What you should bring to the role": [],
        "We want to bring together a team of brilliant tech minds with game-changing ideas. We\u2019re looking for people who will help us re-imagine the way we work and the way we get things done: ": [],
        "To excel in this role, you will likely be adept in the following:": [],
        "What\u2019s in it for you?": [],
        "It\u2019s an incredibly exciting time to join Team Digital, the best people to tell you why, are the people in our team. Take a look at the video below to find out what they think. ": [],
        "https://www.youtube.com/watch?v=wmELR_xmXhw&list=PL37gUk7p48rqLEm-KEkgKuyDpLYsW5LH9&index=12": [],
        "The role is varied, allowing you to take responsibility and ownership, think outside the box and have thoughts and ideas used to create and improve processes. This is a great opportunity to contribute to making improvements - our team thrive on being challenged and ensuring that we deliver the best outcomes for our customers and our business. We\u2019re proud of the positive ways of working we\u2019ve adopted during the pandemic, creating a more flexible and dynamic environment so all our colleagues can thrive.": [],
        "For our office-based roles, we\u2019re moving to a hybrid approach with various options across working from home, office and our sites. Our competitive salary package includes an excellent contributory pension, 26 days holiday per year increasing to 30 with a length of service and a wider benefits scheme including our benefits hub, which is packed full of offers and information to save you money and support your well-being.": [
            [
                "26 days holiday per year increasing to 30 with a length of service and a wider benefits scheme including our benefits hub, which is packed full of offers and information to save you money and support your well-being.",
                "PERK"
            ]
        ],
        "Thames Water is a dynamic, rewarding and diverse place to work, with opportunities around every corner. If you join our team, you\u2019ll enjoy a fulfilling career and flexible working arrangements. We\u2019re also proud to embrace and promote diversity and believe that creating a workforce that reflects the communities we serve will help us to thrive. We encourage applications from everyone and offer extra support for those who need it throughout the recruitment process.": [
            [
                "Thames Water",
                "PERK"
            ]
        ],
        "Find out more about working at Thames Water": [],
        "We deliver life's essential service so our customers, communities and the environment can thrive. This means, when a crisis happens, we all rally around to support our customers. As part of Team Thames, you\u2019ll have the opportunity to sign up to support our customers on the frontline as an Ambassador. Full training will be given for what is undoubtedly an incredibly rewarding experience. It\u2019s also a great opportunity to learn more about our business, meet colleagues and a earn bit of extra money along the way.": [],
        "Disclaimer: due to the high volume of applications we receive, we may close a vacancy earlier than the advertised date. This is so we can manage all the applications properly and give candidates a positive experience. Once closed, we can\u2019t consider any further applications, so please submit your application as soon as possible to avoid disappointment": [],
        "A truly digital mindset. Open to collaboration. Open to risk. Open to new ways of doing things.": [],
        "Obsessed with data. Obsessed with excellence.": [],
        "People who think and behave differently to the way we do. People who don\u2019t want to just be another cog in the machine.": [],
        "Working with Azure and Data bricks, and building pipelines from source systems to our data lake. You will have good hands-on experience with Lake Warehouse and Data Bricks.": [
            [
                "Azure",
                "HARD"
            ],
            [
                "Data bricks",
                "SOFT"
            ],
            [
                "pipelines",
                "SOFT"
            ],
            [
                "Lake Warehouse",
                "SOFT"
            ],
            [
                "Data Bricks",
                "HARD"
            ]
        ],
        "Development and operation of our data platform and environments": [],
        "Analyze, design, plan, execute and evaluate data requirements to support business activities, innovation, and projects": [],
        "Working closely with data architects (to determine what data management systems are appropriate) and data scientists (to determine which data are needed for analysis).": [
            [
                "data management",
                "SOFT"
            ],
            [
                "analysis",
                "SOFT"
            ]
        ],
        "Tackling problems associated with database integration and unstructured data sets": [
            [
                "database integration",
                "SOFT"
            ],
            [
                "unstructured data",
                "SOFT"
            ]
        ],
        "Support for users of our data platform, including advising on best practices What you should bring to the role": [],
        "Experience in data engineering": [
            [
                "data engineering",
                "CERT"
            ]
        ],
        "A degree to BSc or equivalent": [
            [
                "BSc",
                "CERT"
            ]
        ],
        "Excellent interpersonal skills and adept at dealing with stakeholders. Proficient and effective communicator.": [],
        "Competent/advanced programmer in Python ": [
            [
                "Python",
                "SOFT"
            ]
        ]
    },
    {
        "The Company": [],
        "Cognizant (NASDAQ:CTSH) is a leading provider of information technology, consulting, and business process outsourcing services, dedicated to helping the world's leading companies build stronger businesses. Headquartered in Teaneck, New Jersey (U.S.), Cognizant has over 280,000 employees as of January 2021. Cognizant is a member of the NASDAQ-100, the S&P 500, the Forbes Global 1000, and the Fortune 500 and is ranked among the top performing and fastest growing companies in the world.": [],
        "Cognizant Consulting": [],
        "At Cognizant, our consultants orchestrate the capabilities to truly change the game across strategy, design, technology and industry/functional knowledge to deliver insight at speed and solutions at scale. Our consulting services elevate the unique abilities and business aspirations of customers and employees and build relationships based on trust and value.": [],
        "The Data Engineer will propose and implement solutions using a range of AWS infrastructure, including S3, Redshift, Lambda, Step Functions, DynamoDB, AWS Glue, and Matillion. They will liaise with clients to define requirements, refine solutions, and ultimately hand them over to clients\u2019 own technical teams. The ideal candidate will have exposure to CI/CD processes, or at least be keen to learn \u2013 our clients love infrastructure as code, and we like our engineers to own the deployment of their work. Candidates should delight in creating something from nothing on greenfield projects. We\u2019re looking for people who can\u2019t let go of interesting problems. We need people who can work independently; but we\u2019re a close-knit, supportive team \u2013 we like to learn new things and share our ideas so that clients get the best return on their investments.": [
            [
                "AWS",
                "HARD"
            ],
            [
                "S3",
                "HARD"
            ],
            [
                "Redshift",
                "HARD"
            ],
            [
                "Lambda",
                "HARD"
            ],
            [
                "Step Functions",
                "PERK"
            ],
            [
                "DynamoDB",
                "HARD"
            ],
            [
                "Glue",
                "HARD"
            ],
            [
                "Matillion",
                "CERT"
            ],
            [
                "CI/CD processes",
                "SOFT"
            ]
        ],
        "Essential experience:": [],
        "Desirable experience:": [],
        "About Cognizant": [],
        "Experience in analysing and cleansing data using a variety of tools and techniques.": [
            [
                "analysing",
                "SOFT"
            ],
            [
                "cleansing data",
                "SOFT"
            ]
        ],
        "Familiarity with AWS data lake-related components.": [
            [
                "AWS data lake",
                "HARD"
            ]
        ],
        "Hands-on experience with Redshift, Glue, and S3.": [
            [
                "Redshift",
                "HARD"
            ],
            [
                "Glue",
                "HARD"
            ],
            [
                "S3",
                "HARD"
            ]
        ],
        "Extensive experience in ETL and using patterns for cloud data warehouse solutions (e.g. ELT).": [
            [
                "ETL",
                "SOFT"
            ],
            [
                "cloud data warehouse",
                "SOFT"
            ],
            [
                "ELT)",
                "SOFT"
            ]
        ],
        "Hands-on experience with Matillion.": [
            [
                "Matillion",
                "SOFT"
            ]
        ],
        "Familiarity with a variety of Databases, incl. structured RDBMS.": [
            [
                "Databases",
                "SOFT"
            ],
            [
                "RDBMS",
                "HARD"
            ]
        ],
        "Experience in working with a variety of data formats, JSON, XML, CSV, Parquet, etc.": [
            [
                "data",
                "SOFT"
            ],
            [
                "JSON",
                "HARD"
            ],
            [
                "XML",
                "SOFT"
            ],
            [
                "CSV",
                "HARD"
            ],
            [
                "Parquet",
                "SOFT"
            ]
        ],
        "Experience with building and maintaining data dictionaries / meta-data.": [
            [
                "meta-data",
                "SOFT"
            ]
        ],
        "Experience with Linux and cloud environments.": [
            [
                "Linux",
                "HARD"
            ],
            [
                "cloud",
                "SOFT"
            ]
        ],
        "Data Visualisation Technologies (e.g. Amazon QuickSight, Tableau, Looker, QlikSense).": [
            [
                "Data Visualisation Technologies",
                "SOFT"
            ],
            [
                "Amazon QuickSight",
                "HARD"
            ],
            [
                "Tableau",
                "HARD"
            ],
            [
                "Looker",
                "HARD"
            ],
            [
                "QlikSense",
                "HARD"
            ]
        ],
        "Familiarity with large data techniques (Hadoop, MapReduce, Spark, etc.)": [
            [
                "Hadoop",
                "HARD"
            ],
            [
                "MapReduce",
                "SOFT"
            ],
            [
                "Spark",
                "HARD"
            ]
        ],
        "Familiarity with providing data via a microservice API.": [
            [
                "API",
                "SOFT"
            ]
        ],
        "Experience with other public cloud data lakes.": [
            [
                "cloud data lakes",
                "SOFT"
            ]
        ],
        "AWS Certifications (particularly Solution Architect Associate and Big Data Speciality).": [
            [
                "AWS",
                "HARD"
            ],
            [
                "Big Data Speciality",
                "SOFT"
            ]
        ],
        "Machine Learning.": [
            [
                "Machine",
                "SOFT"
            ]
        ]
    },
    {
        "About Sparta Global ": [],
        "We employ people from all backgrounds and give them careers within technology, working with enthusiastic individuals to give them the skills for success within the public and private sectors. We design careers, coach future leaders, and promote a more diverse and equal landscape with our work garnering over 10 awards across L&D and ED&I. We are a Top 20 Employer for Social Mobility and proud to say BCorp certified.": [],
        "About this role": [],
        "You'll become versatile in a wide array of tools across topics covering data focused coding, data visualisation, Cloud Services and Big Data.": [
            [
                "data visualisation",
                "SOFT"
            ],
            [
                "Cloud Services",
                "SOFT"
            ],
            [
                "Big Data",
                "SOFT"
            ]
        ],
        "We're not expecting you to have the proficiencies right away - that's where our award-winning Academy comes in. We are the experts in building skills and confidence in a fun and supportive environment that will not only challenge you but also develop your specialist capabilities ready to work on our clients' projects. ": [],
        "What we're looking for ": [],
        "To be successful for this role you will demonstrate a level of ability Python or similar. You will be passionate about technology and eager to learn programme development to an advanced level.": [
            [
                "Python",
                "HARD"
            ]
        ],
        "We also look for the following traits that align with companies' values:": [],
        "Empathy and Diversity - You operate with integrity, respect everyone and set a good example for the team. ": [
            [
                "Empathy",
                "SOFT"
            ],
            [
                "Diversity",
                "SOFT"
            ]
        ],
        "Drive - You challenge yourself to exceed targets to take pride in your work. ": [
            [
                "Drive",
                "SOFT"
            ]
        ],
        "Collaboration - You work in synergy with others, supportive, approachable, build healthy relationships. ": [
            [
                "Collaboration",
                "SOFT"
            ]
        ],
        "Innovation - Your inquisitiveness knows no bounds and you love to learn. You embrace creativity and enjoy working with different people and their viewpoints. ": [
            [
                "Innovation",
                "SOFT"
            ]
        ],
        "Flexibility - Adaptable to change, you are calm and compassionate when responding to the unexpected.": [
            [
                "Flexibility",
                "SOFT"
            ]
        ],
        "Why should you apply? ": [],
        "We see ourselves as a people-powered business that likes to recognise and reward the hard work of our employees. We promote continuous learning and development with increasing earning potential for everyone who joins us. By the end of your first year, based on our performance metrics, you can expect to earn an average of \u00a330,000.": [],
        "We also provide:": [],
        "Being employed by Sparta Global is an investment in your future that pays dividends along the way. We give you breadth of experience and skills, along with increasing opportunities to develop further and earn more. No two career paths look the same at Sparta.": [],
        "Minimum Requirements ": [],
        "We are a national organisation serving clients across the country. After completing remote training, you may be deployed to various client sites throughout the UK. Flexibility and willingness to relocate are essential as specific locations cannot be guaranteed. We welcome applicants from diverse backgrounds and experience levels, but the successful candidate must, by the start of employment, have permission to work in the UK.": [],
        "Our Recruitment Process ": [],
        "Online Application: Interested candidates can apply online through our application portal. Our talent team will review applications and contact qualified candidates within 48 hours for further recruitment steps.": [],
        "Telephone Interview: Candidates who pass the initial screening will be invited to a telephone interview. We will assess communication skills, motivation, professionalism, and delve deeper into goals, interests, and background.": [],
        "Online Assessments: Successful candidates from the telephone interview will complete online assessments evaluating technical competence in programming and cognitive abilities.": [],
        "Competency Interview: Candidates excelling in the assessments will be invited to a competency interview. This interview provides an opportunity to showcase clear communication of technical concepts and behavioural competencies with relevant examples. We value candidates who demonstrate personality, collaborative skills, and a growth mindset.": [],
        "How to Best Prepare for the Interview ": [],
        "Research the STAR Method (Situation, Task, Action, Result) and our six behavioural competencies. This knowledge will enable you to answer competency-based questions effectively. You can find a handy guide on answering competency questions on our website. ": [],
        "Visit our YouTube Channel: Gain valuable insights and expert advice on virtual interviews, strategies to manage nerves, and tips on nonverbal communication. ": [],
        "We look forward to receiving your application - good luck!": [
            [
                "application",
                "SOFT"
            ]
        ],
        "You'll be designing, building, maintaining, and troubleshooting the data pipelines that enable organisations to store, process, and analyse their data, and ensuring the data is reliable through testing and debugging. Looking for efficiencies and optimising the data pipelines for scalability and performance will be a focus. ": [
            [
                "data pipelines",
                "SOFT"
            ],
            [
                "analyse",
                "SOFT"
            ],
            [
                "data",
                "SOFT"
            ],
            [
                "testing",
                "SOFT"
            ],
            [
                "debugging",
                "SOFT"
            ],
            [
                "data pipelines",
                "SOFT"
            ]
        ],
        "You'll be handling and working with large sets of structured and unstructured data and will be responsible for ensuring that the data is organised and available for data scientists and analysts to use.": [],
        "Working with others is key, you could be working with other engineers, developers, data scientists, analysts and even stakeholders to understand their data needs. ": [],
        "20 days annual leave + bank holidays.": [
            [
                "20 days annual leave + bank holidays",
                "PERK"
            ]
        ],
        "An extra day off for your birthday.": [
            [
                "day off for your birthday",
                "PERK"
            ]
        ],
        "Pension.": [
            [
                "Pension",
                "PERK"
            ]
        ],
        "Discounted gym membership.": [
            [
                "Discounted gym membership",
                "PERK"
            ]
        ],
        "Eye care.": [
            [
                "Eye care",
                "PERK"
            ]
        ],
        "Death in service cover.": [],
        "Cycle to work scheme.": [
            [
                "Cycle to work scheme",
                "PERK"
            ]
        ],
        "Season ticket loan.": [
            [
                "Season ticket loan",
                "PERK"
            ]
        ],
        "Bonuses and structured pay rises.": [],
        "Employee assistance program.": [],
        "Yearly budget for personal development.": [
            [
                "Yearly budget for personal development",
                "PERK"
            ]
        ],
        "Access to alumni and community networks.": [],
        "Opportunities to be brand ambassadors.": []
    },
    {
        "Data Engineer/Data Analyst - Up to 65k (D.O.E)": [],
        "A financial services client of mine is actively looking for a Data Engineer, with big ambitions to create a positive impact in their company - it's an exciting time to join them!": [],
        "The ideal Data engineer will bring the ability to focus on getting the job done, will be a DWH expert and will be act as a key figure in developing this practice within the business.": [],
        "What you'll do:": [],
        "You will need to work across departments to understand what business leaders want to gain from their datasets. You will be expected to create and/or validate Pipelines to support service level artifact promotion such as Power BI Dashboards. You will aid with preparations prior to, and during, any Technical Design Authority approvals.": [
            [
                "validate Pipelines",
                "SOFT"
            ],
            [
                "Power BI",
                "HARD"
            ]
        ],
        "What you'll need:": [],
        "Commercial experience with SQL or Python Experience with DBT or similar technologies like Snowflake Demonstrable experience of data integration and pipeline-centric development (deployed in an Azured environment is highly favoured). Excellent communication skills - verbal and written.": [
            [
                "SQL",
                "HARD"
            ],
            [
                "Python",
                "HARD"
            ],
            [
                "Snowflake",
                "HARD"
            ],
            [
                "data integration",
                "SOFT"
            ],
            [
                "pipeline",
                "SOFT"
            ],
            [
                "communication skills",
                "SOFT"
            ],
            [
                "verbal",
                "SOFT"
            ],
            [
                "written",
                "SOFT"
            ]
        ],
        "Some of the benefits include:": [],
        "Discretionary Bonus Bupa Private medical insurance - Family cover Pension scheme scaling up to - 16% This is a full-time role that offers hybrid working arrangement - 2 days onsite in their Epsom office after the on boarding period.": [
            [
                "Discretionary Bonus Bupa Private medical insurance",
                "PERK"
            ],
            [
                "Pension scheme scaling up to - 16% This is a full-time role",
                "PERK"
            ],
            [
                "hybrid working",
                "PERK"
            ]
        ],
        "If this role sounds of interest, please send your CV to - ": []
    },
    {
        "Data Engineer": [],
        "Location: Remote": [],
        "Day rate: \u00a3300-325 (Outside IR35)": [
            [
                "Day rate: \u00a3300",
                "PERK"
            ]
        ],
        "Contract Length: 6 Months Rolling": [],
        "Understanding Solutions are looking for a Data Engineer to wield the power to ensure data's availability and impeccable quality! Your mission? Craft ingenious operations, processes, and mechanisms that synchronize the graceful flow and accessibility of data, all while adhering to project scope and relentless deadlines!": [
            [
                "flow",
                "SOFT"
            ]
        ],
        "Are you passionate about crafting intricate data systems and pipelines? Do you revel in deciphering the enigmatic tapestry of trends and patterns within data? Can you unveil the hidden gems buried beneath raw data?": [
            [
                "data systems",
                "SOFT"
            ],
            [
                "pipelines",
                "SOFT"
            ],
            [
                "raw data",
                "SOFT"
            ]
        ],
        "Responsibilities:": [],
        "Skills:": [],
        "Breathing life into data ecosystems by masterfully implementing data ingestions, often joining forces with fellow data sorcerers, analysts , DevOps sorcerers and data visionaries.": [
            [
                "data ecosystems",
                "SOFT"
            ]
        ],
        "Guarding the cloud infrastructure and processes with your cybersecurity prowess, implementing the finest practices.": [
            [
                "cloud",
                "SOFT"
            ]
        ],
        "Channeling the powers of modern principles and methodologies to catapult business initiatives and capabilities to unprecedented heights.": [],
        "Becoming the data whisperer, identifying opportunities to enhance data processing, reliability, efficiency, and quality, all while optimizing solution cost and performance.": [
            [
                "enhance data processing",
                "SOFT"
            ]
        ],
        "Proficiency in Python. ": [
            [
                "Python",
                "HARD"
            ]
        ],
        "Strong command of SQL. ": [
            [
                "command",
                "HARD"
            ],
            [
                "SQL",
                "HARD"
            ]
        ],
        "Proven experience in the AWS/Azure data stack. ": [
            [
                "AWS",
                "HARD"
            ],
            [
                "Azure data stack",
                "HARD"
            ]
        ],
        "Familiarity with AWS (CloudFormation/SAM or CDK, Terraform). ": [
            [
                "AWS",
                "HARD"
            ],
            [
                "Terraform",
                "HARD"
            ]
        ]
    },
    {
        "About us": [],
        "Job Purpose": [],
        "An experienced Engineer with the ability to lead technology on large projects spanning multiple teams. Staff Software Engineers will typically have between 8-12 years of experience. As a Staff Software Engineer, you will be involved in the product development process from ideation to launch, working closely with software engineers, product managers, UI/UX designers, and key stakeholders across the organization to deliver the best digital experiences for our users. You will actively contribute to the success of the team by innovating, participating in technical discussions, and contributing to the development of products that will transform the organization.": [],
        "Key Accountabilities": [],
        "\u2022 Able to own assembling architectural plans and artifacts for projects the org is taking on - a driver of system architecture outside of the team scope.\u2022 Executes on large projects for the org and breaks work down for themselves and others. Able to integrate the work within org, identifying edge cases and exception states.\u2022 Usually involved in architectural designs for the org - consulted for their software design expertise on and off the team.\u2022 Looks to solve not just the immediate problem for their team, but similar problems for the org, generalizing to broader solutions where appropriate\u2022Understands and incorporates our key technology frameworks used company-wide, occasionally a contributor to the core engineering solutions.\u2022 Proactively looking to get ahead of what will come next, for the team and for the org on the technical roadmap. \u2022 A driver of the org tech roadmap - a participant in the prioritization and planning of this work.": [],
        "Supervisory/Interpersonal- Experience Required": [],
        "\u2022Ability to lead technology on large projects spanning multiple teams. \u2022 Leads hiring initiatives, assembling new questions, coordinating on changes to hiring practices and process.\u2022 Acts as a conduit for technical knowledge throughout the org, bringing company-wide tech info into their org\u2022 Leads the vision for what we communicate with our documentation.\u2022 Active participant in org-wide meetings and leading when the topic is the teams' work.\u2022 Presents to others on technical topics of interest, fostering growth in knowledge at NG": [
            [
                "org",
                "SOFT"
            ],
            [
                "NG",
                "SOFT"
            ]
        ],
        "Qualifications": [],
        "\u2022 Bachelors Degree or equivalent experience\u2022 Is recognized as an expert within a specific area of expertise with typically 10 or more years experience within the field. May require Bachelor's or Master's degree.\u2022 In depth experience in a highly technical field or in multiple, related fields. Has the ability to initiate and complete projects with minimal guidance from others. Has thorough knowledge of all procedures, transactions and practices in one process. Has a working knowledge of the remaining area's processes. Quickly grasps complex issues and can effectively integrate, compile and analyze complex information and data. Has a basic understanding of relevant industry issues.\u2022 Work performed without direction; exercises latitude in determining assignment objectives.\u2022 Frequently explains information to others who use it to advise and negotiate at higher organizational levels. Maintains occasional external contact with counterparts within and external to the industry .Positively influences others and gains their cooperation. Ability to modify practices and procedures to meet individual situations. Manages people and/or processes to meet operating goals and quality standards": [
            [
                "Bachelors",
                "CERT"
            ],
            [
                "compile",
                "SOFT"
            ],
            [
                "analyze complex",
                "SOFT"
            ],
            [
                "quality",
                "SOFT"
            ]
        ],
        "More Information": []
    },
    {},
    {
        "We are innovative \u2013 and if you are too, we\u2019ll have a lot in common...": [],
        "Are you an experienced EHV Cable Design Engineer ready to make your mark in the energy sector? If so, we have the perfect opportunity for you to join our dynamic UKPN Team. As a key member of our team, you'll play a pivotal role in delivering cutting-edge solutions within the high voltage cable systems domain.": [],
        "The Role:": [],
        "As our EHV Cable Design Engineer, you'll work closely with the project team to ensure designs align with regulatory and safety standards, fostering a culture of compliance and excellence. Your role involves crafting sustainable design solutions for high voltage cable systems, involving the use of state-of-the-art software to perform protection grading studies and high voltage cable calculations. Your responsibilities encompass the design of cable routing and management systems for voltage levels ranging from 11kV to 132kV.": [
            [
                "routing",
                "SOFT"
            ],
            [
                "management",
                "SOFT"
            ]
        ],
        "Here are some of the activities you will be involved with...": [],
        "We'd love to hear from you if...": [],
        "Have a relevant degree and possess knowledge of CYMCAP, CRATER, or other similar design packages.You can generate CSDD documents and have experience in a similar role, along with a track record of working with IDNOs, DNOs, or ICPs.You bring expertise in HV cable system design and rating.Possess a deep understanding of IEC cable standards and CIGRE technical policies, along with the associated suite of technical specifications and supporting documentation.": [
            [
                "CRATER",
                "CERT"
            ],
            [
                "IDNOs",
                "HARD"
            ],
            [
                "DNOs",
                "SOFT"
            ]
        ],
        "If you're a dedicated and skilled EHV Cable Design Engineer looking to join a forward-thinking team, we encourage you to apply today!": [],
        "Our Company": [],
        "Every day we work smarter, greener and use our imaginations.": [],
        "Our purpose at Clancy is simple - we make life better for everyone\u2019s growing families. We play a vital role in providing fresh drinking water and power to millions of homes and businesses and so much more.": [],
        "We are one of the biggest family owned construction businesses in the UK and we care about our people, our clients and the environment.": [],
        "What Next": [],
        "You apply, and we respond within two weeks (we know how annoying it is not to hear anything back)! If you don\u2019t receive feedback within that timescale, please don\u2019t be afraid to chase us - one of our values is to do what we say we will do!": [],
        "Benefits": [],
        "In addition to helping you reach your career goals, a competitive salary, pension, healthcare and holiday allowance starting at 24 days per annum, we also offer perks including Clancy Xtras, our employee benefits programme with discounts for numerous well-known retailers such as Tesco, Sainsbury\u2019s, Currys PC World and Vue Cinemas, cycle to work scheme as well as an Employee Assistance Programme.": [
            [
                "holiday allowance",
                "PERK"
            ],
            [
                "Clancy Xtras",
                "PERK"
            ]
        ],
        "Clancy is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all our employees. If you require any reasonable adjustments to be made for you to attend an interview, please do let us know and we will be happy to accommodate.": [],
        "We are proud signatories of the Armed Forces Covenant and Disability Confident Committed.": [],
        "Produce as-constructed plans in accordance with client specifications.": [],
        "Conduct on-site visits to gather real-time data, ensuring the prompt submission of as-constructed plans.": [
            [
                "real-time data",
                "SOFT"
            ]
        ],
        "Collaborate with Site Engineers to compile data collected on-site.": [
            [
                "compile data collected",
                "SOFT"
            ]
        ],
        "Create precise, high-quality technical drawings and plans based on designs supplied by Preconstruction Teams and designers.": [],
        "Implement modifications to existing drawings.": []
    },
    {
        "Role: Data Engineer\u00a0": [],
        "Industry: Insurance company": [],
        "Salary:\u00a0\u00a380,000- \u00a390,000": [],
        "Location:\u00a0London UK (Hybrid)": [],
        "": [],
        "IHR Tech is proud to be partnering with a leading company within the Insurance sector and they are looking for a Data Engineer to join their team.": [
            [
                "IHR Tech",
                "SOFT"
            ]
        ],
        "You will be supporting the development of a state-of-the-art AWS based data platform and a bespoke data analytics application.": [
            [
                "data analytics",
                "SOFT"
            ]
        ],
        "Responsibilities:": [],
        "Experience\u00a0needed:": [],
        "Reporting to the Head of Advanced Analytics and Data Engineer.": [
            [
                "Analytics",
                "SOFT"
            ]
        ],
        "Working alongside a team of Data Scientist and Data Analytics to support with the development of an AWS state of the art data platform.": [
            [
                "AWS",
                "HARD"
            ]
        ],
        "Developing new data models and core data sets.": [
            [
                "core data",
                "SOFT"
            ]
        ],
        "Developing data extractions, transformations and loading processes\u00a0": [
            [
                "loading processes",
                "SOFT"
            ]
        ],
        "Strong SQL and Python experience.": [
            [
                "SQL",
                "HARD"
            ],
            [
                "Python",
                "HARD"
            ]
        ],
        "Experience in writing complex SQL statements and data transformations in SQL": [
            [
                "SQL",
                "HARD"
            ],
            [
                "data transformations",
                "SOFT"
            ],
            [
                "SQL",
                "SOFT"
            ]
        ],
        "Previous experience in a similar Data Engineer role developing big data cloud platforms\u00a0": [
            [
                "big data",
                "SOFT"
            ]
        ],
        "Software Engineer or Business Intelligence experience is useful.": []
    },
    {
        "Data Engineer (Azure, Power BI, SQL, Python, ETL, Data Factory) - \u00a360k - 3 days in office per week - London ": [
            [
                "Azure",
                "HARD"
            ],
            [
                "Power BI",
                "HARD"
            ],
            [
                "SQL",
                "HARD"
            ],
            [
                "Python",
                "HARD"
            ],
            [
                "ETL",
                "SOFT"
            ],
            [
                "Data Factory",
                "SOFT"
            ]
        ],
        "I am partnered with a growing AD-Tech in central London who are continuing their growth plans and are looking to connect with experienced Data Engineers (Azure, Power BI, SQL, Python, ETL, Data Factory)": [
            [
                "Azure",
                "HARD"
            ],
            [
                "Power BI",
                "HARD"
            ],
            [
                "SQL",
                "HARD"
            ],
            [
                "Python",
                "HARD"
            ],
            [
                "ETL",
                "SOFT"
            ],
            [
                "Data Factory",
                "SOFT"
            ]
        ],
        "The business is based in London and are offering flexible WFH and ask that engineers are in the office 3 days a week.": [],
        "The business has been positively disrupting the Media sector for over a decade and work with brands all across the globe providing their cutting edge data and technology solutions.": [
            [
                "data",
                "SOFT"
            ]
        ],
        "As the business continues to grow so does their data and their now require a Data Engineer (Azure, Power BI, SQL, Python, ETL, Data Factory) to help build out their BI and Azure Data Factory environments. This is not a senior position, the business is looking to connect with Data Engineers (Azure, Power BI, SQL, Python, ETL, Data Factory) who have around 3 - 5 years in engineering and are still looking to grow and progress in their careers.": [
            [
                "Azure",
                "HARD"
            ],
            [
                "Power BI",
                "HARD"
            ],
            [
                "SQL",
                "HARD"
            ],
            [
                "Python",
                "HARD"
            ],
            [
                "ETL",
                "SOFT"
            ],
            [
                "Data Factory",
                "SOFT"
            ],
            [
                "BI",
                "SOFT"
            ],
            [
                "Azure Data Factory",
                "HARD"
            ],
            [
                "Azure",
                "HARD"
            ],
            [
                "Power BI",
                "HARD"
            ],
            [
                "SQL",
                "HARD"
            ],
            [
                "Python",
                "HARD"
            ],
            [
                "ETL",
                "SOFT"
            ],
            [
                "Data Factory",
                "SOFT"
            ],
            [
                "engineering",
                "CERT"
            ]
        ],
        "Skill set required for the Data Engineer (Azure, Power BI, SQL, Python, ETL, Data Factory)": [
            [
                "Azure",
                "HARD"
            ],
            [
                "Power BI",
                "HARD"
            ],
            [
                "SQL",
                "HARD"
            ],
            [
                "Python",
                "HARD"
            ],
            [
                "ETL",
                "SOFT"
            ],
            [
                "Data Factory",
                "SOFT"
            ]
        ],
        "If the above sounds like you and you are a Data Engineer (Azure, Power BI, SQL, Python, ETL, Data Factory), then please click on the apply button below.": [
            [
                "Azure",
                "HARD"
            ],
            [
                "Power BI",
                "HARD"
            ],
            [
                "SQL",
                "HARD"
            ],
            [
                "Python",
                "HARD"
            ],
            [
                "ETL",
                "SOFT"
            ],
            [
                "Data Factory",
                "SOFT"
            ]
        ],
        "SQL": [
            [
                "SQL",
                "HARD"
            ]
        ],
        "Python": [
            [
                "Python",
                "HARD"
            ]
        ],
        "ETL": [
            [
                "ETL",
                "SOFT"
            ]
        ],
        "Azure": [
            [
                "Azure",
                "HARD"
            ]
        ],
        "Data Factory": [
            [
                "Data Factory",
                "SOFT"
            ]
        ],
        "Power BI ": [
            [
                "Power BI",
                "HARD"
            ]
        ]
    },
    {},
    {
        " Our client an industry leading Media and Data company are looking for a versatile Lead Data Engineer to join their team. ": [],
        " Leading a team of highly collaborative data engineers, your role will be to support the management leadership to make decisions for the data analytics and business intelligence systems. This role would suit a data professional with experience in extracting and analysing data from third party providers and presenting data and insights to stakeholders. The position is home based with 1 day per week travel into their London offices. ": [
            [
                "analysing data from",
                "SOFT"
            ]
        ],
        " The role: ": [],
        "": [],
        " \u2022 Liaising with suppliers (Legacy Platforms / Dealing with Suppliers & Clients / Newly built platforms) E.g. Data issues / supplied file issues etc ": [],
        " \u2022 Management of infrastructure (AWS / Azure) ": [
            [
                "AWS",
                "HARD"
            ],
            [
                "Azure",
                "HARD"
            ]
        ],
        " \u2022 Designing new processes to source efficiencies ": [],
        " \u2022 Analyse, transform and organize raw data. ": [
            [
                "Analyse",
                "SOFT"
            ],
            [
                "transform",
                "SOFT"
            ],
            [
                "organize",
                "SOFT"
            ]
        ],
        " \u2022 Architect and help build data systems and pipelines ": [
            [
                "data systems",
                "SOFT"
            ],
            [
                "pipelines",
                "SOFT"
            ]
        ],
        " \u2022 Evaluate business needs and objectives ": [],
        " \u2022 Combine raw information from different sources ": [],
        " \u2022 Work on ETL and data loading processes ": [
            [
                "ETL",
                "SOFT"
            ],
            [
                "data loading",
                "SOFT"
            ]
        ],
        " \u2022 Explore ways to enhance data quality and reliability ": [
            [
                "enhance data quality",
                "SOFT"
            ]
        ],
        " \u2022 Assist in the development of new products and tools including keeping abreast of developments in the field of data capture techniques via OCR and AI ": [
            [
                "OCR",
                "SOFT"
            ],
            [
                "AI",
                "SOFT"
            ]
        ],
        " Key skills required ": [],
        " \u2022 Strong data modelling and SQL/database design skills. ": [
            [
                "data modelling",
                "SOFT"
            ],
            [
                "SQL",
                "HARD"
            ]
        ],
        " \u2022 Proficient with Tableau or Power BI ": [
            [
                "Tableau",
                "HARD"
            ],
            [
                "Power BI",
                "HARD"
            ]
        ],
        " \u2022 Experience with SQL Server Integration Services (SSIS) packages, using SQL Server Data Tools (SSDT), Python, C# ": [
            [
                "SQL Server",
                "HARD"
            ],
            [
                "SQL Server Data Tools",
                "HARD"
            ],
            [
                "Python",
                "HARD"
            ],
            [
                "C#",
                "HARD"
            ]
        ],
        " \u2022 Strong conceptual and problem-solving skills ": [
            [
                "conceptual",
                "SOFT"
            ],
            [
                "problem-solving",
                "SOFT"
            ]
        ],
        " \u2022 Skill and knowledge of ETL processes ": [
            [
                "ETL",
                "SOFT"
            ]
        ],
        " \u2022 Experience of working within cloud-based environment, AWS and Azure ": [
            [
                "cloud",
                "SOFT"
            ],
            [
                "AWS",
                "HARD"
            ],
            [
                "Azure",
                "HARD"
            ]
        ],
        " \u2022 Good programming skills \u2013 SQL, R, Python, JavaScript etc. ": [
            [
                "SQL",
                "HARD"
            ],
            [
                "R",
                "HARD"
            ],
            [
                "Python",
                "HARD"
            ],
            [
                "JavaScript",
                "HARD"
            ]
        ],
        " \u2022 Strong verbal and written communication skills ": [
            [
                "verbal",
                "SOFT"
            ],
            [
                "written",
                "SOFT"
            ]
        ],
        " \u2022 Excellent numerical, analytical and statistical ability ": [
            [
                "numerical",
                "SOFT"
            ],
            [
                "analytical",
                "SOFT"
            ],
            [
                "statistical",
                "SOFT"
            ]
        ],
        " As well as a salary of up to \u00a375,000 the company offer generous benefits including pension, Life Assurance and Group Income Protection. This is a home based role with 1 day per week travel into their London offices. ": [
            [
                "pension",
                "PERK"
            ],
            [
                "Life Assurance",
                "PERK"
            ],
            [
                "Group Income Protection",
                "PERK"
            ]
        ]
    },
    {
        "Senior Data Engineer - Data Orchestration and DevOps": [
            [
                "DevOps",
                "SOFT"
            ]
        ],
        "Python, Prefect, Cube JS, AWS stack, Terraform ": [
            [
                "Python",
                "HARD"
            ],
            [
                "Cube JS",
                "HARD"
            ],
            [
                "AWS stack",
                "HARD"
            ],
            [
                "Terraform",
                "HARD"
            ]
        ],
        "Up to $85k USD - paid B2B ": [
            [
                "B2B",
                "SOFT"
            ]
        ],
        "Fully remote ": [],
        "Are you passionate about data, analytics, and the power of technology? This is your chance to reshape the future of digital publishing through innovative data-driven solutions.": [
            [
                "analytics",
                "SOFT"
            ]
        ],
        "As a Senior Data Engineer, you will be responsible for providing architectural and design guidance, developing Python applications on AWS, building large-scale ETL pipelines, and creating API layers for seamless communication.": [
            [
                "Python",
                "HARD"
            ],
            [
                "AWS",
                "HARD"
            ],
            [
                "ETL pipelines",
                "SOFT"
            ],
            [
                "API",
                "SOFT"
            ]
        ],
        "You will need experience in real-time and batch data pipelining (Airbyte, Airflow, Fivetran, DBT, Kafka, etc.) and DevOps.": [
            [
                "real-time",
                "SOFT"
            ],
            [
                "batch data pipelining",
                "SOFT"
            ],
            [
                "Airbyte",
                "SOFT"
            ],
            [
                "Airflow",
                "SOFT"
            ],
            [
                "Fivetran",
                "HARD"
            ],
            [
                "Kafka",
                "HARD"
            ],
            [
                "DevOps",
                "SOFT"
            ]
        ],
        "Opportunities to develop your career are highly prioritised by providing courses and workshops, other benefits include innovation days and unlimited holiday days.": [],
        "If this role has piqued your interest, apply now!": []
    },
    {
        "Jobt title: Data EngineerLocation: West LondonDepartment: Digital & Product DepartmentSalary Details: \u00a350k-60k depending on experienceAn exciting opportunity for an experienced Data Engineer to join the Engineering team working within the Digital & Product Department.The successful candidate will be responsible for designing and creating the pipelines that transform data in a scalable and repeatable way. You will need to produce efficient code that automates the ingestion and cleansing of data and apply the transformations required to achieve the target data format.You will work with the Engineering and Data & Business Analytics teams to understand the source and target data requirements and use this knowledge to develop data integration solutions to achieve the necessary migrations.The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. You must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products.DUTIES & RESPONSIBILITIES": [
            [
                "pipelines",
                "SOFT"
            ],
            [
                "transform data",
                "SOFT"
            ],
            [
                "ingestion",
                "SOFT"
            ],
            [
                "data",
                "SOFT"
            ],
            [
                "develop data integration",
                "SOFT"
            ],
            [
                "optimal data delivery",
                "SOFT"
            ]
        ],
        "SKILLS REQUIRED": [],
        "Apply below, we'd love to know more about you!": [],
        "Networking People (UK) is acting as an Employment Agency in relation to this vacancy.": [
            [
                "Networking People (UK)",
                "SOFT"
            ]
        ],
        "Understand, build and develop ETL and data integration solutions using a wide array of technologies and data sources": [
            [
                "ETL",
                "SOFT"
            ],
            [
                "data integration",
                "SOFT"
            ]
        ],
        "Explore ways to enhance data quality and reliability": [
            [
                "enhance data quality",
                "SOFT"
            ]
        ],
        "Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, etc.": [
            [
                "optimizing data delivery",
                "SOFT"
            ]
        ],
        "Work with cloud-based infrastructure (Azure) for hosting data solutions and applications": [
            [
                "cloud",
                "SOFT"
            ],
            [
                "Azure",
                "HARD"
            ],
            [
                "applications",
                "SOFT"
            ]
        ],
        "Collaborate with architects, data analysts and data scientists to help meet the business goals": [],
        "Proven experience in development and maintenance of ETL/ELT processes within a medallion architecture": [
            [
                "ETL",
                "SOFT"
            ],
            [
                "ELT processes",
                "SOFT"
            ]
        ],
        "Strong experience with SQL and relational databases": [
            [
                "SQL",
                "HARD"
            ],
            [
                "relational databases",
                "SOFT"
            ]
        ],
        "Good knowledge of the Azure data engineering stack - Azure Data Factory, Azure Synapse, Azure Data Lake": [
            [
                "Azure data",
                "HARD"
            ],
            [
                "stack",
                "SOFT"
            ],
            [
                "Azure Data Factory",
                "HARD"
            ],
            [
                "Azure Synapse",
                "HARD"
            ],
            [
                "Azure Data Lake",
                "HARD"
            ]
        ],
        "Analytical skills related to working with structured and unstructured datasets": [
            [
                "Analytical",
                "SOFT"
            ]
        ],
        "Excellent written and verbal communication skills": [
            [
                "written",
                "SOFT"
            ],
            [
                "verbal communication",
                "SOFT"
            ]
        ],
        "Experience supporting and working with cross-functional teams in a dynamic environment": []
    },
    {
        "Data Engineer - SQL": [
            [
                "SQL",
                "HARD"
            ]
        ],
        "Location: W1D 2DH \ud83c\udf0e": [],
        "Working Arrangements: Hybrid (2-3 days a week on-site)": [],
        "Salary: \u00a350,000 - \u00a360,000": [],
        "Industry: Media": [],
        "Tech Stack: Azure, SQL, T-SQL, PowerBI \ud83d\udc69\ud83c\udffb\u200d\ud83d\udcbb": [
            [
                "Azure",
                "HARD"
            ],
            [
                "SQL",
                "HARD"
            ],
            [
                "T",
                "HARD"
            ],
            [
                "SQL",
                "HARD"
            ]
        ],
        "Great opportunity for a talented Data Engineer (SQL, Azure) to join a media company helping their clients reach specific target audiences.": [
            [
                "SQL",
                "HARD"
            ],
            [
                "Azure",
                "HARD"
            ]
        ],
        "The Company \ud83d\ude80": [],
        "Innovative media/adtech business who have built their own bespoke platform that is used by the likes of McDonald's, Volkswagen, Starbucks, Puma and more.": [],
        "The Role \u2728": [],
        "They are seeking a Data Engineer (SQL, Azure) to help maintain their existing platform and build out new ETL pipelines in collaboration with Data Analysts.": [
            [
                "SQL",
                "HARD"
            ],
            [
                "Azure",
                "HARD"
            ],
            [
                "ETL",
                "SOFT"
            ]
        ],
        "You will be expected to contribute to data quality (SQL, Azure) as well as working on new pipelines, automation tooling, infrastructure and documenting lineage.": [
            [
                "data quality",
                "SOFT"
            ],
            [
                "SQL",
                "HARD"
            ],
            [
                "Azure",
                "HARD"
            ],
            [
                "pipelines",
                "SOFT"
            ],
            [
                "automation",
                "SOFT"
            ]
        ],
        "Desired Skills \u2699\ufe0f": [],
        "Benefits \ud83c\udfd6": [
            [
                "Benefits \ud83c\udfd6",
                "PERK"
            ]
        ],
        "If you are a skilled Data Engineer (SQL, Azure) who is interested in this role then please apply below and I will be in touch with more details.": [
            [
                "SQL",
                "HARD"
            ],
            [
                "Azure",
                "HARD"
            ]
        ],
        "Azure": [
            [
                "Azure",
                "HARD"
            ]
        ],
        "SQL, T-SQL": [
            [
                "SQL",
                "HARD"
            ],
            [
                "T",
                "HARD"
            ],
            [
                "SQL",
                "HARD"
            ]
        ],
        "PowerBI": [],
        "BigQuery": [
            [
                "BigQuery",
                "HARD"
            ]
        ],
        "Python (bonus but not a necessity)": [
            [
                "Python",
                "HARD"
            ]
        ],
        "Flexible working": [
            [
                "Flexible working",
                "PERK"
            ]
        ],
        "Pension, Life Assurance, Health Insurance": [
            [
                "Pension",
                "PERK"
            ],
            [
                "Life Assurance",
                "PERK"
            ],
            [
                "Health Insurance",
                "PERK"
            ]
        ],
        "Enhanced parental leave": [
            [
                "parental leave",
                "PERK"
            ]
        ]
    },
    {
        "We are looking for exceptional Data Engineers to join our growing team of climate and carbon market analysts, researchers and modellers. We would love to hear from you if you have knowledge of data pipelines, data integration and data architecture. You will likely have a high proficiency in SQL, Python, at least one cloud platform (ideally AWS) and experience/knowledge of ETL tools. As well as excellent technical capability, you will be passionate about improving the state of the natural world and addressing the challenge of climate change. You will be driven, a creative thinker and intellectually curious. You should be able to effectively manage your own time and be capable of communicating complex issues to colleagues and senior management level audiences. Trove Research offers an exciting, fast-paced work environment, working with a dynamic and driven team. Our goal is to bring new levels of analysis and insight to accelerate the energy transition to a low carbon economy. Responsibilities will include:": [
            [
                "data pipelines",
                "SOFT"
            ],
            [
                "data integration",
                "SOFT"
            ],
            [
                "data architecture",
                "SOFT"
            ],
            [
                "SQL",
                "HARD"
            ],
            [
                "Python",
                "HARD"
            ],
            [
                "AWS",
                "HARD"
            ],
            [
                "ETL",
                "SOFT"
            ],
            [
                "analysis",
                "SOFT"
            ]
        ],
        "2 October 2023 Home / Careers / Senior Data Engineer": [],
        "Designing, creating, optimizing and maintaining scalable data pipelines": [
            [
                "scalable data pipelines",
                "SOFT"
            ]
        ],
        "Extraction, transformation, and loading of data from a wide variety of data sources using SQL, Python and AWS/Azure": [
            [
                "Extraction",
                "SOFT"
            ],
            [
                "data from",
                "SOFT"
            ],
            [
                "SQL",
                "HARD"
            ],
            [
                "Python",
                "HARD"
            ],
            [
                "AWS",
                "HARD"
            ],
            [
                "Azure",
                "HARD"
            ]
        ],
        "Identifying, designing and implementing internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes": [
            [
                "optimizing data delivery",
                "SOFT"
            ]
        ],
        "Implementing processes to enhance data quality by building robust data validation checks": [
            [
                "enhance data quality",
                "SOFT"
            ],
            [
                "data validation",
                "SOFT"
            ]
        ],
        "Writing unit and integration testing for all data processes": [
            [
                "unit",
                "SOFT"
            ],
            [
                "integration testing",
                "SOFT"
            ]
        ],
        "Ensuring best data engineering practices are implemented": [
            [
                "best data",
                "SOFT"
            ]
        ],
        "Building APIs to create and automate a communication channel with clients and public sources": [],
        "Collaborating with analysts and product teams in UK and India to understand and implement the best data solution": [
            [
                "data",
                "SOFT"
            ]
        ],
        "Helping and guiding junior data engineers in their day-to-day challenges Qualifications and experience:": [],
        "3-4 years of industry experience in data engineering or a similar role": [
            [
                "data engineering",
                "CERT"
            ]
        ],
        "Degree in Computer Science, Software Engineering, IT or a related field": [
            [
                "Computer Science",
                "CERT"
            ],
            [
                "Software Engineering",
                "CERT"
            ]
        ],
        "Experience designing, implementing and optimizing data pipelines": [
            [
                "optimizing data",
                "SOFT"
            ]
        ],
        "Proficient in SQL and relational databases": [
            [
                "SQL",
                "HARD"
            ],
            [
                "relational databases",
                "SOFT"
            ]
        ],
        "Highly skilled in Python language and experience to extract, transform and process data us ing Python": [
            [
                "Python",
                "SOFT"
            ],
            [
                "transform",
                "SOFT"
            ]
        ],
        "Experience in at least one cloud platform (ideally AWS)": [
            [
                "AWS",
                "HARD"
            ]
        ],
        "Strong analytical and problem-solving skills, with keen attention to detail": [
            [
                "analytical",
                "SOFT"
            ],
            [
                "problem-solving",
                "SOFT"
            ],
            [
                "attention to detail",
                "SOFT"
            ]
        ],
        "Ability to learn quickly and adapt to new requirements to make frequent changes in data processes and pipelines": [
            [
                "data processes",
                "SOFT"
            ],
            [
                "pipelines",
                "SOFT"
            ]
        ],
        "Ability to effectively manage data and data security": [
            [
                "data",
                "SOFT"
            ],
            [
                "data",
                "SOFT"
            ]
        ],
        "Ability to prioritize own time and workload": [],
        "Demonstrable enthusiasm for addressing climate change in the corporate sector Nice to have:": [],
        "Experience in MySQL, PowerBI, REST APIs, GitHub, Docker, Excel, MS Fabric Please include a cover letter and CV in your application to [email protected] Don't meet every single requirement? Studies have shown that women and people of colour are less likely to apply to jobs unless they meet every single qualification. At Trove we are dedicated to building a diverse, inclusive, and authentic workplace, so if you're excited about this role but your past experience doesn't align perfectly with every qualification in the job description, we encourage you to apply anyway. You may be just the right candidate for this or other roles. In addition to providing a dynamic work environment with a collaborative, entrepreneurial and mission-driven team. We offer a leading range of benefits: \u00b7 Flexibility on where and how you work, we're fully hybrid with a comfortable office in central London \u00b7 Career growth opportunities and ability to learn and develop alongside global experts \u00b7 Full access to Udemy learning platform and other L&D initiatives \u00b7 Personal development annual cash allowance \u00b7 Generous enhanced maternity leave \u00b7 Pension scheme \u00b7 Regular social events \u00b7 Access to office gym \u00b7 and many more\u2026 https://trove-research.com/wp-content/themes/impeka/images/empty/thumbnail.jpg 150 150 Trove Research Trove Research https://trove-research.com/wp-content/themes/impeka/images/empty/thumbnail.jpg 2 October 2023 2 October 2023": [
            [
                "MySQL",
                "HARD"
            ],
            [
                "REST",
                "SOFT"
            ],
            [
                "GitHub",
                "HARD"
            ],
            [
                "Docker",
                "HARD"
            ],
            [
                "Excel",
                "HARD"
            ],
            [
                "MS",
                "CERT"
            ],
            [
                "annual cash allowance",
                "PERK"
            ],
            [
                "maternity leave",
                "PERK"
            ],
            [
                "Pension scheme",
                "PERK"
            ],
            [
                "150 150 Trove Research Trove Research https://trove-research.com/wp-content/themes/impeka/images/empty/thumbnail.jpg 2 October 2023 2 October 2023",
                "PERK"
            ]
        ]
    },
    {
        "Senior Software Engineer (Python)": [
            [
                "Senior Software Engineer",
                "PERK"
            ],
            [
                "Python",
                "HARD"
            ]
        ],
        "Remote within the UK": [],
        "\u00a360,000-\u00a370,000/annum": [],
        "Must be eligible for SC Clearance - British Passport required": [],
        "iO Associates is currently partnering with one of the UKs largest and most successful public sector consultancy firms who are on the look out for a Senior Software Engineer with experience in Data Engineering.": [
            [
                "iO",
                "HARD"
            ],
            [
                "Data Engineering",
                "CERT"
            ]
        ],
        "They need someone with developer level python skills, and experience designing and building data warehouses and pipelines. ": [
            [
                "python",
                "HARD"
            ],
            [
                "data warehouses",
                "SOFT"
            ],
            [
                "pipelines",
                "SOFT"
            ]
        ],
        "You must have the following skillset": [],
        "- Excellent Python programming skills including Pyspark": [
            [
                "Python",
                "HARD"
            ],
            [
                "Pyspark",
                "SOFT"
            ]
        ],
        "- ETL ": [
            [
                "ETL",
                "SOFT"
            ]
        ],
        "- Ideally AWS Glue - but other cloud platform experience will be considered": [
            [
                "AWS Glue",
                "HARD"
            ]
        ],
        "You should have upwards of four years experience for this role.": [],
        "You can be based anywhere in the UK for this role, and there will only be occasional expectation to attend one of a number of sites accross the UK. ": [],
        "If you fit the above criteria, hit apply and I will call you!": [],
        "Please note - no sponsorship can be offered for this role, and you must hold a UK Passport to be eligible for clearance. ": []
    },
    {
        "Data Engineer - Azure - Hybrid Working": [
            [
                "Azure",
                "HARD"
            ],
            [
                "Hybrid Working",
                "PERK"
            ]
        ],
        "Location: W1D 2DH \ud83c\udf0e": [],
        "Working Arrangements: Hybrid (2-3 days a week on-site)": [],
        "Salary: \u00a350,000 - \u00a360,000": [],
        "Industry: Media": [],
        "Tech Stack: Azure, SQL, T-SQL, PowerBI \ud83d\udc69\ud83c\udffb\u200d\ud83d\udcbb": [
            [
                "Azure",
                "HARD"
            ],
            [
                "SQL",
                "HARD"
            ],
            [
                "T",
                "HARD"
            ],
            [
                "SQL",
                "HARD"
            ]
        ],
        "Great opportunity for a talented Data Engineer (SQL, Azure) to join a media company helping their clients reach specific target audiences.": [
            [
                "SQL",
                "HARD"
            ],
            [
                "Azure",
                "HARD"
            ]
        ],
        "The Company \ud83d\ude80": [],
        "Innovative media/adtech business who have built their own bespoke platform that is used by the likes of McDonald's, Volkswagen, Starbucks, Puma and more.": [],
        "The Role \u2728": [],
        "They are seeking a Data Engineer (SQL, Azure) to help maintain their existing platform and build out new ETL pipelines in collaboration with Data Analysts.": [
            [
                "SQL",
                "HARD"
            ],
            [
                "Azure",
                "HARD"
            ],
            [
                "ETL",
                "SOFT"
            ]
        ],
        "You will be expected to contribute to data quality (SQL, Azure) as well as working on new pipelines, automation tooling, infrastructure and documenting lineage.": [
            [
                "data quality",
                "SOFT"
            ],
            [
                "SQL",
                "HARD"
            ],
            [
                "Azure",
                "HARD"
            ],
            [
                "pipelines",
                "SOFT"
            ],
            [
                "automation",
                "SOFT"
            ]
        ],
        "Desired Skills \u2699\ufe0f": [],
        "Benefits \ud83c\udfd6": [
            [
                "Benefits \ud83c\udfd6",
                "PERK"
            ]
        ],
        "If you are a skilled Data Engineer (SQL, Azure) who is interested in this role then please apply below and I will be in touch with more details.": [
            [
                "SQL",
                "HARD"
            ],
            [
                "Azure",
                "HARD"
            ]
        ],
        "Azure": [
            [
                "Azure",
                "HARD"
            ]
        ],
        "SQL, T-SQL": [
            [
                "SQL",
                "HARD"
            ],
            [
                "T",
                "HARD"
            ],
            [
                "SQL",
                "HARD"
            ]
        ],
        "PowerBI": [],
        "BigQuery": [
            [
                "BigQuery",
                "HARD"
            ]
        ],
        "Python (bonus but not a necessity)": [
            [
                "Python",
                "HARD"
            ]
        ],
        "Flexible working": [
            [
                "Flexible working",
                "PERK"
            ]
        ],
        "Pension, Life Assurance, Health Insurance": [
            [
                "Pension",
                "PERK"
            ],
            [
                "Life Assurance",
                "PERK"
            ],
            [
                "Health Insurance",
                "PERK"
            ]
        ],
        "Enhanced parental leave": [
            [
                "parental leave",
                "PERK"
            ]
        ]
    },
    {
        "Leading solutions for data engineering": [],
        "Maintain the integrity of both the design and the data that is held within the architecture": [],
        "Champion and educate people in the development and use of data engineering best practices": [],
        "Support the Head of Data Engineering and lead by example": [],
        "Contribute to the development of database management services and associated processes relating to the delivery of data solutions": [
            [
                "database management",
                "SOFT"
            ],
            [
                "data solutions",
                "SOFT"
            ]
        ],
        "Provide requirements analysis, documentation, development, delivery and maintenance of data platforms.": [
            [
                "analysis",
                "SOFT"
            ]
        ],
        "Develop database requirements in a structured and logical manner ensuring delivery is aligned with business prioritisation and best practice": [],
        "Design and deliver performance enhancements, application migration processes and version upgrades across a pipeline of BI environments.": [
            [
                "application migration",
                "SOFT"
            ],
            [
                "version upgrades",
                "SOFT"
            ],
            [
                "BI",
                "SOFT"
            ]
        ],
        "Provide support for the scoping and delivery of BI capability to internal users.": [
            [
                "BI",
                "SOFT"
            ]
        ],
        "5 years Data Engineering / ETL development experience (must have)": [
            [
                "ETL development",
                "SOFT"
            ]
        ],
        "Expereuene working within a regulated environment (must have)": [
            [
                "Expereuene working",
                "PERK"
            ]
        ],
        "5 years data design experience in an MI / BI / Analytics environment (Kimball, lake house, data lake)": [
            [
                "lake house",
                "PERK"
            ],
            [
                "data lake",
                "SOFT"
            ]
        ],
        "Excellent Data Warehouse with substantial experience in extracting, reporting and manipulating data from a data warehouse environment are essential": [
            [
                "Data Warehouse",
                "SOFT"
            ],
            [
                "manipulating data from",
                "SOFT"
            ],
            [
                "data warehouse",
                "SOFT"
            ]
        ],
        "Significant technical skills such as Transact SQL language, relational database skills are essential": [
            [
                "relational database",
                "SOFT"
            ]
        ],
        "Evidence of delivering complex data platforms and solutions": [],
        "Experience with cloud data platforms (Microsoft Azure) (nice to have)": [
            [
                "cloud data",
                "SOFT"
            ],
            [
                "Azure",
                "HARD"
            ]
        ],
        "Microsoft SQL Server 2019 certification is desirable": [
            [
                "SQL Server",
                "HARD"
            ]
        ]
    },
    {},
    {
        "Data Engineer - Outside IR35 - \u00a3500-550 per day - Remote working - 6 months initial contract.   My client, a globally established engineering organisation, is looking for an experienced Data Engineer to join them on a contract basis.   As a Data Engineer, you will play a pivotal role in designing, developing, and maintaining our data infrastructure and pipelines, enabling our organization to make data-informed decisions.   Key Responsibilities:   Collaborate with enterprise architects to design and implement data structures and systems supporting analytical and reporting needs.   Implement data quality checks, data governance, and validation processes to ensure data integrity and reliability.   Design, develop, and maintain data pipelines for extracting, transforming, and loading (ETL) data from various sources.   Optimize data pipelines and systems to improve query performance, reduce latency, and make data readily available for analysis.   Integrate and centralize data from disparate sources, including databases, APIs, and external platforms.   Develop monitoring solutions to proactively identify and resolve data-related issues and ensure data pipeline reliability.   Work closely with cross-functional teams to understand data requirements and provide support for data-related projects.   Maintain clear and concise documentation of data processes, pipelines, and data dictionaries.    What You Need to Qualify:   Bachelors degree in computer science, information technology, or related field. Advanced degrees are a plus.   Proven experience as a Data Engineer or in a similar role.   Proficiency in programming languages such as Python, Java, or Scala.   Strong knowledge of ETL processes, data warehousing, and database design.   Familiarity with big data technologies and frameworks, such as Hadoop, Spark, and Kafka.   Experience with data modelling, data transformation, and data quality best practices.   Proficiency in SQL and database management systems (e.g., PostgreSQL, MySQL, or NoSQL databases).   Knowledge of cloud platforms like AWS, Azure, or Google Cloud.   Excellent problem-solving skills and attention to detail.   Strong communication and teamwork skills.   Experience with data visualization tools (e.g., Tableau, Power BI).": [
            [
                "pipelines",
                "SOFT"
            ],
            [
                "systems",
                "SOFT"
            ],
            [
                "analytical",
                "SOFT"
            ],
            [
                "data governance",
                "SOFT"
            ],
            [
                "validation processes",
                "SOFT"
            ],
            [
                "data pipelines",
                "SOFT"
            ],
            [
                "loading",
                "SOFT"
            ],
            [
                "ETL",
                "SOFT"
            ],
            [
                "data pipelines",
                "SOFT"
            ],
            [
                "improve query",
                "SOFT"
            ],
            [
                "databases",
                "SOFT"
            ],
            [
                "APIs",
                "SOFT"
            ],
            [
                "data pipeline",
                "SOFT"
            ],
            [
                "data processes",
                "SOFT"
            ],
            [
                "pipelines",
                "SOFT"
            ],
            [
                "computer science",
                "CERT"
            ],
            [
                "Python",
                "HARD"
            ],
            [
                "Java",
                "HARD"
            ],
            [
                "Scala",
                "HARD"
            ],
            [
                "ETL",
                "SOFT"
            ],
            [
                "data warehousing",
                "SOFT"
            ],
            [
                "database design",
                "SOFT"
            ],
            [
                "big data",
                "SOFT"
            ],
            [
                "Hadoop",
                "HARD"
            ],
            [
                "Spark",
                "HARD"
            ],
            [
                "Kafka",
                "HARD"
            ],
            [
                "data modelling",
                "SOFT"
            ],
            [
                "data transformation",
                "SOFT"
            ],
            [
                "data quality",
                "SOFT"
            ],
            [
                "SQL",
                "HARD"
            ],
            [
                "database management",
                "SOFT"
            ],
            [
                "PostgreSQL",
                "HARD"
            ],
            [
                "MySQL",
                "HARD"
            ],
            [
                "NoSQL",
                "HARD"
            ],
            [
                "cloud",
                "SOFT"
            ],
            [
                "AWS",
                "HARD"
            ],
            [
                "Azure",
                "HARD"
            ],
            [
                "Google Cloud",
                "HARD"
            ],
            [
                "problem-solving",
                "SOFT"
            ],
            [
                "attention to detail",
                "SOFT"
            ],
            [
                "communication",
                "SOFT"
            ],
            [
                "visualization",
                "SOFT"
            ],
            [
                "Tableau",
                "HARD"
            ],
            [
                "Power BI",
                "HARD"
            ]
        ]
    },
    {
        "UK Data & Analytics \u2013 Data Engineer": [],
        "Do you have an interest in Data & Analytics and a desire to expand your skillset and further your career? Are you keen to join a lively team, using your investigative skills to achieve the best outcome for the client? Are you looking for an opportunity to grow and develop in a hybrid role with the flexibility to work virtually and from an Aon office?": [],
        "If so, get ready to embark on a career at Aon! As a data engineer within the UK Data & Analytics team, you will join a growing team which plays a pivotal role within Aon Business Services; innovating, developing, and delivering market leading analytics to our stakeholders. Our standard of excellence accelerates growth and empowers the business to bring the best of Aon to our clients": [],
        "Aon is in the business of better decisions. At Aon, we shape decisions for the better to protect and enrich the lives of people around the world. As an organisation, we are united through trust as one inclusive, diverse team, and we are passionate about helping our colleagues and clients succeed.": [],
        "What your day will look like": [],
        "How this opportunity is different": [],
        "This is an exceptional opportunity to build and influence the reporting landscape across Aon UK": [],
        "The UK Data & Analytics team empowers the business to make better decisions. With our data expertise, we deliver sophisticated analytics to fuel growth, meet client need and boost innovation across the business bringing the future of data analytics to our clients today. With a huge amount of resource and expertise in the team you'll be equipped with all the tools and training you need to succeed.": [],
        "Skills and experience that will lead to success": [],
        "How we support our colleagues": [],
        "In addition to our comprehensive benefits package, we encourage a diverse workforce. Plus, our agile, inclusive environment allows you to manage your wellbeing and work/life balance, ensuring you can be your best self at Aon. Furthermore, all colleagues enjoy two \u201cGlobal Wellbeing Days\u201d each year, encouraging you to take time to focus on yourself. We offer a variety of working style solutions, but we also recognise that flexibility goes beyond just the place of work... and we are all for it. We call this Smart Working!": [],
        "Our continuous learning culture inspires and equips you to learn, share and grow, helping you achieve your fullest potential. As a result, at Aon, you are more connected, more relevant, and more valued. We provide individuals with disabilities reasonable accommodations to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment in accordance with applicable law. Please contact us to request an accommodation on ": [],
        "Aon values an innovative, diverse workplace where all colleagues feel empowered to be their authentic selves. Aon is proud to be an equal opportunity workplace.": [],
        "#LI-JK1": [],
        "#LI-HYBRID": [
            [
                "LI",
                "SOFT"
            ],
            [
                "HYBRID",
                "SOFT"
            ]
        ],
        "Collaborate with our partners to support and develop new and existing solutions, ensuring they are kept informed throughout.": [],
        "Build new data pipeline techniques to ingest data to drive decisions for our clients": [
            [
                "data pipeline",
                "SOFT"
            ],
            [
                "ingest data",
                "SOFT"
            ]
        ],
        "Anticipate, identify and define problems and design and implement solutions to them": [],
        "Test new development items in advance of release to partners to uphold the UK D&A standard of excellence prior to User Acceptance Testing": [
            [
                "Testing",
                "SOFT"
            ]
        ],
        "Identify improvements for UK D&A team standards; implement and communicate these amongst the team.": [],
        "Good knowledge of SQL, DevOps tooling and the software development lifecycle": [
            [
                "SQL",
                "HARD"
            ],
            [
                "DevOps",
                "SOFT"
            ],
            [
                "software development",
                "SOFT"
            ]
        ],
        "Good data management and modelling skills using industry-leading technologies such as Snowflake, Databricks, Python and PySpark.": [
            [
                "data management",
                "SOFT"
            ],
            [
                "modelling",
                "SOFT"
            ],
            [
                "Snowflake",
                "HARD"
            ],
            [
                "Databricks",
                "HARD"
            ],
            [
                "Python",
                "HARD"
            ],
            [
                "PySpark",
                "HARD"
            ]
        ],
        "Experience building data pipelines and developing and using data APIs.": [
            [
                "data pipelines",
                "SOFT"
            ],
            [
                "developing",
                "SOFT"
            ],
            [
                "using data",
                "SOFT"
            ]
        ],
        "Ability to communicate technical and business concepts both verbally and in writing either independently or in collaboration with the wider team.": [],
        "Ability to work in a team environment and to form positive relationships with collaborators of all levels whether in person or virtually.": []
    },
    {
        "Job Role Description": [],
        "Position Purpose: In this role, you will provide support to our Technology Team by overseeing the management, maintenance, and administration of our data processes and external CRM systems.": [
            [
                "CRM",
                "SOFT"
            ]
        ],
        "You'll be working with internal MySQL databases and supporting external CRMs for various campaigns, ensuring smooth data flows to enhance campaign insights and contribute to electoral success.": [
            [
                "MySQL databases",
                "HARD"
            ],
            [
                "CRMs",
                "HARD"
            ],
            [
                "smooth data",
                "SOFT"
            ]
        ],
        "Your primary responsibilities will involve collaborating with a diverse team on a range of projects, ensuring top-quality data is readily available to support our staff and volunteer teams involved in campaigns across the nation.": [
            [
                "top-quality data",
                "SOFT"
            ]
        ],
        "Key Responsibilities:": [],
        "Candidate Qualifications: Essential:": [],
        "Desirable:": [],
        "Comprehend and assist in the management and upkeep of our data processes and CRM databases (Salesforce and NGPVAN Connect).": [
            [
                "CRM databases",
                "SOFT"
            ]
        ],
        "Take charge of data import procedures to ensure the highest data quality within our systems.": [
            [
                "data quality",
                "SOFT"
            ]
        ],
        "Aid HQ teams by generating data exports and customized reports.": [],
        "Resolve CRM database issues and provide technical support to the technology team and HQ.": [
            [
                "CRM database",
                "SOFT"
            ]
        ],
        "Offer support, fixes, and enhancements to existing data processes.": [],
        "Collaborate closely with external technology providers to ensure their work aligns with our standards and quality expectations.": [],
        "Maintain data accuracy, security, and GDPR compliance, along with other regulatory requirements.": [
            [
                "GDPR compliance",
                "SOFT"
            ]
        ],
        "Proficient in SQL, with skills in debugging and troubleshooting relational databases.": [
            [
                "SQL",
                "HARD"
            ],
            [
                "debugging",
                "SOFT"
            ],
            [
                "troubleshooting relational databases",
                "SOFT"
            ]
        ],
        "Capable of working within a team to translate requirements into technical specifications.": [],
        "Strong grasp of data processes (ETL) and experience with both internal MySQL databases and external CRM providers.": [
            [
                "data processes",
                "SOFT"
            ],
            [
                "ETL",
                "SOFT"
            ],
            [
                "MySQL databases",
                "HARD"
            ],
            [
                "CRM",
                "SOFT"
            ]
        ],
        "Competent in CRM database administration (Salesforce and NGPVAN Connect).": [
            [
                "CRM database",
                "SOFT"
            ]
        ],
        "Familiarity with G Suite, IaaS, SaaS, agile development, and open working approaches.": [
            [
                "G Suite",
                "HARD"
            ],
            [
                "IaaS",
                "HARD"
            ],
            [
                "SaaS",
                "SOFT"
            ],
            [
                "agile development",
                "SOFT"
            ]
        ],
        "Adaptive communication skills suitable for collaborating with individuals of varying seniority levels.": [],
        "Ability to adapt to non-standard Salesforce configurations, data models, and use cases.": [],
        "Coding experience in NodeJS.": [
            [
                "NodeJS",
                "HARD"
            ]
        ],
        "Proven ability to work with and understand the needs of volunteers.": [],
        "Proficient at managing and directing external suppliers, particularly regarding technical guidance and consistency.": [],
        "Salesforce certifications, including admin or platform app builder certifications.": [
            [
                "Salesforce",
                "SOFT"
            ]
        ]
    },
    {
        "< Previous Job | Return to search results |": [],
        "Looking to make a bigger impact in your career? Join our dynamic Infrastructure team, and you will have a fantastic opportunity to work with the latest and key secure technologies, develop specialist knowledge and skills, and enjoy some fantastic Civil Service benefits - all while helping to safeguard people, assets and information around the world.": [],
        "Fix complex infrastructure issues as quickly as possible": [],
        "As a subject matter expert within our 3rd line Infrastructure team, you'll be responsible for developing, configuring and resolving complex technical issues in your specialist area. You'll take full ownership and provide specialist knowledge and guidance to your customers and colleagues. In addition, you'll identify and escalate repeatable issues to help ensure they don't happen again.": [],
        "Use your expertise to benefit colleagues and customers alike": [],
        "You'll be expected to mentor junior staff members and to provide authoritative technical advice to a wide range of customers and stakeholders, including members of our change management team. It is therefore essential that you combine excellent communication skills with a solid understanding of how systems work. Specific areas of knowledge we're looking for include:": [
            [
                "communication",
                "SOFT"
            ]
        ],
        "It goes without saying that you'll be a subject matter expert, you'll also hold one or more the following qualifications:": [],
        "Enjoy great career opportunities - and great rewards": [],
        "Our government partners look to us to respond with agility and innovation to their ever-changing security challenges. So there will be plenty of opportunities for you to grapple with complex issues and find ever-more effective solutions to the issues our clients face. You will also enjoy excellent support for career progression, including training on all the latest technologies to help you keep up to speed. Then there's all the benefits we offer. Excellent work-life balance. Flexible working opportunities. On-site OFSTED-registered nursery. Civil Service pension. And lots more. In short, it's your job bigger.": [
            [
                "Flexible working opportunities",
                "PERK"
            ]
        ],
        "All our employees have to be security cleared before joining us, so you will need to undergo a vetting process if you're successful in your application. This role would require you to undergo Developed Vetting (DV) clearance. You can find out more about vetting on our website. You can find out more about vetting on our website - https://www.fcdoservicescareers.co.uk/how-to-apply/": [],
        "Candidates who are judged to be close to meeting the criteria may be considered for other positions in FCDO Services which may be at a lower grade, but have a potential skills match": [],
        "It takes a diverse team to protect a diverse world. The vital work we do takes an incredible community of colleagues, with different skills, backgrounds, cultures and identities. We support every individual, so that you always know you're welcome and valued. It's what makes us a Disability Confident employer. And why we're recognised as a 'Carer Confident' workplace. And it's how you know you're joining an inspiring, inclusive organisation.": [],
        "Hanslope Park based posts attract a Location Allowance of \u00a31,750 per annum.": [],
        "FCDO Services are regulated by the Civil Service Commission.": [
            [
                "FCDO Services",
                "PERK"
            ]
        ],
        "Salary \u00a343,026 - \u00a350,491 plus an allowance of up to \u00a35,000 depending on qualifications and experience plus \u00a31,750 location allowance Location Hanslope Park Closing Date 16 October 2023 at midnight Reference 6655": [
            [
                "Salary \u00a343,026 - \u00a350,491 plus an allowance of",
                "PERK"
            ]
        ],
        "Hadoop / Kafka/ Spark": [
            [
                "Hadoop",
                "HARD"
            ],
            [
                "Kafka/ Spark",
                "HARD"
            ]
        ],
        "SQL/MongoDB/": [
            [
                "SQL",
                "HARD"
            ]
        ],
        "Go / Python / C++": [
            [
                "Python",
                "HARD"
            ],
            [
                "C++",
                "HARD"
            ]
        ],
        "Redshift": [
            [
                "Redshift",
                "HARD"
            ]
        ],
        "AWS Certified Data Analytics": [
            [
                "AWS Certified Data Analytics",
                "CERT"
            ]
        ],
        "Data Engineering Certification": [],
        "Azure Data Analytics": [
            [
                "Azure Data Analytics",
                "HARD"
            ]
        ],
        "GC Professional Data Engineer": []
    },
    {},
    {},
    {
        "Job Title: Data Engineer": [],
        "Location: Chiswick Park": [],
        "Salary:\u00a360k": [],
        "About the Role:": [],
        "Our valued client seeks a Data Engineer to join our Engineering team in the Digital & Product Department. This role involves designing scalable data pipelines, automating data ingestion and cleansing, and collaborating with cross-functional teams to achieve data integration solutions.": [
            [
                "data pipelines",
                "SOFT"
            ],
            [
                "data ingestion",
                "SOFT"
            ]
        ],
        "Roles & Responsibilities:": [],
        "Proficiently comprehend, construct, and advance ETL (Extract, Transform, Load) and data integration solutions, employing a diverse range of technologies and data origins.": [
            [
                "Proficiently comprehend",
                "PERK"
            ],
            [
                "ETL",
                "SOFT"
            ],
            [
                "Transform",
                "SOFT"
            ],
            [
                "Load",
                "SOFT"
            ],
            [
                "data integration",
                "SOFT"
            ]
        ],
        "Expectations & Requirements:": [],
        "Analyse and systematically structure raw data to derive meaningful insights.": [
            [
                "Analyse",
                "SOFT"
            ]
        ],
        "Integrate unprocessed information from various origins into a cohesive dataset.": [
            [
                "dataset",
                "SOFT"
            ]
        ],
        "Innovatively explore strategies to augment data quality and dependability.": [
            [
                "augment data quality",
                "SOFT"
            ]
        ],
        "Recognize, devise, and execute internal process enhancements, including the automation of manual tasks and streamlining data delivery, among other optimizations.": [
            [
                "automation",
                "SOFT"
            ],
            [
                "streamlining data delivery",
                "SOFT"
            ]
        ],
        "Engage with cloud-based infrastructure, specifically Azure, to host data solutions and applications.": [
            [
                "cloud",
                "SOFT"
            ],
            [
                "Azure",
                "HARD"
            ]
        ],
        "Foster collaboration with architects, data analysts, and data scientists to align efforts with the attainment of business objectives.": [],
        "Previous data engineering experience.": [],
        "Proven expertise in developing and maintaining ETL/ELT processes in a medallion architecture.": [
            [
                "ETL",
                "SOFT"
            ],
            [
                "ELT processes",
                "SOFT"
            ]
        ],
        "Strong SQL proficiency and experience with relational databases.": [
            [
                "SQL",
                "HARD"
            ],
            [
                "relational databases",
                "SOFT"
            ]
        ],
        "Proficiency in Azure data engineering tools: Azure Data Factory, Azure Synapse, Azure Data Lake.": [
            [
                "Azure data engineering",
                "SOFT"
            ],
            [
                "Azure Data Factory",
                "HARD"
            ],
            [
                "Azure Synapse",
                "HARD"
            ],
            [
                "Azure Data Lake",
                "HARD"
            ]
        ],
        "Analytical skills for working with structured and unstructured datasets.": [
            [
                "Analytical",
                "SOFT"
            ]
        ],
        "Prior experience with Azure DevOps and understanding of CI/CD.": [
            [
                "Azure DevOps",
                "HARD"
            ],
            [
                "CI/CD",
                "SOFT"
            ]
        ],
        "Familiarity with Apache Spark, including PySpark scripting.": [
            [
                "Apache Spark",
                "HARD"
            ],
            [
                "PySpark",
                "HARD"
            ]
        ],
        "Desirable: Previous experience in creating Data Models.": [
            [
                "creating Data",
                "SOFT"
            ]
        ],
        "Excellent written and verbal communication skills.": [
            [
                "written",
                "SOFT"
            ],
            [
                "verbal communication",
                "SOFT"
            ]
        ],
        "Experience collaborating with cross-functional teams in a dynamic environment.": []
    },
    {
        "You'll be the voice of our customers, using data to tell their stories and put them at the heart of all decision-making": [],
        "We'll look to you to drive the build of effortless, digital first customer experiences": [],
        "If you're ready for a new challenge and want to make a far-reaching impact through your work, this could be the opportunity you're looking for": [],
        "Building advanced automation of data engineering pipelines through removal of manual stages": [
            [
                "automation",
                "SOFT"
            ],
            [
                "pipelines",
                "SOFT"
            ]
        ],
        "Embedding new data techniques into our business through role modelling, training, and experiment design oversight": [
            [
                "data",
                "SOFT"
            ]
        ],
        "Delivering a clear understanding of data platform costs to meet your departments cost saving and income targets": [],
        "Sourcing new data using the most appropriate tooling for the situation": [],
        "Developing solutions for streaming data ingestion and transformations in line with our streaming strategy": [
            [
                "data ingestion",
                "SOFT"
            ]
        ],
        "Experience of designing and implementing solutions that use Entity Resolution and network Generation, ideally Quantexa": [
            [
                "network Generation",
                "SOFT"
            ]
        ],
        "Demonstrable experience of leading and improving engineering teams": [],
        "Experience of ETL technical design, data quality testing, cleansing and monitoring, data sourcing, and exploration and analysis": [
            [
                "ETL",
                "SOFT"
            ],
            [
                "data quality testing",
                "SOFT"
            ],
            [
                "data sourcing",
                "SOFT"
            ],
            [
                "exploration",
                "SOFT"
            ],
            [
                "analysis",
                "SOFT"
            ]
        ],
        "A good understanding of modern code development practices": [
            [
                "code development",
                "SOFT"
            ]
        ],
        "Experience of working in a governed, and regulatory environment": [],
        "Strong communication skills with the ability to proactively engage and manage a wide range of stakeholders": [
            [
                "communication skills",
                "SOFT"
            ]
        ]
    },
    {
        "Apply now \u00bb": [],
        "Req ID #: 219816 Location:": [],
        "Cambridge, GB, CB23 6DP Port Seton, GB, EH32 0TD Tranent, GB, EH33 1EH Harlow, GB, CM19 5TR Dublin, IE, D09": [
            [
                "IE",
                "HARD"
            ],
            [
                "D09",
                "HARD"
            ]
        ],
        "At Charles River, we are passionate about improving the quality of people's lives. When you join our global family, you will help create healthier lives for millions of patients and their families.": [],
        "Charles River employees are innovative thinkers, who are dedicated to continuous learning and improvement. We will empower you with the resources you need to grow and develop in your career.": [
            [
                "continuous learning",
                "SOFT"
            ]
        ],
        "As a Charles River employee, you will be part of an industry-leading, customer-focused company at the forefront of drug development. Your skills will play a key role in bringing life-saving therapies to market faster through simpler, quicker, and more digitalized processes. Whether you are in lab operations, finance, IT, sales, or another area, when you work at Charles River, you will be the difference every day for patients across the globe.": [
            [
                "customer-focused company",
                "SOFT"
            ],
            [
                "lab operations",
                "SOFT"
            ]
        ],
        "Sr. Data & DevOps Engineer - SQL-DBA": [
            [
                "SQL",
                "HARD"
            ],
            [
                "DBA",
                "HARD"
            ]
        ],
        "If you are an experienced Oracle or SQL Server DBA consultant with experience in cloud platforms ( Azure or AWS ) , this job may be for you! This role will work collaboratively, focusing on the company's growing estate of SQL Server and Oracle databases, installing and maintaining the database software, setting up user accounts, managing the backup strategy and regularly maintaining database security. The team is seeking top notch talent in the Database Administrator space.": [
            [
                "Oracle",
                "HARD"
            ],
            [
                "SQL Server DBA",
                "HARD"
            ],
            [
                "cloud",
                "SOFT"
            ],
            [
                "Azure",
                "HARD"
            ],
            [
                "AWS",
                "HARD"
            ],
            [
                "SQL Server",
                "HARD"
            ],
            [
                "Oracle",
                "HARD"
            ],
            [
                "database software",
                "SOFT"
            ],
            [
                "database",
                "SOFT"
            ]
        ],
        "If you are interested in using your leadership, expertise and innovative skills to be part of a company making a difference in the Life Sciences arena, expert in database management both on premise and in the cloud, are a thought leader in the data space, have a passion for data and analytics, and like to mentor others, then we want to talk to you! A truly exciting and unique opportunity awaits you!": [
            [
                "database management",
                "SOFT"
            ],
            [
                "cloud",
                "SOFT"
            ],
            [
                "analytics",
                "SOFT"
            ]
        ],
        "Note : Very strong Oracle & SQL DBA expertise required, it's a fully remote role with some occasional travel needs as per business requirements.": [
            [
                "Oracle",
                "HARD"
            ],
            [
                "SQL",
                "HARD"
            ]
        ],
        "Basic Qualifications:": [
            [
                "Basic Qualifications:",
                "PERK"
            ]
        ],
        "\u00b7 Installing and maintaining Relational database software.": [
            [
                "Relational database software",
                "SOFT"
            ]
        ],
        "\u00b7 Creating storage database structures with high-level security features.": [
            [
                "database",
                "SOFT"
            ]
        ],
        "\u00b7 Altering storage structures to meet the evolving needs of the company.": [],
        "\u00b7 Setting up database user accounts.": [
            [
                "database user accounts",
                "SOFT"
            ]
        ],
        "\u00b7 Finding and debugging malfunctioning programs affecting the database integrity.": [
            [
                "debugging malfunctioning",
                "SOFT"
            ],
            [
                "database",
                "SOFT"
            ]
        ],
        "\u00b7 Creating autonomous database backups.": [
            [
                "database",
                "SOFT"
            ]
        ],
        "\u00b7 Regularly updating database security protocols.": [
            [
                "database",
                "SOFT"
            ]
        ],
        "\u00b7 Good understanding of Relational Database Management Systems (RDBMS) running in the cloud with IaaS and PaaS services": [
            [
                "Relational Database Management Systems",
                "SOFT"
            ],
            [
                "IaaS",
                "HARD"
            ],
            [
                "PaaS",
                "HARD"
            ]
        ],
        "\u00b7 Experience with Cloud infrastructure, Databases, Storage, Databricks, Azure Data Factory (AWS, Azure preferred)": [
            [
                "Cloud",
                "SOFT"
            ],
            [
                "Databricks",
                "HARD"
            ],
            [
                "Azure Data Factory",
                "HARD"
            ],
            [
                "AWS",
                "HARD"
            ],
            [
                "Azure",
                "HARD"
            ]
        ],
        "Qualifications / Requirements": [],
        "\u00b7 Bachelor's degree in computer engineering or computer science.": [
            [
                "computer engineering",
                "CERT"
            ],
            [
                "computer science",
                "CERT"
            ]
        ],
        "\u00b7 very strong work experience as a Database administrator.": [],
        "\u00b7 Familiarity with RDMS design, coding, and documentation.": [],
        "\u00b7 Knowledge of database backup procedures, recovery systems, and SQL.": [
            [
                "database backup",
                "SOFT"
            ],
            [
                "SQL",
                "HARD"
            ]
        ],
        "\u00b7 Great presentation skills": [
            [
                "presentation",
                "SOFT"
            ]
        ],
        "About Corporate Functions The Corporate Functions provide operational support across Charles River in areas such as Human Resources, Finance, IT, Legal, Sales, Quality Assurance, Marketing, and Corporate Development. They partner with their colleagues across the company to develop and drive strategies and to set global standards. The functions are essential to providing a bridge between strategic vision and operational readiness, to ensure ongoing functional innovation and capability improvement.": [],
        "About Charles River Charles River is an early-stage contract research organization (CRO). We have built upon our foundation of laboratory animal medicine and science to develop a diverse portfolio of discovery and safety assessment services, both Good Laboratory Practice (GLP) and non-GLP, to support clients from target identification through preclinical development. Charles River also provides a suite of products and services to support our clients' clinical laboratory testing needs and manufacturing activities. Utilizing this broad portfolio of products and services enables our clients to create a more flexible drug development model, which reduces their costs, enhances their productivity and effectiveness to increase speed to market.": [
            [
                "Laboratory Practice",
                "SOFT"
            ],
            [
                "non-GLP",
                "SOFT"
            ],
            [
                "preclinical development",
                "SOFT"
            ],
            [
                "Charles",
                "CERT"
            ],
            [
                "drug development",
                "SOFT"
            ]
        ],
        "With over 20,000 employees within 110 facilities in over 20 countries around the globe, we are strategically positioned to coordinate worldwide resources and apply multidisciplinary perspectives in resolving our client's unique challenges. Our client base includes global pharmaceutical companies, biotechnology companies, government agencies and hospitals and academic institutions around the world. At Charles River, we are passionate about our role in improving the quality of people's lives. Our mission, our excellent science and our strong sense of purpose guide us in all that we do, and we approach each day with the knowledge that our work helps to improve the health and well-being of many across the globe. We have proudly supported the development of 86% of the drugs approved by the FDA in 2021.": [
            [
                "86% of the drugs approved",
                "PERK"
            ]
        ],
        "Equal Employment Opportunity Charles River Laboratories is an Equal Opportunity Employer - all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.": [
            [
                "Equal Employment Opportunity",
                "PERK"
            ]
        ],
        "If you are interested in applying to Charles River Laboratories and need special assistance or an accommodation due to a disability to complete any forms or to otherwise participate in the resume submission process, please contact a member of our Human Resources team by sending an e-mail message to [email protected]. This contact is for accommodation requests for individuals with disabilities only and cannot be used to inquire about the status of applications.": [],
        "For more information, please visit www.criver.com.": [],
        "Job Segment: Biology, Biotech, Database, Cloud, SQL, Technology, Science Apply now \u00bb": [
            [
                "Database",
                "SOFT"
            ],
            [
                "Cloud",
                "SOFT"
            ],
            [
                "SQL",
                "HARD"
            ],
            [
                "Science",
                "CERT"
            ]
        ]
    },
    {
        "Data Centre M&E Engineer - 45k + 10% Bonus - Slough\u00a0": [
            [
                "10% Bonus - Slough\u00a0",
                "PERK"
            ]
        ],
        "I am currently recruiting on behalf of a Data Centre business who have a need for an M&E Engineer to come and join their team on a flagship site in Slough. This is an excellent opportunity to work client direct at a thriving business who will provide training and progression in the critical data centre sector\u00a0": [],
        "The successful applicant will be required to work a Monday - Friday working week with the flexibilty to work overtime when required\u00a0": [],
        "Your role will be to ensure all critical plant on site is working in a safe and efficient manager. The site uses both Electrical and Mechanical Plant inclusive of AHU's, FCU's, CRAC Units, Down Flow Units, Generators, UPS, lighting, BMS, battery banks etc.\u00a0": [
            [
                "Flow Units",
                "PERK"
            ],
            [
                "BMS",
                "HARD"
            ]
        ],
        "LV & HV Shutdowns will take place onsite so a working knowledge would be a bonus\u00a0": [],
        "Required Qualifications & Attributes:\u00a0": [],
        "If the above is of some interest please get in touch with Josh @ InVictus\u00a0": [],
        "All applicants must come from an Electrical background with the relevant City & Guilds qualifications in Electrical Installation\u00a0": [
            [
                "Electrical Installation",
                "CERT"
            ]
        ],
        "17th/18th edition is a must\u00a0": [],
        "A strong working\u00a0knowledge\u00a0of Electrical Maintenance/Building Services is required for the role\u00a0": [],
        "Previous critical site experience is desired\u00a0": [],
        "Previous HV/LV experience will be look at with interest\u00a0": []
    },
    {
        "Quant Data Developer": [],
        "London": [],
        "\u00a3120,000 to \u00a3150,000 base  ": [],
        "Overview:": [],
        " Key Responsibilities of a Quantitative Data Developer:  ": [],
        " Required Skills and Experience for a Python Data Engineer:  ": [
            [
                "Python Data Engineer",
                "CERT"
            ]
        ],
        " If your experience matches the role, click apply and let's catch up!": [],
        "McGregor Boyall is an equal opportunity employer and do not discriminate on any grounds. ": [
            [
                "McGregor",
                "CERT"
            ]
        ],
        "McGregor Boyall is thrilled to be partnered with a global hedge fund searching for a Python Data Engineer": [
            [
                "McGregor",
                "CERT"
            ],
            [
                "Python Data Engineer",
                "CERT"
            ]
        ],
        "As part of this dynamic organization, you will collaborate closely with quant researchers and data teams to test and evaluate new alternative datasets.": [],
        "Keywords: Python, Quantitative, Data Engineer, Developer, Python, C#, C++, NLP": [
            [
                "Python",
                "SOFT"
            ],
            [
                "Python",
                "HARD"
            ],
            [
                "C#",
                "HARD"
            ],
            [
                "C++",
                "HARD"
            ],
            [
                "NLP",
                "SOFT"
            ]
        ],
        "Salary range - \u00a3120,000 to \u00a3150,000 base + package (flexible depending on experience)": [
            [
                "Salary range - \u00a3120,000",
                "PERK"
            ]
        ],
        "Test/evaluate new alternative datasets in collaboration with quant researchers and data teams": [],
        "Leverage ML and statistical approaches to automate extraction of predictive signals": [
            [
                "ML",
                "SOFT"
            ],
            [
                "statistical",
                "SOFT"
            ],
            [
                "automate",
                "SOFT"
            ],
            [
                "predictive",
                "SOFT"
            ]
        ],
        "Lead end-to-end DS projects: data prep, exploration, prototyping, and deployment": [
            [
                "prototyping",
                "SOFT"
            ]
        ],
        "Apply diverse techniques like supervised/unsupervised learning, NLP, anomaly detection": [
            [
                "NLP",
                "SOFT"
            ],
            [
                "anomaly detection",
                "SOFT"
            ]
        ],
        "Partner with data engineers, quants, and others to architect robust systems": [
            [
                "systems",
                "SOFT"
            ]
        ],
        "Proficiency in Python, C/C++, C# preferred": [
            [
                "Python",
                "HARD"
            ],
            [
                "C",
                "HARD"
            ],
            [
                "C++",
                "HARD"
            ],
            [
                "C#",
                "HARD"
            ]
        ],
        "Passion for financial markets, data analysis, and coding": [
            [
                "data analysis",
                "SOFT"
            ],
            [
                "coding",
                "SOFT"
            ]
        ],
        "Skilled in structured and unstructured data management": [
            [
                "data management",
                "SOFT"
            ]
        ],
        "Knowledge of cloud platforms like AWS, GCP": [
            [
                "cloud",
                "SOFT"
            ],
            [
                "AWS",
                "HARD"
            ],
            [
                "GCP",
                "HARD"
            ]
        ],
        "Intellectually curious to uncover new datasets": [
            [
                "datasets",
                "SOFT"
            ]
        ],
        "Able to work independently within a global team": [],
        "Post Graduate degree in Data Science, Computer Engineering, Mathematics or Statistics": [
            [
                "Graduate",
                "CERT"
            ],
            [
                "Data Science",
                "CERT"
            ],
            [
                "Computer Engineering",
                "CERT"
            ]
        ],
        "3+ years as Data Engineer/Analyst/Scientist": [],
        "Background in NLP required": [
            [
                "NLP",
                "SOFT"
            ]
        ],
        "Thrives in fast-paced, high-performance culture": [
            [
                "fast-paced",
                "SOFT"
            ]
        ]
    },
    {
        "Role OVO-View": [],
        "Location: Hub based! Bristol, London, Glasgow or Remote! (But you have the flexibility to work wherever suits you best)Team: Platform Technology, Data PlatformSalary banding: \u00a355,770 - \u00a380,170Experience: Mid-level / SeniorWorking pattern:Full-TimeReporting to: Software Engineering ManagerSponsorship: Unfortunately we are unable to offer sponsorship for this role.This role in 3 words: Explore, Cross-collaboration, OwnershipTop 3 qualities for this role: Communication, Collaborative, Adaptability": [
            [
                "PlatformSalary banding: \u00a355,770 - \u00a380,170Experience",
                "PERK"
            ]
        ],
        "Everyone belongs at OVO": [],
        "At OVO, we are on a mission to solve one of humanity's biggest challenges, the climate crisis. And we know it takes all of us to change the world. That's why we need diverse people from all gender identities, ethnicities, ages, sexual orientations, life experiences and backgrounds to join us.": [],
        "Teamworking for the planet": [],
        "Everything we do here spins around Plan Zero. So, naturally, the team you\u2019ll be joining plays a gigantic role in making that happen. Here\u2019s how:": [],
        "We\u2019re hiring world-changers. Every role we\u2019re hiring plays their own part in our mission; our role is to find those people and bring them on our Zero Carbon journey. Across OVO we have around 350 Engineers all with varying backgrounds and levels of experience. One key thing that all of our engineers have in common is a desire to develop brilliant, industry redefining products as well as their own skills.": [],
        "This role in a nutshell:": [
            [
                "nutshell",
                "SOFT"
            ]
        ],
        "You\u2019ll be part of OVO\u2019s Data Engineering Team, which will act as a Centre of Excellence for data engineering across OVO, providing strategy, advice and guidance, reusable approaches, patterns and assets, and tool recommendations. This team creates and facilitates a Data Engineering community to support data engineers working in product and business teams as they build data pipelines and transform data for new products and for analytics and reporting.": [
            [
                "data pipelines",
                "SOFT"
            ],
            [
                "transform data",
                "SOFT"
            ],
            [
                "analytics",
                "SOFT"
            ]
        ],
        "Your key outcomes will be:": [],
        "You\u2019ll be a successful Data Engineer at OVO if you have\u2026": [],
        "Let\u2019s talk about what\u2019s in it for you": [],
        "We\u2019ll pay you between \u00a355,770 and \u00a380,170, depending on your specific skills and experience. If your expectations are a little different, have a chat with us!": [],
        "We keep our pay ranges broad on purpose to give us, and you, flexibility to match your experience to our zero carbon mission.": [],
        "You\u2019ll be eligible for an on-target bonus of 15%. We have one OVO bonus plan that focuses on the collective performance of our people to deliver our Plan Zero goal. ": [
            [
                "15%. We have one OVO bonus plan",
                "PERK"
            ]
        ],
        "We also offer plenty of green benefits and progressive policies to help you feel like you belong at OVO\u2026and there\u2019s flex pay. It\u2019s an extra 9% of your salary on top of your core pay to use as you like. You can take it as cash, add to your pension, or choose to spend it on a huge range of flex benefits. Here\u2019s a taster of what\u2019s on offer: ": [
            [
                "9% of your salary on top of your core pay",
                "PERK"
            ]
        ],
        "For starters, you\u2019ll get 34 days of holiday (including bank holidays). For your healthWith benefits like a healthcare cash plan or private medical insurance depending on your career level, critical illness cover, life assurance, health assessments, and moreFor your wellbeingWith gym membership, gadget, travel and cyber insurance, workplace ISA, will writing services, DNA testing, dental insurance, and more For your lifestyle With extra holiday buying, discount dining, culture cards, tech loans, and supporting your favourite charities with give-as-you-earn donationsFor your home Get up to \u00a3300 off any OVO Energy plan (when you pay by Direct Debit), plus personal carbon offsetting and great discounts on smart thermostats and EV chargersFor your commute Nab a great deal on ultra-low emission car leasing, plus our cycle to work scheme and public transport season ticket loans Want to hear about our full range of flexible benefits and progressive people policies? Our People Team can tell you everything you need to know.": [
            [
                "dental insurance",
                "PERK"
            ],
            [
                "holiday buying",
                "PERK"
            ],
            [
                "benefits and progressive people policies? Our People Team can tell you everything you need to know",
                "PERK"
            ]
        ],
        "For your Belonging": [],
        "To find better ways to support our people, we need to listen to each other\u2019s experiences and find ways to build a truly inclusive and diverse workplace. As part of this, we have 8 Belonging Networks at OVO. Led by our people, for our people - so when you join OVO, you can play a part - big or small - with any of the Networks. It's up to you.": [],
        "Oh, and one last thing...": [],
        "We\u2019d be thrilled if you tick off all our boxes yet we also believe it\u2019s just as important we tick off all of yours. And if you think you have most of what we\u2019re looking for but not every single thing, go ahead and hit apply. We\u2019d still love to hear from you!": [],
        "If you have any additional requirements, there\u2019s a space to let us know on the application form; we want to make the process as easy and comfortable for you as possible..": [],
        "Part of a team responsible for setting a strategy for data engineering across OVO and developing guidelines for how we work across our streaming and data warehouse technologies.": [
            [
                "data warehouse",
                "SOFT"
            ]
        ],
        "Building a data engineering community with engineers working in product teams, providing advice and guidance, tool recommendations etc.": [],
        "Building and maintaining integration pipelines that power our automated journeys.": [
            [
                "integration",
                "SOFT"
            ]
        ],
        "Transforming streaming data to meet target schemas.": [],
        "Being part of an agile engineering team where you will have the opportunity to influence technology selection.": [
            [
                "agile engineering",
                "CERT"
            ]
        ],
        "Establishing good data engineering practices including using infrastructure as code; contributing to automated testing strategies; setting up monitoring and alerting tools; employing CI/CD best practices to deploy regularly to production.": [
            [
                "automated testing",
                "SOFT"
            ],
            [
                "monitoring",
                "SOFT"
            ],
            [
                "CI/CD best practices",
                "SOFT"
            ]
        ],
        "Participating/leading efforts in the Open Source community, speaking at conferences, contributing to OVO\u2019s tech blog, etc.": [],
        "Engaging and encouraging engagement in the wider community through conference participation and contributing to our Tech Blog.": [],
        "Working with key stakeholders to understand their data needs and help deliver solutions that provide them with excellent quality data that allows teams to realise their objectives.": [
            [
                "quality data",
                "SOFT"
            ]
        ],
        "Experience of designing, building, monitoring and managing large-scale data products, pipelines, tooling and platforms.": [
            [
                "pipelines",
                "SOFT"
            ]
        ],
        "A track record as a Data Engineer, setting strategy and defining ways of working.": [],
        "Experience working on streaming ETL solutions utilising streaming data processing tools (e.g. Kafka Streams, Kinesis, Spark or similar).Experience developing cloud-based solutions on GCP (preferably), AWS or Azure using Infrastructure as code tools such as Terraform.": [
            [
                "ETL",
                "SOFT"
            ],
            [
                "data processing",
                "SOFT"
            ],
            [
                "Kafka",
                "HARD"
            ],
            [
                "Kinesis",
                "HARD"
            ],
            [
                "Spark",
                "HARD"
            ],
            [
                "similar).Experience",
                "HARD"
            ],
            [
                "cloud",
                "SOFT"
            ],
            [
                "GCP",
                "SOFT"
            ],
            [
                "AWS",
                "HARD"
            ],
            [
                "Azure",
                "HARD"
            ],
            [
                "code tools",
                "SOFT"
            ],
            [
                "Terraform",
                "HARD"
            ]
        ],
        "Excellent knowledge of at least one programming language e.g. Scala, Python, Typescript": [
            [
                "Scala",
                "HARD"
            ],
            [
                "Python",
                "HARD"
            ],
            [
                "Typescript",
                "HARD"
            ]
        ],
        "Knowledge of the best engineering practices and continuous delivery.": [],
        "An understanding that building quality software is essential and you value automation and continuous delivery.": [
            [
                "automation",
                "SOFT"
            ]
        ],
        "A love for building scalable, resilient solutions, and you enjoy influencing the team\u2019s technology selection and architectural direction.": [],
        "You will be comfortable working in an agile software development environment and have experience of CI/CD and deployment strategies": [
            [
                "software development",
                "SOFT"
            ],
            [
                "CI/CD",
                "SOFT"
            ]
        ],
        "A background in Software Engineering": [
            [
                "Software Engineering",
                "CERT"
            ]
        ]
    },
    {
        "Offshore Cable Engineer - HV & EHV Electrical Cables": [
            [
                "Offshore Cable Engineer",
                "PERK"
            ]
        ],
        "Key Responsibilities:": [],
        "Qualifications and Skills:": [],
        "Lead front-end electrical engineering design works related to high voltage (HV) and extra-high voltage (EHV) land and subsea cable networks, crucial for project consenting and financial modelling.": [],
        "Develop electrical package contract documentation, including cable specifications, and manage their preparation.": [],
        "Write, review, and update technical specifications for electrical cable work packages on offshore wind generation projects.": [
            [
                "update technical",
                "SOFT"
            ]
        ],
        "Coordinate and manage project electrical cable engineering works, ensuring the correct application of technical specifications.": [],
        "Review external contractors' and consultants' specifications and documentation for the procurement of long-lead equipment, ensuring compliance with SSE Renewables' requirements.": [
            [
                "SSE Renewables'",
                "SOFT"
            ]
        ],
        "Interpret project objectives to develop the most suitable techno-commercial design solutions and technology choices.": [],
        "Manage 3rd parties to ensure the effective delivery of specific engineering and design works within the cable work package, including quality management and verification of their work.": [
            [
                "quality management",
                "SOFT"
            ]
        ],
        "Electrical Cable Experience - HV / EHV": [],
        "Degree level educated in a relevant field or equivalent.": [],
        "Offshore Experience - ideally within a Wind Farm setting": [
            [
                "Offshore Experience",
                "SOFT"
            ],
            [
                "Wind Farm",
                "SOFT"
            ]
        ],
        "Inter Array Cable experience.": []
    },
    {
        "Design, development, and maintenance of scalable, efficient data pipelines using Azure Data Factory, Azure Databricks and SQL SMS": [
            [
                "data pipelines",
                "SOFT"
            ],
            [
                "Azure Data Factory",
                "HARD"
            ],
            [
                "Azure Databricks",
                "HARD"
            ],
            [
                "SQL",
                "HARD"
            ]
        ],
        "Collaboration within the data team to understand requirements and design models that support business requirements": [],
        "Support the data team through data extraction, transformation, and loading (ETL) to provide efficient data access solutions.": [
            [
                "data extraction",
                "SOFT"
            ],
            [
                "loading",
                "SOFT"
            ],
            [
                "ETL",
                "SOFT"
            ],
            [
                "data access",
                "SOFT"
            ]
        ],
        "Monitor and troubleshoot data pipelines, identifying and resolving any performance issues and proactively working on measures to maintain data availability and reliability": [
            [
                "data pipelines",
                "SOFT"
            ],
            [
                "data availability",
                "SOFT"
            ]
        ],
        "Accountable for accuracy and quality of data pipelines and the transported data.": [
            [
                "data pipelines",
                "SOFT"
            ]
        ],
        "Accountable to Project Managers and Head or Architecture/Data for all actions.": [],
        "Provide mentoring and guidance to wider Digital and ICT teams on best practices in Data Engineering.": [
            [
                "Data Engineering",
                "CERT"
            ]
        ],
        "At least 3 years of experience as Data Engineer in Azure environment for medium/large organisation.": [
            [
                "Azure",
                "HARD"
            ]
        ],
        "Experiene with Power Apps\u00a0": [
            [
                "Power",
                "SOFT"
            ]
        ],
        "Preferable to have Azure qualifications/certifications for data engineering.": [
            [
                "Azure",
                "HARD"
            ]
        ],
        "Master\u2019s or Bachelor\u2019s degree in relevant subjects, or the equivalent on-the-job experience/professional certification.": [
            [
                "relevant subjects",
                "CERT"
            ]
        ],
        "Up to \u00a360K basic salary": [],
        "Pension": [
            [
                "Pension",
                "PERK"
            ]
        ],
        "Healthcare": [],
        "Hybrid \u2013 3 days in the office": [
            [
                "Hybrid",
                "PERK"
            ],
            [
                "3 days",
                "PERK"
            ]
        ],
        "Office Locations: Preston, Doncaster or Hoddesdon": []
    },
    {
        "Login to Send Message": [],
        "Bishops Cleeve, Gloucestershire GCHQ": [],
        "Flexible Working: Full-time and part-time opportunities (flexible working hours available) with a mix of office and remote working dependent on business needs.": [
            [
                "Flexible Working: Full-time",
                "PERK"
            ]
        ],
        "About Us": [],
        "GCHQ is an intelligence, cyber and security agency with a mission to keep the UK safe. We use cutting-edge technology, ingenuity and partnerships to identify, analyse and disrupt threats. Working with our intelligence partners MI5 and MI6, we protect the UK from terrorism, cyber-attacks and espionage. At GCHQ you'll do varied and fascinating work in a supportive and inclusive environment that puts the emphasis on teamwork.": [
            [
                "analyse",
                "SOFT"
            ]
        ],
        "Available Roles": [],
        "Lead Data Scientist": [],
        "As a Lead Data Scientist, you'll use your technical and creative abilities to shape intelligence production at GCHQ.": [],
        "You'll take a lead role in the AI Lab - a multidisciplinary team. As you guide others to develop the solutions that keep us ahead of our adversaries, you'll operate on mission-critical systems. You'll use your expert knowledge and experience to create new tradecraft for a wide range of customers. You'll work across scalable artificial intelligence and machine learning, ensuring we have the necessary insight to overcome fresh challenges and develop new capabilities.": [
            [
                "AI Lab",
                "SOFT"
            ],
            [
                "scalable artificial",
                "SOFT"
            ],
            [
                "machine learning",
                "SOFT"
            ]
        ],
        "As you use data science to support our vital mission, you'll enjoy a senior role that's shaped to suit your strengths and designed to help you grow in new ways. In the AI Lab, you'll find you can focus on anything from Mission to Machine Learning operations (ML Ops), with the knowledge that everything you do will be driven by our mission to protect the country.": [
            [
                "Machine Learning operations (ML",
                "SOFT"
            ]
        ],
        "Lead Data Engineer": [],
        "As a Lead Data Engineer, you'll pioneer work that leads the way for AI development at GCHQ. You'll lead Development Operations (DevOps) and data engineering in the AI Lab, ensuring our team has the best internal and external infrastructure capabilities. Using your technical expertise, you'll advise across projects on tools such as OpenShift, Docker and Kubernetes.": [
            [
                "AI",
                "SOFT"
            ],
            [
                "OpenShift",
                "HARD"
            ],
            [
                "Docker",
                "HARD"
            ],
            [
                "Kubernetes",
                "HARD"
            ]
        ],
        "You'll also explore new tools, techniques and practices that will enhance our work, steering the team on software engineering best practice in a range of programming languages. And throughout the development lifecycle, you'll interact closely with our engineering and customer teams to enable the smooth transition of products to operational use. Outside of our organisation, you'll build working relationships with industry and partners, growing our technical capability.": [],
        "For both Lead Data Scientist and Lead Data Engineer roles, there may be annual opportunities for overseas travel for collaboration, conferences, and training.": [],
        "About You": [],
        "Whether you join us a Lead Data Scientist or Lead Data Engineer, you'll benefit from having a STEM or computer science related degree, or equivalent qualification.": [
            [
                "STEM",
                "CERT"
            ],
            [
                "degree",
                "CERT"
            ]
        ],
        "If you don't have a degree, don't let that stop you from applying, but you'll need to be able to demonstrate significant experience and expertise.": [],
        "Ideally you'll have operated as a subject matter expert, influencing the technical direction and design of products. Experience of managing teams will be an advantage. You also have an aptitude and passion for passing on knowledge, and developing and mentoring others. And you're experienced in working with end users, and collaborating with internal and external partners.": [],
        "To excel in the Lead Data Scientist role, you'll need a data science or software engineering background. You're also familiar with Python, R, Kubernetes, Docker, JavaScript, Java. And you're knowledgeable about everything from machine learning training, deployment, and maintenance to exploratory data analysis and statistical reasoning. Crucially, you have the technical knowledge to take a prototype Machine Learning capability and build it into a functional system component.": [
            [
                "software engineering",
                "CERT"
            ],
            [
                "Python",
                "HARD"
            ],
            [
                "R",
                "HARD"
            ],
            [
                "Kubernetes",
                "HARD"
            ],
            [
                "Docker",
                "HARD"
            ],
            [
                "JavaScript",
                "HARD"
            ],
            [
                "Java",
                "HARD"
            ],
            [
                "exploratory data analysis",
                "SOFT"
            ],
            [
                "statistical",
                "SOFT"
            ],
            [
                "Machine Learning",
                "SOFT"
            ]
        ],
        "To become our Lead Data Engineer, you'll understand how machine learning can help make the best use of our data. You can communicate technical concepts to a non-technical audience. And you can spot opportunities, working across teams to make them happen. You're comfortable with building relationships and leading engagements with external and international partners, as well as other skill communities. You're also able to understand our Machine Learning Operations (ML Ops) ecosystem and data platforms, and assess emerging opportunities.": [
            [
                "machine learning",
                "SOFT"
            ],
            [
                "Machine Learning Operations (ML",
                "SOFT"
            ]
        ],
        "Training and Development": [
            [
                "Training and Development",
                "PERK"
            ]
        ],
        "At GCHQ, we're committed to giving you all the support and guidance you need to build a fulfilling career at the heart of national security. From day one, you'll have a comprehensive introduction and a buddy to support you, as you settle in. And because everyone has different ways of learning, we'll invest in your skills through reading materials, taught courses, conferences and more.": [
            [
                "reading materials",
                "SOFT"
            ]
        ],
        "All our Data Scientists are given the opportunity to attend training, either as discrete courses, or as part of a programme, such as our Data Science Development Programme (DSDP) which runs over 12weeks and covers:": [],
        "Technical training is further complimented with personal and leadership training available to all staff.": [],
        "Rewards and Benefits": [],
        "You'll receive a starting salary of \u00a348,104 - \u00a353,637 dependent on skills and experience, plus other benefits including:": [],
        "Equal Opportunities": [
            [
                "Equal Opportunities",
                "PERK"
            ]
        ],
        "At GCHQ diversity and inclusion are critical to our mission. To protect the UK, we need a truly diverse workforce that reflects the society we serve. This includes diversity in every sense of the word: those with different backgrounds, ages, ethnicities, gender identities, sexual orientations, ways of thinking and those with disabilities or neurodivergent conditions. We therefore welcome and encourage applications from everyone, including those from groups that are under-represented in our workforce such as women, those from an ethnic minority background, people with disabilities and those from low socio-economic backgrounds.": [],
        "Find out more about our culture, working environment and diversity on our website:": [],
        "We're Disability Confident": [],
        "GCHQ are proud to have achieved Leader status within the DWP's Disability Confident scheme. This is aimed at encouraging employers to think differently about disability and take action to improve how they recruit, retain and develop disabled people. Being Disability Confident, we aim to offer a person-to-person interview to any candidate who self-identifies as disabled. This is our 'Offer of Interview' (OOI).": [],
        "What to Expect": [],
        "Our recruitment process is fair, transparent, and based on merit. Here is a brief overview of each stage, in order:": [],
        "Please note, you must successfully pass each stage of the process to progress to the next. Your application may take around 6 - 9 months to process including vetting, so we advise you continue any current employment until you have received your final job offer.": [],
        "Before You Apply": [],
        "To work at GCHQ you need to be a British citizen or hold dual British nationality. You can read our full eligibility criteria here.": [],
        "This role requires the highest security clearance, known as Developed Vetting (DV). It's something everyone in the UK Intelligence Community undertakes. You can find out more about the vetting process here.": [],
        "Please note we have a strict drugs policy, so once you start your application, you can't take any recreational drugs and you'll need to declare your previous drug usage at the relevant stage.": [],
        "The role is based in Cheltenham, so you'll need to live within a commutable distance. Please consider any financial implications and practicalities before submitting an application, as we do not offer relocation costs.": [],
        "Please note, you should only launch your application from within the UK. If you are based overseas, you should wait until you visit the UK to launch an application. Applying from outside the UK will impact on our ability to progress your application. You should not discuss your application, other than with your partner or a close family member.": [],
        "Exploratory Data Analysis and Visualisation": [
            [
                "Exploratory Data Analysis",
                "SOFT"
            ],
            [
                "Visualisation",
                "SOFT"
            ]
        ],
        "Mathematics and Statistics": [],
        "Machine Learning": [
            [
                "Machine",
                "SOFT"
            ]
        ],
        "Algorithms and Graph Theory": [
            [
                "Algorithms",
                "SOFT"
            ],
            [
                "Graph",
                "SOFT"
            ]
        ],
        "Big Data Technologies": [
            [
                "Big Data Technologies",
                "SOFT"
            ]
        ],
        "25 Days Annual Leave automatically rising to 30 days after 5 years' service, and an additional 10.5 days public and privilege holidays": [
            [
                "25 Days Annual Leave automatically rising",
                "PERK"
            ],
            [
                "30 days after 5 years' service",
                "PERK"
            ]
        ],
        "Opportunities to be recognised through our employee performance scheme": [],
        "Interest-free season ticket loan": [
            [
                "Interest",
                "SOFT"
            ]
        ],
        "Excellent pension scheme": [
            [
                "pension scheme",
                "PERK"
            ]
        ],
        "Cycle to work scheme": [
            [
                "Cycle to work scheme",
                "PERK"
            ]
        ],
        "Facilities such as a gym, restaurant and on-site coffee bars (at some locations)": [],
        "Paid parental and adoption leave": [],
        "CV and Application question sift": [
            [
                "sift",
                "SOFT"
            ]
        ],
        "A competency based and technical interview - looking at your motivation for the role and your understanding of data science": [],
        "If successful, you will receive a conditional offer of employment": [],
        "Date Posted: Posted 26 mins ago": [],
        "Expiration date: 12 November 2023": [],
        "Location: Bishops Cleeve, Gloucestershire": [],
        "Job Title: Lead Data Scientist & Lead Data Engineer": []
    },
    {
        "  Proud member of the Disability Confident employer scheme  ": []
    },
    {
        "Shift Data Centre Maintenance Engineer - Building Services - Slough\u00a0 \u00a355,000 + package and overtime 4 on 4 off days and nights 7am - 7pm": [
            [
                "Shift Data Centre Maintenance Engineer - Building Services - Slough\u00a0 \u00a355,000 + package and overtime 4 on 4 off days",
                "PERK"
            ]
        ],
        "My client is a global facilities management service provider, who are currently working in partnership an world renowned Data Centre operator based in Slough, they are actively recruiting for a Data Centre Shift Maintenance Engineer from either an Electrical or a mechanical background to manage the day-to-day M&E services located at a large Data Centre complex in Slough.": [],
        "Job Specification:": [],
        "\u00b7Working in a shift team of 4 x engineers, ": [],
        "\u00b7Ensuring that routine PPMs are carried out to all mechanical and electrical plant in accordance with site task schedules and asset lists": [],
        "\u00b7PPM and reactive maintenance to power/temporary power, lighting/emergency lighting, sprinkler systems, fire alarm tests, pumps, motors, heat exchangers, air-conditioning plant, AHU\u2019s, FCU\u2019s, VAV\u2019s, CRAC Units, filter systems, and fault-finding": [
            [
                "sprinkler systems",
                "SOFT"
            ],
            [
                "filter systems",
                "SOFT"
            ]
        ],
        "\u00b7HV and LV switching/site shutdowns": [],
        "\u00b7Organising subcontract labour": [],
        "\u00b7Organising quotes and the labour/procurement of materials and parts": [],
        "\u00b7Responsible for Health & Safety, logbooks, risk assessment, permits to work etc.": [],
        "Salary & Package:": [],
        "\u00b7\u00a355,000.00 per annum\u00a0": [],
        "\u00b7Continental Shift Pattern (Days and Nights)": [
            [
                "Continental Shift Pattern (Days and Nights)",
                "PERK"
            ]
        ],
        "\u00b720 Days Annual Leave": [
            [
                "20 Days Annual Leave",
                "PERK"
            ]
        ],
        "\u00b7Full Uniform + Full set of Tools": [],
        "\u00b7Training (HV-AP/LVAP, ILM, etc.)": [],
        "\u00b7Fantastic Opportunity within the Data Centre Industry": [],
        "Qualifications": [],
        "\u00b7City and Guilds level 1, 2, 3 Electrical or Mechanical engineering\u00a0": [],
        "\u00b718th edition\u00a0": [],
        "\u00b7Minimum of 5 years building services experience or similar (marine engineering for example)": []
    },
    {
        "From its inception in 1994, Chrissie Rucker\u2019s vision was to build a company that specialised in stylish, white, designer-quality items for the home that were not only exceptional quality, but also outstanding value for money. In addition to this devotion to simplicity, it was imperative the customer was put at the heart of everything and provided with a second-to-none shopping experience - and so The White Company was born.": [],
        "": [],
        "Note: This role is being advertised in London Head Office & Swan Valley, Northampton but there is only 1 live vacancy. We are open to either location.": [],
        "Reporting to the Development Manager (Retail and Integrations), you will be building and optimising on our Data platform architecture. Working with Azure based technologies, you will be building and maintaining data pipelines, querying and analysing data and optimizing data flows. You will support broader development teams to deliver data solutions for key business requirements.": [
            [
                "Azure",
                "HARD"
            ],
            [
                "data pipelines",
                "SOFT"
            ],
            [
                "querying",
                "SOFT"
            ],
            [
                "analysing data",
                "SOFT"
            ],
            [
                "optimizing data",
                "SOFT"
            ]
        ],
        "Essential": [],
        "Desirable": [],
        "At The White Company, we value our employees for always going the extra mile for every one of our customers; we reward this with great benefits and competitive salaries.": [],
        "Additional Benefits": [
            [
                "Additional Benefits",
                "PERK"
            ]
        ],
        "At The White Company we are committed to creating an inclusive culture that welcomes and celebrates a diversity of backgrounds and identities.": [],
        "We are working together to ensure our environment is one where people can bring their authentic selves to work, where their contribution is valued, ability enhanced, and perspective appreciated. Where difference is respected, encouraged, and celebrated. Where you can feel you belong.": [],
        "We are committed to an active Equality Diversity and Inclusion Policy, which starts with our recruitment and selection process.": [
            [
                "Inclusion Policy",
                "SOFT"
            ]
        ],
        "We'd love you to join us on our journey.": [],
        "Create and maintain optimal data pipelines / ETL processes using A Data Factory": [
            [
                "optimal data pipelines",
                "SOFT"
            ],
            [
                "ETL",
                "SOFT"
            ],
            [
                "Data Factory",
                "SOFT"
            ]
        ],
        "Assemble large, complex data sets using Azure Data lake that meet functional / non-functional business requirements.": [
            [
                "Azure Data lake",
                "HARD"
            ]
        ],
        "Architect and extend data models for data visualisation, analytics and data transfer": [
            [
                "visualisation",
                "SOFT"
            ],
            [
                "analytics",
                "SOFT"
            ],
            [
                "data transfer",
                "SOFT"
            ]
        ],
        "Analyse and tune performance of data delivery and ensure scalability of data processes": [
            [
                "Analyse",
                "SOFT"
            ],
            [
                "data delivery",
                "SOFT"
            ],
            [
                "data processes",
                "SOFT"
            ]
        ],
        "Work with data and analytics experts to strive for greater functionality in our data systems.": [
            [
                "data",
                "SOFT"
            ],
            [
                "data systems",
                "SOFT"
            ]
        ],
        "Support in data governance within the data platform (Azure Data Lake & other Data Platform services)": [
            [
                "data governance",
                "SOFT"
            ],
            [
                "Azure Data Lake",
                "HARD"
            ]
        ],
        "Strong proficiency with SQL with knowledge of best practices including debugging & troubleshooting": [
            [
                "SQL",
                "HARD"
            ],
            [
                "debugging",
                "SOFT"
            ]
        ],
        "Extensive experience in optimizing complex SQL statements and processes": [
            [
                "optimizing",
                "SOFT"
            ],
            [
                "SQL",
                "HARD"
            ]
        ],
        "Solid Relational Database design skills with an eye for performance optimisation.": [
            [
                "Relational Database",
                "SOFT"
            ]
        ],
        "Strong knowledge of different database models": [
            [
                "database",
                "SOFT"
            ]
        ],
        "Experience of creating PowerBI solutions and dashboards from row level data including data structure optimisation through to visualisation and dashboard creation.": [
            [
                "dashboards from",
                "SOFT"
            ],
            [
                "data structure",
                "SOFT"
            ],
            [
                "visualisation",
                "SOFT"
            ],
            [
                "dashboard creation",
                "SOFT"
            ]
        ],
        "Experience using modern data platforms and services (Data Lake, Azure Data Factory, SQL (On-prem/Cloud, Data Bricks) to build conformed and derived data models for the purpose of integrations, reporting and analytics": [
            [
                "Azure Data Factory",
                "HARD"
            ],
            [
                "SQL",
                "HARD"
            ],
            [
                "Cloud",
                "SOFT"
            ],
            [
                "Data Bricks",
                "HARD"
            ],
            [
                "analytics",
                "SOFT"
            ]
        ],
        "An ability to translate business requirements into technical specifications.": [],
        "Attention to detail and ability to QA own and other team member's work.": [
            [
                "Attention to detail",
                "PERK"
            ]
        ],
        "Good experience with ETL": [
            [
                "ETL",
                "SOFT"
            ]
        ],
        "Good experience with Analytical/Reporting solutions": [
            [
                "Analytical",
                "SOFT"
            ]
        ],
        "Have programmable experience with Python, Scala or Java.": [
            [
                "Python",
                "HARD"
            ],
            [
                "Scala",
                "HARD"
            ],
            [
                "Java",
                "HARD"
            ]
        ],
        "Some experience working with Machine Learning algorithms": [
            [
                "Machine Learning",
                "SOFT"
            ]
        ],
        "Have cloud based experience, preferably with Azure.": [
            [
                "Have cloud",
                "HARD"
            ],
            [
                "Azure",
                "HARD"
            ]
        ],
        "Exposure to high performing, low latency or large volume data systems (i.e. 1 billion+ records, terabyte size database).": [
            [
                "data systems",
                "SOFT"
            ],
            [
                "terabyte size database",
                "SOFT"
            ]
        ],
        "Experience with code versioning tools such as (GIT/SVN)": [
            [
                "code versioning",
                "SOFT"
            ],
            [
                "GIT",
                "HARD"
            ],
            [
                "SVN",
                "HARD"
            ]
        ],
        "Working within a continuous integration environment with automated builds, deployment and unit testing.": [
            [
                "integration",
                "SOFT"
            ],
            [
                "unit testing",
                "SOFT"
            ]
        ],
        "Experience working with retail & commerce data entities": [],
        "Discount -Up to 50% discount (dependent on contract type)": [
            [
                "50% discount (dependent on contract type)",
                "PERK"
            ]
        ],
        "Holiday \u2013 25 days, increasing to 28 days with length of service": [
            [
                "25 days",
                "PERK"
            ]
        ],
        "Holiday Buy \u2013 opportunity to buy up to an additional 5 days holiday": [
            [
                "Holiday Buy \u2013 opportunity to buy up to an additional 5 days holiday",
                "PERK"
            ]
        ],
        "Bonus Potential - In addition to our competitive salaries, all our permanent employees are entitled to join a discretionary bonus scheme. (dependent on contract type)": [
            [
                "Bonus Potential - In",
                "PERK"
            ]
        ],
        "Perkplace Benefits Platform \u2013 offering a variety of discounts across wellbeing and lifestyle": [
            [
                "Perkplace Benefits Platform \u2013 offering a",
                "PERK"
            ]
        ],
        "Wagestream Money Management app - access to Wagestream gives you power over your pay and supports financial wellbeing": [],
        "Continued Development \u2013\u00a0Inclusion on our Leadership Development Programme": [],
        "Pension Scheme": [
            [
                "Pension Scheme",
                "PERK"
            ]
        ],
        "Life Assurance": [
            [
                "Life Assurance",
                "PERK"
            ]
        ],
        "Private Medical": [
            [
                "Private Medical",
                "PERK"
            ]
        ],
        "Fruit basket daily": [],
        "Tea and coffee provided": [],
        "Working from Home - option to work from home on Mondays and Fridays": [
            [
                "Working from Home",
                "PERK"
            ]
        ],
        "Social - Christmas party/social events throughout the year": [],
        "Seasonal Sample Sales": [],
        "Volunteer Day - with a charity of your choice": [
            [
                "Volunteer Day - with a charity of your",
                "PERK"
            ]
        ],
        "Great Location - Close to transport links - over ground Shepherds Bush station /underground White City & Wood Lane stations. Westfield shopping centre with shops restaurants/bars/cinema and gym\u00a0": [
            [
                "Location",
                "SOFT"
            ]
        ]
    },
    {
        "Senior Software Engineer (Python)": [
            [
                "Senior Software Engineer",
                "PERK"
            ],
            [
                "Python",
                "HARD"
            ]
        ],
        "Remote within the UK": [],
        "\u00a360,000-\u00a370,000/annum": [],
        "Must be eligible for SC Clearance - British Passport required": [],
        "iO Associates is currently partnering with one of the UKs largest and most successful public sector consultancy firms who are on the look out for a Senior Software Engineer with experience in Data Engineering.": [
            [
                "iO",
                "HARD"
            ],
            [
                "Data Engineering",
                "CERT"
            ]
        ],
        "They need someone with developer level python skills, and experience designing and building data warehouses and pipelines. ": [
            [
                "python",
                "HARD"
            ],
            [
                "data warehouses",
                "SOFT"
            ],
            [
                "pipelines",
                "SOFT"
            ]
        ],
        "You must have the following skillset": [],
        "- Excellent Python programming skills including Pyspark": [
            [
                "Python",
                "HARD"
            ],
            [
                "Pyspark",
                "SOFT"
            ]
        ],
        "- ETL ": [
            [
                "ETL",
                "SOFT"
            ]
        ],
        "- Ideally AWS Glue - but other cloud platform experience will be considered": [
            [
                "AWS Glue",
                "HARD"
            ]
        ],
        "You should have upwards of four years experience for this role.": [],
        "You can be based anywhere in the UK for this role, and there will only be occasional expectation to attend one of a number of sites accross the UK. ": [],
        "If you fit the above criteria, hit apply and I will call you!": [],
        "Please note - no sponsorship can be offered for this role, and you must hold a UK Passport to be eligible for clearance. ": []
    },
    {
        "We employ people from all backgrounds and give them careers within technology, working with eager and enthusiastic individuals to give them the skills for success within the public and private sectors. We design careers, coach future leaders, and promote a more diverse and equal landscape with our work garnering over 10 awards across L&D and ED&I. We are a Top 20 Employer for Social Mobility and very proud to say BCorp certified. ": [],
        "Technology permeates all sectors: From Health Services to Education, Media to Sustainable Energy. With it we improve everyday life, connecting and positively impacting in the world in a way that has never been seen before. A career in tech is fascinating and offers endless progression by future-proofing you against change in the workplace. The benefits, flexibility and wages speak for themselves, highly specialised technology skill sets are in demand and we can provide you with, or enhance, those capabilities. Our belief is that anyone from any background has the capability of working within technology, and we've proven this by consistently supporting successful careers for almost a decade. ": [],
        "You'll be designing, building, maintaining, and troubleshooting the data pipelines that enable organizations to store, process, and analyse their data, and ensuring the data is reliable through testing and debugging.": [
            [
                "data pipelines",
                "SOFT"
            ],
            [
                "analyse",
                "SOFT"
            ],
            [
                "data",
                "SOFT"
            ],
            [
                "testing",
                "SOFT"
            ],
            [
                "debugging",
                "SOFT"
            ]
        ],
        "We're not expecting you to be an expert right away - that's where our award-winning Academy comes in. We're experts in building skills and confidence in a fun and supportive environment that will not only challenge you, but help you develop into a confident and capable consultant ready to be placed with a client in London. ": [],
        "To be successful in this role, you should have skills in either C#, Java, Python or similar. You will be passionate about technology and eager to learn and grow your skills.": [
            [
                "C#",
                "HARD"
            ],
            [
                "Java",
                "HARD"
            ],
            [
                "Python",
                "HARD"
            ]
        ],
        "Traits that align with companies' values: ": [],
        "Empathy and Diversity - You operate with integrity, respect everyone and set a good example for the team. ": [
            [
                "Empathy",
                "SOFT"
            ],
            [
                "Diversity",
                "SOFT"
            ]
        ],
        "Drive - You challenge yourself to exceed targets to take pride in your work. ": [
            [
                "Drive",
                "SOFT"
            ]
        ],
        "Collaboration - You work in synergy with others, supportive, approachable, build healthy relationships. ": [
            [
                "Collaboration",
                "SOFT"
            ]
        ],
        "Innovation - Your inquisitiveness knows no bounds and you love to learn. You embrace creativity and enjoy working with different people and their viewpoints. ": [
            [
                "Innovation",
                "SOFT"
            ]
        ],
        "Flexibility - Adaptable to change, you are calm and compassionate when responding to the unexpected. ": [
            [
                "Flexibility",
                "SOFT"
            ]
        ],
        "We will make sure you are ready and confident for the workplace through our salaried and fully remote training. As part of Sparta's all-female Academy, we will provide you with in-depth knowledge of the technical tools and skills you'll need to succeed in your future career in an all-female training environment. You'll learn alongside like-minded individuals and develop your business acumen, collaborative skills, and personal development. Don't just believe us, hear from our women in tech through their testimonials!": [],
        "We see ourselves as a people-powered business that likes to recognise and reward the hard work of our employees. We promote continuous learning and development with increasing earning potential for everyone who joins us. By the end of your first year, based on our performance metrics, you can expect to earn an average of \u00a329,000.": [],
        "We also provide: ": [],
        "We are an equal opportunities employer and welcome applications from candidates from all backgrounds and levels of experience. To be eligible for this role, all you need is: ": [],
        "Please note that we cannot accept your application if;": [],
        "We are an equal opportunities employer and welcome applications from candidates from all backgrounds and levels of experience. To be eligible to apply, all you need is to...": [],
        "Once you have applied, you will be contacted by a dedicated Talent Coordinator to provide advice, application support and guidance. We look forward to receiving your application - good luck!": [
            [
                "application",
                "SOFT"
            ]
        ],
        "Please note, unfortunately we cannot accept applications from candidates who require visa sponsorship or are on a Visa that will require sponsorship after it expires. We also can only accept applications from candidates currently living and residing in the UK. ": [],
        "Looking for efficiencies and optimising the data pipelines for scalability and performance will be a focus.": [
            [
                "data pipelines",
                "SOFT"
            ]
        ],
        "You'll be handling and working with large sets of structured and unstructured data and will be responsible for ensuring that the data is organized and available for data scientists and analysts to use.": [],
        "Working with others is key, you could be working with other engineers, developers, data scientists, analysts and even stakeholders to understand their data needs.": [],
        "20 days annual leave + bank holidays. ": [
            [
                "20 days annual leave + bank holidays",
                "PERK"
            ]
        ],
        "An extra day off for your birthday. ": [
            [
                "day off for your birthday",
                "PERK"
            ]
        ],
        "Pension. ": [
            [
                "Pension",
                "PERK"
            ]
        ],
        "Discounted gym membership. ": [
            [
                "Discounted gym membership",
                "PERK"
            ]
        ],
        "Eye care. ": [
            [
                "Eye care",
                "PERK"
            ]
        ],
        "Death in service cover. ": [],
        "Cycle to work scheme. ": [
            [
                "Cycle to work scheme",
                "PERK"
            ]
        ],
        "Season ticket loan. ": [
            [
                "Season ticket loan",
                "PERK"
            ]
        ],
        "Bonuses and structured pay rises. ": [],
        "Employee assistance program. ": [],
        "Yearly budget for personal development. ": [
            [
                "Yearly budget for personal development",
                "PERK"
            ]
        ],
        "Access to alumni and community networks. ": [],
        "Opportunities to be brand ambassadors. ": [],
        "To hold the full rights to work in the UK without sponsorship. ": [],
        "You require visa sponsorship. ": [],
        "You are on a visa that will require sponsorship after it expires. ": [],
        "You are not currently living and residing in the UK. ": [],
        "Hold the full rights to work in the UK without sponsorship": []
    }
]